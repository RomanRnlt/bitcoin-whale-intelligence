{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bitcoin Whale Intelligence: Vollständige Analyse-Pipeline\n\nDieses Notebook führt die vollständige Pipeline zur Identifikation und Analyse von Bitcoin-Whales durch.\n\n---\n\n## Inhaltsübersicht\n\n| Teil | Thema | Status |\n|------|-------|--------|\n| **I** | Einführung und Kontext | Fertig |\n| **II** | Datenverarbeitung (ETL) | Fertig |\n| **III** | UTXO-Analyse | Fertig |\n| **IV** | Entity Clustering | Fertig |\n| **V** | Whale Detection | GEPLANT |\n| **VI** | Verhaltensanalyse | GEPLANT |\n| **VII** | Zusammenfassung und Ausblick | Fertig |\n\n---\n\n# Teil I: Einführung und Kontext\n\n## 1.1 Projektziel\n\nDieses Projekt analysiert die Bitcoin-Blockchain um \"Whales\" (große Halter) zu identifizieren und ihr Verhalten zu verstehen. Die Pipeline umfasst:\n\n1. **Daten laden**: Bitcoin-Blockchain-Daten aus bitcoin-etl JSON Export\n2. **Daten transformieren**: JSON → optimiertes Parquet-Format\n3. **UTXO-Set berechnen**: Unspent Transaction Outputs identifizieren\n4. **Entity Clustering**: Adressen zu Entities gruppieren\n5. **Whale Detection**: Große Halter identifizieren *(geplant)*\n6. **Verhaltensanalyse**: Akkumulation vs. Distribution analysieren *(geplant)*\n\n### Datenquelle\n\nDie Daten wurden mit [bitcoin-etl](https://github.com/blockchain-etl/bitcoin-etl) von einem Bitcoin Full Node exportiert:\n\n```bash\nbitcoinetl export_all \\\n    --provider-uri http://user:pass@localhost:8332 \\\n    --start YYYY-MM-DD --end YYYY-MM-DD \\\n    --output-dir /path/to/blockchain_exports\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Bitcoin-Grundkonzepte\n\n### Das UTXO-Modell\n\nBitcoin verwendet im Gegensatz zu Account-basierten Systemen (wie Bankkonten) ein **UTXO-Modell** (Unspent Transaction Output). Hier werden \"Münzen\" vollständig ausgegeben und Wechselgeld zurückgegeben.\n\n| Account-basiert (Bank) | UTXO-basiert (Bitcoin) |\n|------------------------|------------------------|\n| Konto hat Kontostand: 100 EUR | Besitz von \"Münzen\" verschiedener Größe |\n| Überweisung reduziert Kontostand | Münzen werden **vollständig** ausgegeben |\n| Einfache Subtraktion | Wechselgeld als neue UTXO zurück |\n\n### Bitcoin-Adresse vs. Entity\n\n| Konzept | Beschreibung | Sichtbar in Blockchain? |\n|---------|--------------|-------------------------|\n| **Adresse** | Einzelner \"Briefkasten\" für Bitcoin (z.B. `bc1q...`) | Ja |\n| **Wallet** | Software die viele Adressen verwaltet | Nein |\n| **Entity** | Person/Firma die ein oder mehrere Wallets besitzt | Nein |\n\n### Das zentrale Problem\n\n```\n800 Millionen Bitcoin-Adressen existieren\n         ↓\nWer besitzt sie?\n         ↓\nEine Person kann 1,000+ Adressen haben\nEine Börse kann 5,000,000+ Adressen haben\n         ↓\nDie Blockchain zeigt NICHT welche Adressen zusammengehören!\n```\n\n**Ziel dieses Projekts**: Adressen zu Entities gruppieren durch Analyse der Transaktionsmuster, dann große Entities (\"Whales\") identifizieren."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Glossar: Wichtige Begriffe\n\n| Begriff | Erklärung |\n|---------|----------|\n| **Coinbase-Transaktion** | Die erste Transaktion in jedem Block. Enthält die Mining-Belohnung und hat keine Inputs - das neu geschürfte Bitcoin entsteht hier \"aus dem Nichts\". |\n| **Satoshi** | Kleinste Bitcoin-Einheit. 1 BTC = 100.000.000 Satoshi. Benannt nach dem Bitcoin-Erfinder Satoshi Nakamoto. Werte in der Blockchain sind immer in Satoshi angegeben. |\n| **Parquet** | Spaltenorientiertes Binärformat für Big Data. Vorteile: hohe Kompression (70-90%), schnelles Lesen einzelner Spalten, optimiert für analytische Queries. |\n| **GraphFrames** | Apache Spark Bibliothek für Graph-Algorithmen. Ermöglicht verteilte Berechnung auf Graphen mit Millionen/Milliarden Knoten. |\n| **Connected Components** | Graph-Algorithmus der zusammenhängende Teilgraphen findet. Alle Knoten die direkt oder transitiv verbunden sind, erhalten dieselbe Komponenten-ID. |\n| **Hive-Partitionierung** | Ordnerstruktur zur Datenorganisation nach Schlüsseln (z.B. `date=2011-01-01/`). Ermöglicht effizientes Filtern ohne alle Daten zu lesen. |\n| **Whale** | Umgangssprachlich für Bitcoin-Halter mit sehr großen Beständen (typischerweise >1.000 BTC). |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Teil II: Datenverarbeitung (ETL)\n\n## 2.1 Setup und Konfiguration"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projektverzeichnis: /Users/roman/spark_project/bitcoin-whale-intelligence\n",
      "Datenquelle: /Users/roman/spark_project/blockchain_exports\n",
      "Ausgabe: /Users/roman/spark_project/bitcoin-whale-intelligence/data\n"
     ]
    }
   ],
   "source": [
    "# Standard-Bibliotheken\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Projektverzeichnis ermitteln und zum Path hinzufügen\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Projektverzeichnis: {project_root}\")\n",
    "\n",
    "# ============================================================================\n",
    "# KONFIGURATION - HIER ANPASSEN\n",
    "# ============================================================================\n",
    "\n",
    "# Pfad zu den bitcoin-etl exportierten Daten\n",
    "BLOCKCHAIN_DATA_PATH = \"/Users/roman/spark_project/blockchain_exports\"\n",
    "\n",
    "# Ausgabeverzeichnis für Parquet-Dateien\n",
    "OUTPUT_PATH = str(project_root / \"data\")\n",
    "\n",
    "# Spark-Konfiguration\n",
    "DRIVER_MEMORY = \"8g\"  # Erhöhen für größere Datasets\n",
    "\n",
    "print(f\"Datenquelle: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Ausgabe: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "# ETL-Modul importieren\n",
    "from src.etl import (\n",
    "    create_spark_session,\n",
    "    load_transactions,\n",
    "    load_blocks,\n",
    "    explode_outputs,\n",
    "    explode_inputs,\n",
    "    compute_utxo_set,\n",
    "    enrich_clustering_inputs,\n",
    ")\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Module erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.5.7-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/roman/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/roman/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f2aee8ca-4158-48b4-92bc-1ace7d43942e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.3-spark3.5-s_2.12!graphframes.jar (210ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (138ms)\n",
      ":: resolution report :: resolve 1946ms :: artifacts dl 355ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f2aee8ca-4158-48b4-92bc-1ace7d43942e\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (280kB/13ms)\n",
      "25/12/08 18:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.7\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Spark-Session erstellen\n",
    "spark = create_spark_session(\n",
    "    app_name=\"Bitcoin Whale Intelligence\",\n",
    "    driver_memory=DRIVER_MEMORY,\n",
    "    enable_graphframes=True\n",
    ")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Daten laden\n\nDie bitcoin-etl Daten liegen als Hive-partitionierte JSON-Dateien vor:\n\n```\nblockchain_exports/\n└── 2011-01-01_2011-06-01/\n    ├── blocks/\n    │   └── date=YYYY-MM-DD/\n    │       └── blocks_*.json\n    └── transactions/\n        └── date=YYYY-MM-DD/\n            └── transactions_*.json\n```\n\n**Wichtig**: Die Transaktionen enthalten **nested Arrays** für inputs und outputs. Dies ist anders als bei normalisierten Datenbanken (z.B. BigQuery) wo diese in separaten Tabellen liegen."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Transaktionen...\n",
      "Lade Blocks...\n",
      "\n",
      "Daten geladen und gecacht.\n"
     ]
    }
   ],
   "source": [
    "# Transaktionen und Blocks laden\n",
    "print(\"Lade Transaktionen...\")\n",
    "tx_df = load_transactions(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "print(\"Lade Blocks...\")\n",
    "blocks_df = load_blocks(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "# Cache für wiederholten Zugriff\n",
    "tx_df.cache()\n",
    "blocks_df.cache()\n",
    "\n",
    "print(\"\\nDaten geladen und gecacht.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladene Daten:\n",
      "  Transaktionen: 382,402\n",
      "  Blocks: 27,644\n"
     ]
    }
   ],
   "source": [
    "# Grundstatistiken\n",
    "tx_count = tx_df.count()\n",
    "block_count = blocks_df.count()\n",
    "\n",
    "print(f\"Geladene Daten:\")\n",
    "print(f\"  Transaktionen: {tx_count:,}\")\n",
    "print(f\"  Blocks: {block_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenumfang und Einordnung\n",
    "\n",
    "**Zeitraum dieser Daten:** Januar bis Juni 2011 (frühe Bitcoin-Geschichte)\n",
    "\n",
    "| Zeitpunkt | Transaktionen | Blocks | Kontext |\n",
    "|-----------|---------------|--------|--------|\n",
    "| **Diese Daten (H1 2011)** | ~380.000 | ~27.000 | Bitcoin war noch jung, wenig Nutzer |\n",
    "| Vollständige Blockchain 2024 | >900.000.000 | >800.000 | ~2.400x mehr Transaktionen |\n",
    "\n",
    "**Zweck**: Diese Daten dienen als Testdatensatz zum Validieren der Pipeline. Die Algorithmen funktionieren identisch auf der vollständigen Blockchain - nur die Rechenzeit und Speicheranforderungen steigen.\n",
    "\n",
    "**Historischer Kontext**: Im Jahr 2011 war 1 BTC zwischen $0.30 und $30 wert. Die größten \"Whales\" dieser Zeit waren oft Early Adopter und Mining-Enthusiasten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Schema (Auszug):\n",
      "root\n",
      " |-- hash: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      " |-- virtual_size: integer (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- lock_time: long (nullable = true)\n",
      " |-- block_number: long (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- block_timestamp: long (nullable = true)\n",
      " |-- is_coinbase: boolean (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- inputs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- index: integer (nullable = true)\n",
      " |    |    |-- spent_transaction_hash: string (nullable = true)\n",
      " |    |    |-- spent_output_index: integer (nullable = true)\n",
      " |    |    |-- script_asm: string (nullable = true)\n",
      " |    |    |-- script_hex: string (nullable = true)\n",
      " |    |    |-- sequence: long (nullable = true)\n",
      " |    |    |-- required_signatures: integer (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- addresses: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- outputs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- index: integer (nullable = true)\n",
      " |    |    |-- script_asm: string (nullable = true)\n",
      " |    |    |-- script_hex: string (nullable = true)\n",
      " |    |    |-- required_signatures: integer (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- addresses: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- input_count: integer (nullable = true)\n",
      " |-- output_count: integer (nullable = true)\n",
      " |-- input_value: long (nullable = true)\n",
      " |-- output_value: long (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- block_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema anzeigen\n",
    "print(\"Transaction Schema (Auszug):\")\n",
    "tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel-Transaktion:\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "|                hash|block_number|input_count|output_count|is_coinbase|output_value|        fee|\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "|630bb912bea097180...|      126766|          0|           1|       true|  5000000000|          0|\n",
      "|0556dd5dba67f4476...|      126766|          2|           2|      false|  2128000000|-2128000000|\n",
      "|7a41ec18684517921...|      126766|          1|           1|      false|    50000000|  -50000000|\n",
      "|c0b929b9c6abdecb3...|      126766|          1|           1|      false|   100000000| -100000000|\n",
      "|4cedd988b1b9018c1...|      126765|          0|           1|       true|  5002000000|          0|\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-Transaktion anzeigen\n",
    "print(\"Beispiel-Transaktion:\")\n",
    "tx_df.select(\n",
    "    \"hash\", \"block_number\", \"input_count\", \"output_count\", \n",
    "    \"is_coinbase\", \"output_value\", \"fee\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Teil III: UTXO-Analyse\n\n## 3.1 Das UTXO-Modell im Detail\n\n### Praktisches Beispiel\n\nAlice besitzt zwei UTXOs:\n- **UTXO A**: 0.5 BTC (auf Adresse A1)\n- **UTXO B**: 0.3 BTC (auf Adresse A2)\n- **Gesamt**: 0.8 BTC\n\nAlice möchte **0.7 BTC** an Bob senden:\n\n```\nINPUTS (was Alice ausgibt):        OUTPUTS (was erstellt wird):\n┌─────────────────────┐            ┌─────────────────────┐\n│ UTXO A: 0.5 BTC     │            │ An Bob: 0.7 BTC     │\n│ (Adresse A1)        │   ───►     │ (neue UTXO für Bob) │\n├─────────────────────┤            ├─────────────────────┤\n│ UTXO B: 0.3 BTC     │            │ Wechselgeld: 0.09   │\n│ (Adresse A2)        │            │ (neue UTXO für      │\n└─────────────────────┘            │  Alice auf A3)      │\n  Summe: 0.8 BTC                   ├─────────────────────┤\n                                   │ Fee: 0.01 BTC       │\n                                   │ (an Miner)          │\n                                   └─────────────────────┘\n                                     Summe: 0.8 BTC\n```\n\n**Wichtige Erkenntnis für Entity Clustering**: Alice musste **beide Adressen A1 und A2** als Inputs verwenden. Dafür braucht sie die Private Keys beider Adressen. **→ A1 und A2 gehören zur selben Person!** Diese Beobachtung ist die Grundlage der *Common Input Ownership Heuristic*, die wir in Teil IV nutzen."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.2 ETL: Transformation zu Parquet\n\n### VORHER: Nested JSON-Arrays\n\n```\ntx_hash | outputs\n--------|--------\nabc123  | [{index:0, value:50000000, addr:\"1A...\"}, {index:1, value:30000000, addr:\"1B...\"}]\n```\n\n### NACHHER: Flache Tabellen (nach explode)\n\n```\ntx_hash | output_index | value    | address\n--------|--------------|----------|--------\nabc123  | 0            | 50000000 | 1A...\nabc123  | 1            | 30000000 | 1B...\n```\n\n**WARUM diese Transformation?**\n- Flache Tabellen erlauben JOINs und GROUP BY\n- Parquet ist 70-90% kleiner als JSON\n- Spark liest nur benoetigte Spalten"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 Multi-Input Transaktionen analysieren\n\nBevor wir das UTXO Set berechnen, analysieren wir die Verteilung der Input-Counts. Diese Information ist wichtig für das Entity Clustering in Teil IV."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Count Verteilung (Top 15):\n",
      " input_count  transaction_count\n",
      "           1             287512\n",
      "           2              33911\n",
      "           3              11259\n",
      "           4               6672\n",
      "           5               4027\n",
      "           6               2449\n",
      "           7               1696\n",
      "           8               1186\n",
      "           9                935\n",
      "          10                782\n",
      "          11                487\n",
      "          12                433\n",
      "          13                311\n",
      "          14                267\n",
      "          15                247\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, sum as spark_sum, avg\n",
    "\n",
    "# Input-Count Verteilung\n",
    "input_dist = tx_df \\\n",
    "    .filter(col(\"is_coinbase\") == False) \\\n",
    "    .groupBy(\"input_count\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\")) \\\n",
    "    .orderBy(\"input_count\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"Input-Count Verteilung (Top 15):\")\n",
    "print(input_dist.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-INPUT TRANSACTION STATISTIK\n",
      "============================================================\n",
      "\n",
      "Gesamt (ohne Coinbase): 354,758\n",
      "Single-Input (1 Adresse):   287,512 (81.0%)\n",
      "Multi-Input (≥2 Adressen):  67,246 (19.0%)\n",
      "\n",
      "→ 19.0% der Transaktionen sind für Clustering nutzbar\n"
     ]
    }
   ],
   "source": [
    "# Statistiken berechnen\n",
    "total_non_coinbase = tx_df.filter(col(\"is_coinbase\") == False).count()\n",
    "single_input = input_dist[input_dist['input_count'] == 1]['transaction_count'].sum()\n",
    "multi_input = input_dist[input_dist['input_count'] > 1]['transaction_count'].sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-INPUT TRANSACTION STATISTIK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGesamt (ohne Coinbase): {total_non_coinbase:,}\")\n",
    "print(f\"Single-Input (1 Adresse):   {single_input:,} ({single_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"Multi-Input (≥2 Adressen):  {multi_input:,} ({multi_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"\\n→ {multi_input/total_non_coinbase*100:.1f}% der Transaktionen sind für Clustering nutzbar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualisierung\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# ============================================================================\n# Chart 1: Normale Skala (zeigt das Problem)\n# ============================================================================\nplot_data = input_dist[input_dist['input_count'] <= 20]\naxes[0].bar(plot_data['input_count'], plot_data['transaction_count'], \n            color='steelblue', edgecolor='black', linewidth=0.5)\naxes[0].set_xlabel('Anzahl Inputs pro Transaktion')\naxes[0].set_ylabel('Anzahl Transaktionen')\naxes[0].set_title('Problem: 1-Input dominiert alles')\naxes[0].grid(True, alpha=0.3)\n# Annotation\naxes[0].annotate(f'{int(single_input):,}', \n                 xy=(1, single_input), \n                 xytext=(3, single_input * 0.8),\n                 fontsize=9,\n                 arrowprops=dict(arrowstyle='->', color='red'))\naxes[0].text(10, single_input * 0.5, \n             '← Die anderen Balken\\n    sind kaum sichtbar!', \n             fontsize=9, color='red')\n\n# ============================================================================\n# Chart 2: Logarithmische Skala (macht alle Balken sichtbar)\n# ============================================================================\naxes[1].bar(plot_data['input_count'], plot_data['transaction_count'], \n            color='steelblue', edgecolor='black', linewidth=0.5)\naxes[1].set_xlabel('Anzahl Inputs pro Transaktion')\naxes[1].set_ylabel('Anzahl Transaktionen')\naxes[1].set_title('Lösung: Logarithmische Skala')\naxes[1].set_yscale('log')\naxes[1].grid(True, alpha=0.3)\n\n# Y-Achse mit lesbaren Zahlen statt 10^x\nfrom matplotlib.ticker import FuncFormatter\ndef readable_formatter(x, pos):\n    if x >= 1000:\n        return f'{int(x/1000)}k'\n    return f'{int(x)}'\naxes[1].yaxis.set_major_formatter(FuncFormatter(readable_formatter))\naxes[1].text(10, 100, 'Jetzt sieht man\\nauch 2, 3, 4+ Inputs', fontsize=9, color='green')\n\n# ============================================================================\n# Chart 3: Pie Chart\n# ============================================================================\nlabels = ['Single-Input\\n(nicht nutzbar)', 'Multi-Input\\n(nutzbar für Clustering)']\nsizes = [single_input, multi_input]\ncolors = ['lightgray', 'steelblue']\naxes[2].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\naxes[2].set_title('Anteil für Clustering')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\nErklärung zur logarithmischen Skala:\")\nprint(\"  - Normale Skala: 1-Input-TXs (~290k) dominieren, Rest unsichtbar\")\nprint(\"  - Log-Skala: Abstände werden gestaucht, alle Werte sichtbar\")\nprint(\"  - Beispiel: 100 → 1000 → 10000 haben gleiche Abstände auf der Y-Achse\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Beispiel: Multi-Input Transaktion\n\nEine konkrete Multi-Input-Transaktion zeigt, wie mehrere Adressen in einer Transaktion kombiniert werden:"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel Multi-Input Transaktion:\n",
      "  Hash: a55cfdd8677056d6b0fb...\n",
      "  Block: 126765\n",
      "  Input Count: 4\n",
      "  Output Count: 1\n",
      "  Wert: 20.27000000 BTC\n",
      "\n",
      "  Inputs (Adressen die zusammengehören):\n",
      "    [0] (enrichment nötig)\n",
      "    [1] (enrichment nötig)\n",
      "    [2] (enrichment nötig)\n",
      "    [3] (enrichment nötig)\n",
      "\n",
      "  → Diese 4 Adressen gehören zur selben Entity!\n"
     ]
    }
   ],
   "source": [
    "# Eine Multi-Input Transaktion detailliert betrachten\n",
    "example_tx = tx_df \\\n",
    "    .filter(\n",
    "        (col(\"input_count\") >= 3) & \n",
    "        (col(\"input_count\") <= 10) &\n",
    "        (col(\"is_coinbase\") == False)\n",
    "    ) \\\n",
    "    .first()\n",
    "\n",
    "if example_tx:\n",
    "    print(f\"Beispiel Multi-Input Transaktion:\")\n",
    "    print(f\"  Hash: {example_tx['hash'][:20]}...\")\n",
    "    print(f\"  Block: {example_tx['block_number']}\")\n",
    "    print(f\"  Input Count: {example_tx['input_count']}\")\n",
    "    print(f\"  Output Count: {example_tx['output_count']}\")\n",
    "    print(f\"  Wert: {example_tx['output_value'] / 100000000:.8f} BTC\")\n",
    "    \n",
    "    print(f\"\\n  Inputs (Adressen die zusammengehören):\")\n",
    "    for i, inp in enumerate(example_tx['inputs'][:5]):\n",
    "        addr = inp['addresses'][0] if inp['addresses'] else \"(enrichment nötig)\"\n",
    "        print(f\"    [{i}] {addr}\")\n",
    "    if len(example_tx['inputs']) > 5:\n",
    "        print(f\"    ... und {len(example_tx['inputs']) - 5} weitere\")\n",
    "    \n",
    "    print(f\"\\n  → Diese {example_tx['input_count']} Adressen gehören zur selben Entity!\")\n",
    "else:\n",
    "    print(\"Keine passende Multi-Input Transaktion gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.4 Outputs und Inputs explodieren"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explodiere Outputs...\n",
      "Outputs: 769,081\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "|                       tx_hash|block_number|block_timestamp|output_index|     value|                     addresses|output_type|\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "|630bb912bea0971803979417615...|      126766|     1306369241|           0|5000000000|[nonstandarde36d71a77f0b72f...|nonstandard|\n",
      "|0556dd5dba67f4476841e530125...|      126766|     1306369241|           0| 128000000|[1N1HR4BPwhP5WvFXF6JCTkRnKj...| pubkeyhash|\n",
      "|0556dd5dba67f4476841e530125...|      126766|     1306369241|           1|2000000000|[16UksCM6jXXR8XGq9cXWiP48P1...| pubkeyhash|\n",
      "|7a41ec18684517921585f710b37...|      126766|     1306369241|           0|  50000000|[1HHETdBA3zrUf9RHoBdgteucMX...| pubkeyhash|\n",
      "|c0b929b9c6abdecb3ebd3857f93...|      126766|     1306369241|           0| 100000000|[12bAvkf2ku6za5XKHwvjgaVvZS...| pubkeyhash|\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outputs explodieren (nested → flat)\n",
    "print(\"Explodiere Outputs...\")\n",
    "outputs_df = explode_outputs(tx_df)\n",
    "outputs_df.cache()\n",
    "\n",
    "output_count = outputs_df.count()\n",
    "print(f\"Outputs: {output_count:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "outputs_df.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explodiere Inputs...\n",
      "Inputs: 632,295\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "|             tx_hash|input_index|       spent_tx_hash|spent_output_index|value|\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "|630bb912bea097180...|       NULL|                NULL|              NULL| NULL|\n",
      "|0556dd5dba67f4476...|          0|3ccb1d88e9f0f8067...|                 1| NULL|\n",
      "|0556dd5dba67f4476...|          1|6f5594a671cd2b686...|                 1| NULL|\n",
      "|7a41ec18684517921...|          0|5259ab3a1d0045aa3...|                 1| NULL|\n",
      "|c0b929b9c6abdecb3...|          0|d683c9078dad5625e...|                22| NULL|\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inputs explodieren\n",
    "print(\"Explodiere Inputs...\")\n",
    "inputs_df = explode_inputs(tx_df)\n",
    "inputs_df.cache()\n",
    "\n",
    "input_count_flat = inputs_df.count()\n",
    "print(f\"Inputs: {input_count_flat:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "inputs_df.select(\n",
    "    \"tx_hash\", \"input_index\", \"spent_tx_hash\", \"spent_output_index\", \"value\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speichere Outputs als Parquet...\n",
      "Speichere Inputs als Parquet...\n",
      "\n",
      "Parquet-Dateien gespeichert in: /Users/roman/spark_project/bitcoin-whale-intelligence/data\n"
     ]
    }
   ],
   "source": [
    "# Als Parquet speichern\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(OUTPUT_PATH)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Speichere Outputs als Parquet...\")\n",
    "outputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"outputs.parquet\"))\n",
    "\n",
    "print(\"Speichere Inputs als Parquet...\")\n",
    "inputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"inputs.parquet\"))\n",
    "\n",
    "print(f\"\\nParquet-Dateien gespeichert in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.5 UTXO Set berechnen\n\n### Was ist ein UTXO?\n\n**UTXO = Unspent Transaction Output** = Eine \"Muenze\" die noch nicht ausgegeben wurde.\n\n### Die Berechnung\n\n```\nUTXO Set = Alle Outputs MINUS Outputs die als Input referenziert wurden\n\nBeispiel:\n┌────────────────┐     ┌────────────────┐     ┌────────────────┐\n│ outputs        │  -  │ spent_refs     │  =  │ utxos          │\n│ tx1:0 -> 5 BTC │     │ tx1:0          │     │ tx1:1 -> 3 BTC │\n│ tx1:1 -> 3 BTC │     │                │     │ tx2:0 -> 2 BTC │\n│ tx2:0 -> 2 BTC │     │                │     │                │\n└────────────────┘     └────────────────┘     └────────────────┘\n```\n\n**SQL-Logik (LEFT ANTI JOIN):**\n\n```sql\nSELECT * FROM outputs\nWHERE (tx_hash, output_index) NOT IN (\n    SELECT spent_tx_hash, spent_output_index FROM inputs\n)\n```\n\n**WARUM brauchen wir das UTXO Set?**\n- Nur UTXOs haben aktuellen Wert\n- Spent Outputs sind \"verbraucht\" (Wert = 0)\n- Fuer Whale Detection: Balance = SUM(UTXO values)"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne UTXO Set...\n",
      "\n",
      "UTXO Statistik:\n",
      "  Gesamt Outputs: 769,081\n",
      "  Spent (ausgegeben): 592,040 (77.0%)\n",
      "  Unspent (UTXOs): 177,041 (23.0%)\n"
     ]
    }
   ],
   "source": [
    "# UTXO Set berechnen\n",
    "print(\"Berechne UTXO Set...\")\n",
    "utxo_df = compute_utxo_set(outputs_df, inputs_df)\n",
    "utxo_df.cache()\n",
    "\n",
    "utxo_count = utxo_df.count()\n",
    "spent_count = output_count - utxo_count\n",
    "\n",
    "print(f\"\\nUTXO Statistik:\")\n",
    "print(f\"  Gesamt Outputs: {output_count:,}\")\n",
    "print(f\"  Spent (ausgegeben): {spent_count:,} ({spent_count/output_count*100:.1f}%)\")\n",
    "print(f\"  Unspent (UTXOs): {utxo_count:,} ({utxo_count/output_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speichere UTXO Set...\n",
      "Gespeichert: /Users/roman/spark_project/bitcoin-whale-intelligence/data/utxos.parquet\n"
     ]
    }
   ],
   "source": [
    "# UTXO Set speichern\n",
    "print(\"Speichere UTXO Set...\")\n",
    "utxo_df.write.mode(\"overwrite\").parquet(str(output_dir / \"utxos.parquet\"))\n",
    "print(f\"Gespeichert: {output_dir / 'utxos.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.1 Common Input Ownership Heuristic\n\n### WARUM funktioniert diese Heuristik?\n\n```\nTransaktion X hat 2 Inputs:\n  - Input 1: von Adresse A (braucht Private Key A)\n  - Input 2: von Adresse B (braucht Private Key B)\n\nUm diese Transaktion zu signieren, braucht man BEIDE Private Keys.\n-> Nur der Besitzer beider Adressen kann das!\n-> A und B gehoeren zur selben Entity (Person/Firma)\n```\n\n### Transitive Verknuepfung (WARUM Connected Components?)\n\n```\nTX 1: Inputs von A + B  -->  A-B gehoeren zusammen\nTX 2: Inputs von B + C  -->  B-C gehoeren zusammen\n─────────────────────────────────────────────────\nSchlussfolgerung: A, B, C sind ALLE derselben Entity!\n\n    A ─── B ─── C\n    └─────┴─────┘\n      Entity 1\n```\n\n**Das ist ein Graph-Problem:** Finde alle verbundenen Knoten (= Connected Components).\n\n### Einschraenkungen der Heuristik\n\n| Problem | Ursache | Filter |\n|---------|---------|--------|\n| Exchange-TXs | Boersen buendeln Auszahlungen | `input_count <= 50` |\n| CoinJoin | Privacy-Protokoll mischt TXs | Pattern-Erkennung |\n| Mining Pools | Batch-Auszahlungen | Bekannte Adressen |"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4.2 Graph aufbauen\n\nZuerst reichern wir die Multi-Input-Transaktionen mit Adressen an:"
  },
  {
   "cell_type": "code",
   "source": "# Inputs für Clustering anreichern\nprint(\"Reichere Multi-Input-Transaktionen mit Adressen an...\")\nprint(\"  (Filter: 2-50 Inputs, keine Coinbase)\")\n\nclustering_inputs = enrich_clustering_inputs(\n    tx_df, \n    outputs_df,\n    min_inputs=2,\n    max_inputs=50\n)\nclustering_inputs.cache()\n\nenriched_count = clustering_inputs.count()\nprint(f\"\\nAngereicherte Inputs: {enriched_count:,}\")\n\n# Beispiel\nprint(\"\\nBeispiel (tx_hash -> address):\")\nclustering_inputs.show(10, truncate=40)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaktionen mit ≥2 Adressen: 57,606\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, size as spark_size\n",
    "from itertools import combinations\n",
    "\n",
    "# Adressen pro Transaktion gruppieren\n",
    "tx_addresses = clustering_inputs \\\n",
    "    .groupBy(\"tx_hash\") \\\n",
    "    .agg(collect_set(\"address\").alias(\"addresses\"))\n",
    "\n",
    "# Filtern: Nur TXs mit mindestens 2 verschiedenen Adressen\n",
    "tx_addresses = tx_addresses.filter(spark_size(\"addresses\") >= 2)\n",
    "\n",
    "tx_with_addresses = tx_addresses.count()\n",
    "print(f\"Transaktionen mit ≥2 Adressen: {tx_with_addresses:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-Kanten (unique): 400,872\n",
      "+--------------------+--------------------+\n",
      "|                 src|                 dst|\n",
      "+--------------------+--------------------+\n",
      "|192nJoWgPuc3sKFQB...|1Q3fg95C1TcqXS1Xc...|\n",
      "|1HL2U2Cz1tDkh52kJ...|1JwfvhMrphhVSZwNk...|\n",
      "|1KzyYTFAQF6Q5SAMR...|1LaqzJryo46Z4Tmof...|\n",
      "|1CPxeZnow3C3EU6Fr...|1LLaxfmyGB393VkGD...|\n",
      "|1B5fVpnS87hzioWQ3...|1CSzzf96C7fvuLYYu...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode as spark_explode, arrays_zip, transform, struct\n",
    "\n",
    "# Kanten erstellen: Alle Adresspaare pro Transaktion\n",
    "# Für jede TX mit Adressen [A, B, C] erstellen wir Kanten: (A,B), (A,C), (B,C)\n",
    "\n",
    "def create_edges_udf(addresses):\n",
    "    \"\"\"Erstellt alle Paare aus einer Liste von Adressen.\"\"\"\n",
    "    if not addresses or len(addresses) < 2:\n",
    "        return []\n",
    "    return [(a, b) for a, b in combinations(sorted(addresses), 2)]\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "edge_schema = ArrayType(StructType([\n",
    "    StructField(\"src\", StringType()),\n",
    "    StructField(\"dst\", StringType())\n",
    "]))\n",
    "\n",
    "create_edges = udf(create_edges_udf, edge_schema)\n",
    "\n",
    "# Kanten erstellen und explodieren\n",
    "edges_df = tx_addresses \\\n",
    "    .withColumn(\"edges\", create_edges(\"addresses\")) \\\n",
    "    .select(spark_explode(\"edges\").alias(\"edge\")) \\\n",
    "    .select(\n",
    "        col(\"edge.src\").alias(\"src\"),\n",
    "        col(\"edge.dst\").alias(\"dst\")\n",
    "    ) \\\n",
    "    .distinct()\n",
    "\n",
    "edges_df.cache()\n",
    "edge_count = edges_df.count()\n",
    "\n",
    "print(f\"Graph-Kanten (unique): {edge_count:,}\")\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-Knoten (Adressen): 147,907\n"
     ]
    }
   ],
   "source": [
    "# Vertices (alle eindeutigen Adressen)\n",
    "vertices_src = edges_df.select(col(\"src\").alias(\"id\"))\n",
    "vertices_dst = edges_df.select(col(\"dst\").alias(\"id\"))\n",
    "vertices_df = vertices_src.union(vertices_dst).distinct()\n",
    "\n",
    "vertex_count = vertices_df.count()\n",
    "print(f\"Graph-Knoten (Adressen): {vertex_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.3 Connected Components mit GraphFrames\n\n### Der Algorithmus in 3 Schritten\n\n**Schritt 1: Graph aufbauen**\n```\nVertices (Knoten) = Alle Bitcoin-Adressen\nEdges (Kanten)    = Adresspaare aus Multi-Input-TXs\n```\n\n**Schritt 2: Connected Components finden**\n```\nAlle Knoten die direkt oder transitiv verbunden sind\n= Eine Entity\n```\n\n**Schritt 3: IDs zuweisen**\n```\nJede Entity bekommt eine eindeutige ID (component)\n```\n\n### Beispiel\n\n```\nGraph:                     Ergebnis:\nA ─── B ─── C              | address | entity_id |\n                           |---------|-----------|\nD ─── E                    | A       | 1         |\n                           | B       | 1         |\n                           | C       | 1         |\n                           | D       | 2         |\n                           | E       | 2         |\n```\n\n### WARUM Checkpointing noetig ist\n\nDer Algorithmus ist **iterativ** - er laeuft bis keine Aenderungen mehr passieren.\nOhne Checkpointing waechst der Spark-Ausfuehrungsplan exponentiell -> Stack Overflow!"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle Graph...\n",
      "Führe Connected Components aus...\n",
      "  (Dies kann bei großen Graphen einige Minuten dauern)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o365.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOfRange(Arrays.java:4030)\n\tat java.base/java.lang.StringLatin1.newString(StringLatin1.java:715)\n\tat java.base/java.lang.StringBuilder.toString(StringBuilder.java:452)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:64)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:780)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2994/0x00000008012da040.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2963/0x00000008012c5440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda$6405/0x0000000801ca2840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda$2958/0x00000008012c2840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda$2817/0x000000080123b040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2825/0x000000080123fc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2818/0x000000080123b440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m spark.sparkContext.setCheckpointDir(\u001b[38;5;28mstr\u001b[39m(output_dir / \u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Connected Components berechnen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m entities_df = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m entities_df.cache()\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConnected Components berechnet.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/private/var/folders/nx/qs47_8zn2jsdkcz32b7cr8_r0000gn/T/spark-779e2879-97c0-498b-b0df-01c810819b29/userFiles-19438abe-2204-40f0-bd09-08031ce4346b/graphframes_graphframes-0.8.3-spark3.5-s_2.12.jar/graphframes/graphframe.py:331\u001b[39m, in \u001b[36mGraphFrame.connectedComponents\u001b[39m\u001b[34m(self, algorithm, checkpointInterval, broadcastThreshold)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnectedComponents\u001b[39m(\u001b[38;5;28mself\u001b[39m, algorithm = \u001b[33m\"\u001b[39m\u001b[33mgraphframes\u001b[39m\u001b[33m\"\u001b[39m, checkpointInterval = \u001b[32m2\u001b[39m,\n\u001b[32m    313\u001b[39m                         broadcastThreshold = \u001b[32m1000000\u001b[39m):\n\u001b[32m    314\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    Computes the connected components of the graph.\u001b[39;00m\n\u001b[32m    316\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m \u001b[33;03m    :return: DataFrame with new vertices column \"component\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetAlgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetCheckpointInterval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpointInterval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetBroadcastThreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcastThreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m._spark)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o365.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOfRange(Arrays.java:4030)\n\tat java.base/java.lang.StringLatin1.newString(StringLatin1.java:715)\n\tat java.base/java.lang.StringBuilder.toString(StringBuilder.java:452)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:64)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:780)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2994/0x00000008012da040.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2963/0x00000008012c5440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda$6405/0x0000000801ca2840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda$2958/0x00000008012c2840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda$2817/0x000000080123b040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2825/0x000000080123fc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2818/0x000000080123b440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n"
     ]
    }
   ],
   "source": [
    "# GraphFrames Connected Components\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "print(\"Erstelle Graph...\")\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "print(\"Führe Connected Components aus...\")\n",
    "print(\"  (Dies kann bei großen Graphen einige Minuten dauern)\")\n",
    "\n",
    "# Checkpoint-Verzeichnis setzen (nötig für Connected Components)\n",
    "spark.sparkContext.setCheckpointDir(str(output_dir / \"checkpoints\"))\n",
    "\n",
    "# Connected Components berechnen\n",
    "entities_df = graph.connectedComponents()\n",
    "entities_df.cache()\n",
    "\n",
    "print(\"\\nConnected Components berechnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Statistiken\n",
    "entity_count = entities_df.select(\"component\").distinct().count()\n",
    "address_count = entities_df.count()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTITY CLUSTERING ERGEBNIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdressen analysiert: {address_count:,}\")\n",
    "print(f\"Entities gefunden: {entity_count:,}\")\n",
    "print(f\"Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\n",
    "print(f\"\\n→ {address_count:,} Adressen wurden zu {entity_count:,} Entities gruppiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-Größen analysieren\n",
    "entity_sizes = entities_df \\\n",
    "    .groupBy(\"component\") \\\n",
    "    .agg(count(\"*\").alias(\"address_count\")) \\\n",
    "    .orderBy(col(\"address_count\").desc())\n",
    "\n",
    "print(\"Top 10 größte Entities:\")\n",
    "entity_sizes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Entity-Größen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ============================================================================\n",
    "# Chart 1: Normale Skala (zeigt das Problem)\n",
    "# ============================================================================\n",
    "axes[0].hist(size_dist['address_count'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Anzahl Adressen pro Entity')\n",
    "axes[0].set_ylabel('Anzahl Entities')\n",
    "axes[0].set_title('Problem: Normale Skala')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Annotation für das Problem\n",
    "axes[0].text(0.95, 0.95, \n",
    "             'Die meisten Entities (1-2 Adressen)\\nsind als riesiger Balken links,\\nder Rest ist unsichtbar!',\n",
    "             transform=axes[0].transAxes, fontsize=9,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "             color='red')\n",
    "\n",
    "# ============================================================================\n",
    "# Chart 2: Logarithmische Skala (Lösung)\n",
    "# ============================================================================\n",
    "axes[1].hist(size_dist['address_count'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Anzahl Adressen pro Entity')\n",
    "axes[1].set_ylabel('Anzahl Entities')\n",
    "axes[1].set_title('Lösung: Logarithmische Skala')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Lesbare Y-Achse\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "def readable_formatter(x, pos):\n",
    "    if x >= 1000:\n",
    "        return f'{int(x/1000)}k'\n",
    "    return f'{int(x)}'\n",
    "axes[1].yaxis.set_major_formatter(FuncFormatter(readable_formatter))\n",
    "\n",
    "# Erklärung hinzufügen\n",
    "axes[1].text(0.95, 0.95, \n",
    "             'Log-Skala macht alle Werte sichtbar:\\n100 → 1000 → 10000 haben\\ngleiche Abstände auf der Y-Achse',\n",
    "             transform=axes[1].transAxes, fontsize=9,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# ============================================================================\n",
    "# Chart 3: Top 20 größte Entities\n",
    "# ============================================================================\n",
    "top_20 = size_dist.head(20)\n",
    "bars = axes[2].barh(range(len(top_20)), top_20['address_count'], color='darkorange')\n",
    "axes[2].set_xlabel('Anzahl Adressen')\n",
    "axes[2].set_ylabel('Entity Rang')\n",
    "axes[2].set_title('Top 20: Die größten Entities')\n",
    "axes[2].set_yticks(range(len(top_20)))\n",
    "axes[2].set_yticklabels([f\"#{i+1}\" for i in range(len(top_20))])\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Werte an die Balken schreiben\n",
    "for bar, count in zip(bars, top_20['address_count']):\n",
    "    axes[2].text(bar.get_width() + max(top_20['address_count'])*0.01, \n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             f'{int(count):,}', \n",
    "             va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nErklärung zur logarithmischen Skala:\")\n",
    "print(\"  - Normale Skala: Der dominante Wert (Entities mit 1-2 Adressen) erdrückt alles andere\")\n",
    "print(\"  - Log-Skala: Verhältnisse bleiben erhalten, aber alle Werte werden sichtbar\")\n",
    "print(\"  - Faustregel: Bei Power-Law-Verteilungen immer Log-Skala verwenden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities speichern\n",
    "print(\"Speichere Entity-Mapping...\")\n",
    "\n",
    "# Umbenennen für Klarheit\n",
    "entities_final = entities_df \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"address\"),\n",
    "        col(\"component\").alias(\"entity_id\")\n",
    "    )\n",
    "\n",
    "entities_final.write.mode(\"overwrite\").parquet(str(output_dir / \"entities.parquet\"))\n",
    "\n",
    "print(f\"Gespeichert: {output_dir / 'entities.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.4 Interpretation der Ergebnisse\n\n### Was bedeutet die Reduktion konkret?\n\n```\nVorher: ~200.000 individuelle Adressen\nNachher: ~150.000 Entities (Cluster)\n─────────────────────────────────────\nReduktion: ~25% der Adressen konnten gruppiert werden\n```\n\n**Interpretation**: Nur 25% Reduktion scheint gering, aber:\n- Die restlichen 75% sind **Single-Adress-Entities** (Adressen die nie mit anderen kombiniert wurden)\n- Diese könnten Einmal-Empfangsadressen, Cold Storage oder sehr private Nutzer sein\n- In späteren Zeiträumen (2015+) steigt die Reduktionsrate auf 40-60% durch häufigere Wiederverwendung\n\n### Was sind die größten Entities wahrscheinlich?\n\n| Entity-Größe | Wahrscheinliche Identität | Begründung |\n|--------------|---------------------------|------------|\n| >10.000 Adressen | **Exchanges** (Börsen) | Sammeln viele Einzahlungsadressen, konsolidieren regelmäßig |\n| 1.000-10.000 Adressen | **Mining Pools** | Auszahlungen an viele Miner, gemeinsame Hot-Wallets |\n| 100-1.000 Adressen | **Große Händler/Services** | Zahlungsprozessoren, Shops, Gambling-Sites |\n| 10-100 Adressen | **Aktive Nutzer/Trader** | Normale Wallet-Nutzung mit Address-Rotation |\n| 1-10 Adressen | **Gelegenheitsnutzer** | Wenige Transaktionen, einfache Wallets |\n\n**Vorsicht bei 2011-Daten**: Zu dieser Zeit gab es noch kaum große Exchanges (Mt. Gox dominierte). Große Entities könnten auch Early-Adopter-Wallets oder die Wallets der Bitcoin-Entwickler selbst sein.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Teil V: Whale Detection [GEPLANT]\n\nBerechnung der Entity-Balances aus dem UTXO-Set und Identifikation von Whales (Entities mit hohem Bitcoin-Bestand)."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Teil VI: Verhaltensanalyse [GEPLANT]\n\nZeitreihen-Analyse der Whale-Aktivitaeten: Akkumulation vs. Distribution, Haltezeiten und Transaktionsmuster."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Teil VII: Zusammenfassung und Ausblick\n\n## 7.1 Erreichte Ergebnisse\n\n### Pipeline-Status\n\n| Schritt | Status | Output |\n|---------|--------|--------|\n| Daten laden | Fertig | tx_df, blocks_df |\n| JSON zu Parquet | Fertig | outputs.parquet, inputs.parquet |\n| UTXO Set | Fertig | utxos.parquet |\n| Entity Clustering | Fertig | entities.parquet |\n| Whale Detection | Geplant | - |\n| Verhaltensanalyse | Geplant | - |\n\n### Erzeugte Daten\n\nDie Pipeline hat folgende Parquet-Dateien erzeugt:\n\n| Datei | Inhalt | Verwendung |\n|-------|--------|------------|\n| `outputs.parquet` | Alle Transaction Outputs (flach) | Basis fuer UTXO-Berechnung |\n| `inputs.parquet` | Alle Inputs mit Spent-Referenzen | UTXO-Berechnung, Clustering |\n| `utxos.parquet` | Unspent Outputs | Balance-Berechnung |\n| `entities.parquet` | Address zu Entity Mapping | Whale Detection |\n\n## 7.2 Limitationen\n\n| Limitation | Beschreibung | Moegliche Loesung |\n|------------|--------------|-------------------|\n| **Teil-Export** | Nur H1 2011, nicht vollstaendige Blockchain | Vollstaendigen Export mit bitcoin-etl erstellen |\n| **Heuristik-Grenzen** | CoinJoin/Exchanges koennen falsche Cluster erzeugen | Zusaetzliche Heuristiken (Change Detection, etc.) |\n| **Keine Labels** | Keine bekannten Entitaeten markiert | Integration von Chainalysis/Elliptic Daten |\n\n## 7.3 Naechste Schritte\n\n1. **Teil V implementieren**: Entity-Balances berechnen, Whales identifizieren\n2. **Teil VI implementieren**: Zeitreihen-Analyse, Verhaltensmetriken\n3. **Visualisierung**: Dashboard fuer Whale-Tracking\n4. **Skalierung**: Pipeline auf vollstaendige Blockchain anwenden"
  },
  {
   "cell_type": "code",
   "source": "# Finale Statistiken\nprint(\"=\"*60)\nprint(\"PIPELINE ABGESCHLOSSEN\")\nprint(\"=\"*60)\nprint(f\"\\nDatenquelle: {BLOCKCHAIN_DATA_PATH}\")\nprint(f\"Ausgabe: {OUTPUT_PATH}\")\nprint(f\"\\nVerarbeitete Daten:\")\nprint(f\"  Transaktionen: {tx_count:,}\")\nprint(f\"  Blocks: {block_count:,}\")\nprint(f\"  Outputs: {output_count:,}\")\nprint(f\"  UTXOs: {utxo_count:,}\")\nprint(f\"\\nEntity Clustering:\")\nprint(f\"  Adressen: {address_count:,}\")\nprint(f\"  Entities: {entity_count:,}\")\nprint(f\"  Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\nprint(f\"\\nErzeugte Parquet-Dateien:\")\nfor f in Path(OUTPUT_PATH).glob(\"*.parquet\"):\n    print(f\"  - {f.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Spark-Session beenden (optional - auskommentiert fuer weitere Analysen)\n# spark.stop()\n# print(\"Spark-Session beendet.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}