{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bitcoin Whale Intelligence: VollstÃ¤ndige Analyse-Pipeline\n\nDieses Notebook fÃ¼hrt die vollstÃ¤ndige Pipeline zur Identifikation und Analyse von Bitcoin-Whales durch.\n\n---\n\n## InhaltsÃ¼bersicht\n\n| Teil | Thema | Status |\n|------|-------|--------|\n| **I** | EinfÃ¼hrung und Kontext | âœ… Fertig |\n| **II** | Datenverarbeitung (ETL) | âœ… Fertig |\n| **III** | UTXO-Analyse | âœ… Fertig |\n| **IV** | Entity Clustering | âœ… Fertig |\n| **V** | Whale Detection | ðŸ”œ Geplant |\n| **VI** | Verhaltensanalyse | ðŸ”œ Geplant |\n| **VII** | Zusammenfassung und Ausblick | âœ… Fertig |\n\n---\n\n# Teil I: EinfÃ¼hrung und Kontext\n\n## 1.1 Projektziel\n\nDieses Projekt analysiert die Bitcoin-Blockchain um \"Whales\" (groÃŸe Halter) zu identifizieren und ihr Verhalten zu verstehen. Die Pipeline umfasst:\n\n1. **Daten laden**: Bitcoin-Blockchain-Daten aus bitcoin-etl JSON Export\n2. **Daten transformieren**: JSON â†’ optimiertes Parquet-Format\n3. **UTXO-Set berechnen**: Unspent Transaction Outputs identifizieren\n4. **Entity Clustering**: Adressen zu Entities gruppieren\n5. **Whale Detection**: GroÃŸe Halter identifizieren *(geplant)*\n6. **Verhaltensanalyse**: Akkumulation vs. Distribution analysieren *(geplant)*\n\n### Datenquelle\n\nDie Daten wurden mit [bitcoin-etl](https://github.com/blockchain-etl/bitcoin-etl) von einem Bitcoin Full Node exportiert:\n\n```bash\nbitcoinetl export_all \\\n    --provider-uri http://user:pass@localhost:8332 \\\n    --start YYYY-MM-DD --end YYYY-MM-DD \\\n    --output-dir /path/to/blockchain_exports\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Bitcoin-Grundkonzepte\n\n### Das UTXO-Modell\n\nBitcoin verwendet im Gegensatz zu Account-basierten Systemen (wie Bankkonten) ein **UTXO-Modell** (Unspent Transaction Output). Hier werden \"MÃ¼nzen\" vollstÃ¤ndig ausgegeben und Wechselgeld zurÃ¼ckgegeben.\n\n| Account-basiert (Bank) | UTXO-basiert (Bitcoin) |\n|------------------------|------------------------|\n| Konto hat Kontostand: 100 EUR | Besitz von \"MÃ¼nzen\" verschiedener GrÃ¶ÃŸe |\n| Ãœberweisung reduziert Kontostand | MÃ¼nzen werden **vollstÃ¤ndig** ausgegeben |\n| Einfache Subtraktion | Wechselgeld als neue UTXO zurÃ¼ck |\n\n### Bitcoin-Adresse vs. Entity\n\n| Konzept | Beschreibung | Sichtbar in Blockchain? |\n|---------|--------------|-------------------------|\n| **Adresse** | Einzelner \"Briefkasten\" fÃ¼r Bitcoin (z.B. `bc1q...`) | Ja |\n| **Wallet** | Software die viele Adressen verwaltet | Nein |\n| **Entity** | Person/Firma die ein oder mehrere Wallets besitzt | Nein |\n\n### Das zentrale Problem\n\n```\n800 Millionen Bitcoin-Adressen existieren\n         â†“\nWer besitzt sie?\n         â†“\nEine Person kann 1,000+ Adressen haben\nEine BÃ¶rse kann 5,000,000+ Adressen haben\n         â†“\nDie Blockchain zeigt NICHT welche Adressen zusammengehÃ¶ren!\n```\n\n**Ziel dieses Projekts**: Adressen zu Entities gruppieren durch Analyse der Transaktionsmuster, dann groÃŸe Entities (\"Whales\") identifizieren."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Glossar: Wichtige Begriffe\n\n| Begriff | ErklÃ¤rung |\n|---------|----------|\n| **Coinbase-Transaktion** | Die erste Transaktion in jedem Block. EnthÃ¤lt die Mining-Belohnung und hat keine Inputs - das neu geschÃ¼rfte Bitcoin entsteht hier \"aus dem Nichts\". |\n| **Satoshi** | Kleinste Bitcoin-Einheit. 1 BTC = 100.000.000 Satoshi. Benannt nach dem Bitcoin-Erfinder Satoshi Nakamoto. Werte in der Blockchain sind immer in Satoshi angegeben. |\n| **Parquet** | Spaltenorientiertes BinÃ¤rformat fÃ¼r Big Data. Vorteile: hohe Kompression (70-90%), schnelles Lesen einzelner Spalten, optimiert fÃ¼r analytische Queries. |\n| **GraphFrames** | Apache Spark Bibliothek fÃ¼r Graph-Algorithmen. ErmÃ¶glicht verteilte Berechnung auf Graphen mit Millionen/Milliarden Knoten. |\n| **Connected Components** | Graph-Algorithmus der zusammenhÃ¤ngende Teilgraphen findet. Alle Knoten die direkt oder transitiv verbunden sind, erhalten dieselbe Komponenten-ID. |\n| **Hive-Partitionierung** | Ordnerstruktur zur Datenorganisation nach SchlÃ¼sseln (z.B. `date=2011-01-01/`). ErmÃ¶glicht effizientes Filtern ohne alle Daten zu lesen. |\n| **Whale** | Umgangssprachlich fÃ¼r Bitcoin-Halter mit sehr groÃŸen BestÃ¤nden (typischerweise >1.000 BTC). |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Teil II: Datenverarbeitung (ETL)\n\n## 2.1 Setup und Konfiguration"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projektverzeichnis: /Users/roman/spark_project/bitcoin-whale-intelligence\n",
      "Datenquelle: /Users/roman/spark_project/blockchain_exports\n",
      "Ausgabe: /Users/roman/spark_project/bitcoin-whale-intelligence/data\n"
     ]
    }
   ],
   "source": [
    "# Standard-Bibliotheken\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Projektverzeichnis ermitteln und zum Path hinzufÃ¼gen\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Projektverzeichnis: {project_root}\")\n",
    "\n",
    "# ============================================================================\n",
    "# KONFIGURATION - HIER ANPASSEN\n",
    "# ============================================================================\n",
    "\n",
    "# Pfad zu den bitcoin-etl exportierten Daten\n",
    "BLOCKCHAIN_DATA_PATH = \"/Users/roman/spark_project/blockchain_exports\"\n",
    "\n",
    "# Ausgabeverzeichnis fÃ¼r Parquet-Dateien\n",
    "OUTPUT_PATH = str(project_root / \"data\")\n",
    "\n",
    "# Spark-Konfiguration\n",
    "DRIVER_MEMORY = \"8g\"  # ErhÃ¶hen fÃ¼r grÃ¶ÃŸere Datasets\n",
    "\n",
    "print(f\"Datenquelle: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Ausgabe: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "# ETL-Modul importieren\n",
    "from src.etl import (\n",
    "    create_spark_session,\n",
    "    load_transactions,\n",
    "    load_blocks,\n",
    "    explode_outputs,\n",
    "    explode_inputs,\n",
    "    compute_utxo_set,\n",
    "    enrich_clustering_inputs,\n",
    ")\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Module erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.5.7-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/roman/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/roman/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f2aee8ca-4158-48b4-92bc-1ace7d43942e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.3-spark3.5-s_2.12!graphframes.jar (210ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (138ms)\n",
      ":: resolution report :: resolve 1946ms :: artifacts dl 355ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f2aee8ca-4158-48b4-92bc-1ace7d43942e\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (280kB/13ms)\n",
      "25/12/08 18:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.7\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Spark-Session erstellen\n",
    "spark = create_spark_session(\n",
    "    app_name=\"Bitcoin Whale Intelligence\",\n",
    "    driver_memory=DRIVER_MEMORY,\n",
    "    enable_graphframes=True\n",
    ")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Daten laden\n\nDie bitcoin-etl Daten liegen als Hive-partitionierte JSON-Dateien vor:\n\n```\nblockchain_exports/\nâ””â”€â”€ 2011-01-01_2011-06-01/\n    â”œâ”€â”€ blocks/\n    â”‚   â””â”€â”€ date=YYYY-MM-DD/\n    â”‚       â””â”€â”€ blocks_*.json\n    â””â”€â”€ transactions/\n        â””â”€â”€ date=YYYY-MM-DD/\n            â””â”€â”€ transactions_*.json\n```\n\n**Wichtig**: Die Transaktionen enthalten **nested Arrays** fÃ¼r inputs und outputs. Dies ist anders als bei normalisierten Datenbanken (z.B. BigQuery) wo diese in separaten Tabellen liegen."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Transaktionen...\n",
      "Lade Blocks...\n",
      "\n",
      "Daten geladen und gecacht.\n"
     ]
    }
   ],
   "source": [
    "# Transaktionen und Blocks laden\n",
    "print(\"Lade Transaktionen...\")\n",
    "tx_df = load_transactions(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "print(\"Lade Blocks...\")\n",
    "blocks_df = load_blocks(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "# Cache fÃ¼r wiederholten Zugriff\n",
    "tx_df.cache()\n",
    "blocks_df.cache()\n",
    "\n",
    "print(\"\\nDaten geladen und gecacht.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladene Daten:\n",
      "  Transaktionen: 382,402\n",
      "  Blocks: 27,644\n"
     ]
    }
   ],
   "source": [
    "# Grundstatistiken\n",
    "tx_count = tx_df.count()\n",
    "block_count = blocks_df.count()\n",
    "\n",
    "print(f\"Geladene Daten:\")\n",
    "print(f\"  Transaktionen: {tx_count:,}\")\n",
    "print(f\"  Blocks: {block_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenumfang und Einordnung\n",
    "\n",
    "**Zeitraum dieser Daten:** Januar bis Juni 2011 (frÃ¼he Bitcoin-Geschichte)\n",
    "\n",
    "| Zeitpunkt | Transaktionen | Blocks | Kontext |\n",
    "|-----------|---------------|--------|--------|\n",
    "| **Diese Daten (H1 2011)** | ~380.000 | ~27.000 | Bitcoin war noch jung, wenig Nutzer |\n",
    "| VollstÃ¤ndige Blockchain 2024 | >900.000.000 | >800.000 | ~2.400x mehr Transaktionen |\n",
    "\n",
    "**Zweck**: Diese Daten dienen als Testdatensatz zum Validieren der Pipeline. Die Algorithmen funktionieren identisch auf der vollstÃ¤ndigen Blockchain - nur die Rechenzeit und Speicheranforderungen steigen.\n",
    "\n",
    "**Historischer Kontext**: Im Jahr 2011 war 1 BTC zwischen $0.30 und $30 wert. Die grÃ¶ÃŸten \"Whales\" dieser Zeit waren oft Early Adopter und Mining-Enthusiasten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Schema (Auszug):\n",
      "root\n",
      " |-- hash: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      " |-- virtual_size: integer (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- lock_time: long (nullable = true)\n",
      " |-- block_number: long (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- block_timestamp: long (nullable = true)\n",
      " |-- is_coinbase: boolean (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- inputs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- index: integer (nullable = true)\n",
      " |    |    |-- spent_transaction_hash: string (nullable = true)\n",
      " |    |    |-- spent_output_index: integer (nullable = true)\n",
      " |    |    |-- script_asm: string (nullable = true)\n",
      " |    |    |-- script_hex: string (nullable = true)\n",
      " |    |    |-- sequence: long (nullable = true)\n",
      " |    |    |-- required_signatures: integer (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- addresses: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- outputs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- index: integer (nullable = true)\n",
      " |    |    |-- script_asm: string (nullable = true)\n",
      " |    |    |-- script_hex: string (nullable = true)\n",
      " |    |    |-- required_signatures: integer (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- addresses: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- input_count: integer (nullable = true)\n",
      " |-- output_count: integer (nullable = true)\n",
      " |-- input_value: long (nullable = true)\n",
      " |-- output_value: long (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- block_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema anzeigen\n",
    "print(\"Transaction Schema (Auszug):\")\n",
    "tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel-Transaktion:\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "|                hash|block_number|input_count|output_count|is_coinbase|output_value|        fee|\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "|630bb912bea097180...|      126766|          0|           1|       true|  5000000000|          0|\n",
      "|0556dd5dba67f4476...|      126766|          2|           2|      false|  2128000000|-2128000000|\n",
      "|7a41ec18684517921...|      126766|          1|           1|      false|    50000000|  -50000000|\n",
      "|c0b929b9c6abdecb3...|      126766|          1|           1|      false|   100000000| -100000000|\n",
      "|4cedd988b1b9018c1...|      126765|          0|           1|       true|  5002000000|          0|\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-Transaktion anzeigen\n",
    "print(\"Beispiel-Transaktion:\")\n",
    "tx_df.select(\n",
    "    \"hash\", \"block_number\", \"input_count\", \"output_count\", \n",
    "    \"is_coinbase\", \"output_value\", \"fee\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Teil III: UTXO-Analyse\n\n## 3.1 Das UTXO-Modell im Detail\n\n### Praktisches Beispiel\n\nAlice besitzt zwei UTXOs:\n- **UTXO A**: 0.5 BTC (auf Adresse A1)\n- **UTXO B**: 0.3 BTC (auf Adresse A2)\n- **Gesamt**: 0.8 BTC\n\nAlice mÃ¶chte **0.7 BTC** an Bob senden:\n\n```\nINPUTS (was Alice ausgibt):        OUTPUTS (was erstellt wird):\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ UTXO A: 0.5 BTC     â”‚            â”‚ An Bob: 0.7 BTC     â”‚\nâ”‚ (Adresse A1)        â”‚   â”€â”€â”€â–º     â”‚ (neue UTXO fÃ¼r Bob) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ UTXO B: 0.3 BTC     â”‚            â”‚ Wechselgeld: 0.09   â”‚\nâ”‚ (Adresse A2)        â”‚            â”‚ (neue UTXO fÃ¼r      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  Alice auf A3)      â”‚\n  Summe: 0.8 BTC                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                                   â”‚ Fee: 0.01 BTC       â”‚\n                                   â”‚ (an Miner)          â”‚\n                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     Summe: 0.8 BTC\n```\n\n**Wichtige Erkenntnis fÃ¼r Entity Clustering**: Alice musste **beide Adressen A1 und A2** als Inputs verwenden. DafÃ¼r braucht sie die Private Keys beider Adressen. **â†’ A1 und A2 gehÃ¶ren zur selben Person!** Diese Beobachtung ist die Grundlage der *Common Input Ownership Heuristic*, die wir in Teil IV nutzen."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.2 ETL: Transformation zu Parquet\n\nBevor wir das UTXO Set berechnen, transformieren wir die Daten in ein effizienteres Format:\n\n| JSON (Rohdaten) | Parquet (optimiert) |\n|-----------------|---------------------|\n| Text-basiert, langsam zu parsen | BinÃ¤r, schnell zu lesen |\n| Keine Kompression | Snappy-Kompression (70-90% kleiner) |\n| Liest immer alles | Liest nur benÃ¶tigte Spalten |\n\n### Datenfluss\n\n```\ntransactions.json\n      â”‚\n      â”œâ”€â”€â–º explode_outputs() â”€â”€â–º outputs.parquet\n      â”‚\n      â””â”€â”€â–º explode_inputs()  â”€â”€â–º inputs.parquet (fÃ¼r spent-Referenzen)\n                                       â”‚\n                                       â–¼\n                               UTXO-Berechnung\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 Multi-Input Transaktionen analysieren\n\nBevor wir das UTXO Set berechnen, analysieren wir die Verteilung der Input-Counts. Diese Information ist wichtig fÃ¼r das Entity Clustering in Teil IV."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Count Verteilung (Top 15):\n",
      " input_count  transaction_count\n",
      "           1             287512\n",
      "           2              33911\n",
      "           3              11259\n",
      "           4               6672\n",
      "           5               4027\n",
      "           6               2449\n",
      "           7               1696\n",
      "           8               1186\n",
      "           9                935\n",
      "          10                782\n",
      "          11                487\n",
      "          12                433\n",
      "          13                311\n",
      "          14                267\n",
      "          15                247\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, sum as spark_sum, avg\n",
    "\n",
    "# Input-Count Verteilung\n",
    "input_dist = tx_df \\\n",
    "    .filter(col(\"is_coinbase\") == False) \\\n",
    "    .groupBy(\"input_count\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\")) \\\n",
    "    .orderBy(\"input_count\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"Input-Count Verteilung (Top 15):\")\n",
    "print(input_dist.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-INPUT TRANSACTION STATISTIK\n",
      "============================================================\n",
      "\n",
      "Gesamt (ohne Coinbase): 354,758\n",
      "Single-Input (1 Adresse):   287,512 (81.0%)\n",
      "Multi-Input (â‰¥2 Adressen):  67,246 (19.0%)\n",
      "\n",
      "â†’ 19.0% der Transaktionen sind fÃ¼r Clustering nutzbar\n"
     ]
    }
   ],
   "source": [
    "# Statistiken berechnen\n",
    "total_non_coinbase = tx_df.filter(col(\"is_coinbase\") == False).count()\n",
    "single_input = input_dist[input_dist['input_count'] == 1]['transaction_count'].sum()\n",
    "multi_input = input_dist[input_dist['input_count'] > 1]['transaction_count'].sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-INPUT TRANSACTION STATISTIK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGesamt (ohne Coinbase): {total_non_coinbase:,}\")\n",
    "print(f\"Single-Input (1 Adresse):   {single_input:,} ({single_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"Multi-Input (â‰¥2 Adressen):  {multi_input:,} ({multi_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"\\nâ†’ {multi_input/total_non_coinbase*100:.1f}% der Transaktionen sind fÃ¼r Clustering nutzbar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualisierung\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# ============================================================================\n# Chart 1: Normale Skala (zeigt das Problem)\n# ============================================================================\nplot_data = input_dist[input_dist['input_count'] <= 20]\naxes[0].bar(plot_data['input_count'], plot_data['transaction_count'], \n            color='steelblue', edgecolor='black', linewidth=0.5)\naxes[0].set_xlabel('Anzahl Inputs pro Transaktion')\naxes[0].set_ylabel('Anzahl Transaktionen')\naxes[0].set_title('Problem: 1-Input dominiert alles')\naxes[0].grid(True, alpha=0.3)\n# Annotation\naxes[0].annotate(f'{int(single_input):,}', \n                 xy=(1, single_input), \n                 xytext=(3, single_input * 0.8),\n                 fontsize=9,\n                 arrowprops=dict(arrowstyle='->', color='red'))\naxes[0].text(10, single_input * 0.5, \n             'â† Die anderen Balken\\n    sind kaum sichtbar!', \n             fontsize=9, color='red')\n\n# ============================================================================\n# Chart 2: Logarithmische Skala (macht alle Balken sichtbar)\n# ============================================================================\naxes[1].bar(plot_data['input_count'], plot_data['transaction_count'], \n            color='steelblue', edgecolor='black', linewidth=0.5)\naxes[1].set_xlabel('Anzahl Inputs pro Transaktion')\naxes[1].set_ylabel('Anzahl Transaktionen')\naxes[1].set_title('LÃ¶sung: Logarithmische Skala')\naxes[1].set_yscale('log')\naxes[1].grid(True, alpha=0.3)\n\n# Y-Achse mit lesbaren Zahlen statt 10^x\nfrom matplotlib.ticker import FuncFormatter\ndef readable_formatter(x, pos):\n    if x >= 1000:\n        return f'{int(x/1000)}k'\n    return f'{int(x)}'\naxes[1].yaxis.set_major_formatter(FuncFormatter(readable_formatter))\naxes[1].text(10, 100, 'Jetzt sieht man\\nauch 2, 3, 4+ Inputs', fontsize=9, color='green')\n\n# ============================================================================\n# Chart 3: Pie Chart\n# ============================================================================\nlabels = ['Single-Input\\n(nicht nutzbar)', 'Multi-Input\\n(nutzbar fÃ¼r Clustering)']\nsizes = [single_input, multi_input]\ncolors = ['lightgray', 'steelblue']\naxes[2].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\naxes[2].set_title('Anteil fÃ¼r Clustering')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\nErklÃ¤rung zur logarithmischen Skala:\")\nprint(\"  - Normale Skala: 1-Input-TXs (~290k) dominieren, Rest unsichtbar\")\nprint(\"  - Log-Skala: AbstÃ¤nde werden gestaucht, alle Werte sichtbar\")\nprint(\"  - Beispiel: 100 â†’ 1000 â†’ 10000 haben gleiche AbstÃ¤nde auf der Y-Achse\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Beispiel: Multi-Input Transaktion\n\nEine konkrete Multi-Input-Transaktion zeigt, wie mehrere Adressen in einer Transaktion kombiniert werden:"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel Multi-Input Transaktion:\n",
      "  Hash: a55cfdd8677056d6b0fb...\n",
      "  Block: 126765\n",
      "  Input Count: 4\n",
      "  Output Count: 1\n",
      "  Wert: 20.27000000 BTC\n",
      "\n",
      "  Inputs (Adressen die zusammengehÃ¶ren):\n",
      "    [0] (enrichment nÃ¶tig)\n",
      "    [1] (enrichment nÃ¶tig)\n",
      "    [2] (enrichment nÃ¶tig)\n",
      "    [3] (enrichment nÃ¶tig)\n",
      "\n",
      "  â†’ Diese 4 Adressen gehÃ¶ren zur selben Entity!\n"
     ]
    }
   ],
   "source": [
    "# Eine Multi-Input Transaktion detailliert betrachten\n",
    "example_tx = tx_df \\\n",
    "    .filter(\n",
    "        (col(\"input_count\") >= 3) & \n",
    "        (col(\"input_count\") <= 10) &\n",
    "        (col(\"is_coinbase\") == False)\n",
    "    ) \\\n",
    "    .first()\n",
    "\n",
    "if example_tx:\n",
    "    print(f\"Beispiel Multi-Input Transaktion:\")\n",
    "    print(f\"  Hash: {example_tx['hash'][:20]}...\")\n",
    "    print(f\"  Block: {example_tx['block_number']}\")\n",
    "    print(f\"  Input Count: {example_tx['input_count']}\")\n",
    "    print(f\"  Output Count: {example_tx['output_count']}\")\n",
    "    print(f\"  Wert: {example_tx['output_value'] / 100000000:.8f} BTC\")\n",
    "    \n",
    "    print(f\"\\n  Inputs (Adressen die zusammengehÃ¶ren):\")\n",
    "    for i, inp in enumerate(example_tx['inputs'][:5]):\n",
    "        addr = inp['addresses'][0] if inp['addresses'] else \"(enrichment nÃ¶tig)\"\n",
    "        print(f\"    [{i}] {addr}\")\n",
    "    if len(example_tx['inputs']) > 5:\n",
    "        print(f\"    ... und {len(example_tx['inputs']) - 5} weitere\")\n",
    "    \n",
    "    print(f\"\\n  â†’ Diese {example_tx['input_count']} Adressen gehÃ¶ren zur selben Entity!\")\n",
    "else:\n",
    "    print(\"Keine passende Multi-Input Transaktion gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.4 Outputs und Inputs explodieren"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explodiere Outputs...\n",
      "Outputs: 769,081\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "|                       tx_hash|block_number|block_timestamp|output_index|     value|                     addresses|output_type|\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "|630bb912bea0971803979417615...|      126766|     1306369241|           0|5000000000|[nonstandarde36d71a77f0b72f...|nonstandard|\n",
      "|0556dd5dba67f4476841e530125...|      126766|     1306369241|           0| 128000000|[1N1HR4BPwhP5WvFXF6JCTkRnKj...| pubkeyhash|\n",
      "|0556dd5dba67f4476841e530125...|      126766|     1306369241|           1|2000000000|[16UksCM6jXXR8XGq9cXWiP48P1...| pubkeyhash|\n",
      "|7a41ec18684517921585f710b37...|      126766|     1306369241|           0|  50000000|[1HHETdBA3zrUf9RHoBdgteucMX...| pubkeyhash|\n",
      "|c0b929b9c6abdecb3ebd3857f93...|      126766|     1306369241|           0| 100000000|[12bAvkf2ku6za5XKHwvjgaVvZS...| pubkeyhash|\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outputs explodieren (nested â†’ flat)\n",
    "print(\"Explodiere Outputs...\")\n",
    "outputs_df = explode_outputs(tx_df)\n",
    "outputs_df.cache()\n",
    "\n",
    "output_count = outputs_df.count()\n",
    "print(f\"Outputs: {output_count:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "outputs_df.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explodiere Inputs...\n",
      "Inputs: 632,295\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "|             tx_hash|input_index|       spent_tx_hash|spent_output_index|value|\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "|630bb912bea097180...|       NULL|                NULL|              NULL| NULL|\n",
      "|0556dd5dba67f4476...|          0|3ccb1d88e9f0f8067...|                 1| NULL|\n",
      "|0556dd5dba67f4476...|          1|6f5594a671cd2b686...|                 1| NULL|\n",
      "|7a41ec18684517921...|          0|5259ab3a1d0045aa3...|                 1| NULL|\n",
      "|c0b929b9c6abdecb3...|          0|d683c9078dad5625e...|                22| NULL|\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inputs explodieren\n",
    "print(\"Explodiere Inputs...\")\n",
    "inputs_df = explode_inputs(tx_df)\n",
    "inputs_df.cache()\n",
    "\n",
    "input_count_flat = inputs_df.count()\n",
    "print(f\"Inputs: {input_count_flat:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "inputs_df.select(\n",
    "    \"tx_hash\", \"input_index\", \"spent_tx_hash\", \"spent_output_index\", \"value\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speichere Outputs als Parquet...\n",
      "Speichere Inputs als Parquet...\n",
      "\n",
      "Parquet-Dateien gespeichert in: /Users/roman/spark_project/bitcoin-whale-intelligence/data\n"
     ]
    }
   ],
   "source": [
    "# Als Parquet speichern\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(OUTPUT_PATH)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Speichere Outputs als Parquet...\")\n",
    "outputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"outputs.parquet\"))\n",
    "\n",
    "print(\"Speichere Inputs als Parquet...\")\n",
    "inputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"inputs.parquet\"))\n",
    "\n",
    "print(f\"\\nParquet-Dateien gespeichert in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.5 UTXO Set berechnen\n\nDas **UTXO Set** (Unspent Transaction Output Set) enthÃ¤lt alle Outputs die noch nicht ausgegeben wurden.\n\n### Berechnung\n\n```\nUTXO Set = Alle Outputs MINUS Outputs die als Input referenziert wurden\n\nSQL-Ã„quivalent:\nSELECT * FROM outputs\nWHERE (tx_hash, output_index) NOT IN (\n    SELECT spent_tx_hash, spent_output_index FROM inputs\n)\n```\n\n**Hinweis**: Bei Teil-Exporten (nicht die gesamte Blockchain) kann das UTXO Set unvollstÃ¤ndig sein, da spending-Referenzen aus spÃ¤teren BlÃ¶cken fehlen."
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne UTXO Set...\n",
      "\n",
      "UTXO Statistik:\n",
      "  Gesamt Outputs: 769,081\n",
      "  Spent (ausgegeben): 592,040 (77.0%)\n",
      "  Unspent (UTXOs): 177,041 (23.0%)\n"
     ]
    }
   ],
   "source": [
    "# UTXO Set berechnen\n",
    "print(\"Berechne UTXO Set...\")\n",
    "utxo_df = compute_utxo_set(outputs_df, inputs_df)\n",
    "utxo_df.cache()\n",
    "\n",
    "utxo_count = utxo_df.count()\n",
    "spent_count = output_count - utxo_count\n",
    "\n",
    "print(f\"\\nUTXO Statistik:\")\n",
    "print(f\"  Gesamt Outputs: {output_count:,}\")\n",
    "print(f\"  Spent (ausgegeben): {spent_count:,} ({spent_count/output_count*100:.1f}%)\")\n",
    "print(f\"  Unspent (UTXOs): {utxo_count:,} ({utxo_count/output_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speichere UTXO Set...\n",
      "Gespeichert: /Users/roman/spark_project/bitcoin-whale-intelligence/data/utxos.parquet\n"
     ]
    }
   ],
   "source": [
    "# UTXO Set speichern\n",
    "print(\"Speichere UTXO Set...\")\n",
    "utxo_df.write.mode(\"overwrite\").parquet(str(output_dir / \"utxos.parquet\"))\n",
    "print(f\"Gespeichert: {output_dir / 'utxos.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Teil IV: Entity Clustering\n\n## 4.1 Common Input Ownership Heuristic\n\n### Die Heuristik\n\n**Beobachtung**: Wenn eine Transaktion mehrere Adressen als Inputs verwendet, mÃ¼ssen alle diese Adressen von derselben Person kontrolliert werden.\n\n**Warum?** Um eine Bitcoin-Transaktion zu signieren, braucht man die Private Keys **aller** Input-Adressen. Nur wer alle Keys besitzt, kann die Transaktion erstellen.\n\n### Transitive VerknÃ¼pfung\n\n```\nTransaction 1: Inputs von A1 + A2  â†’  A1 und A2 gehÃ¶ren zusammen\nTransaction 2: Inputs von A2 + A3  â†’  A2 und A3 gehÃ¶ren zusammen\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSchlussfolgerung: A1, A2, A3 gehÃ¶ren alle zur selben Person!\n\nGraph-Darstellung:\nâ”Œâ”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”\nâ”‚ A1 â”‚ â”€â”€tx1â”€â”€ â”‚ A2 â”‚ â”€â”€tx2â”€â”€ â”‚ A3 â”‚\nâ””â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”˜\n          â†“\n  Connected Component\n          â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Entity 1: {A1, A2, A3}         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### EinschrÃ¤nkungen\n\nDie Heuristik funktioniert **nicht** bei:\n\n1. **Exchange-Transaktionen**: BÃ¶rsen bÃ¼ndeln Auszahlungen vieler Nutzer\n   - **LÃ¶sung**: Filter `input_count > 50` â†’ wahrscheinlich Exchange\n\n2. **CoinJoin**: Privacy-Protokoll das Transaktionen mehrerer Nutzer mischt\n   - **LÃ¶sung**: Pattern-Erkennung (alle Outputs gleich groÃŸ)\n\n3. **Mining-Pools**: Batch-Auszahlungen an Miner\n   - **LÃ¶sung**: Bekannte Pool-Adressen filtern"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4.2 Graph aufbauen\n\nZuerst reichern wir die Multi-Input-Transaktionen mit Adressen an:"
  },
  {
   "cell_type": "code",
   "source": "# Inputs fÃ¼r Clustering anreichern\nprint(\"Reichere Multi-Input-Transaktionen mit Adressen an...\")\nprint(\"  (Filter: 2-50 Inputs, keine Coinbase)\")\n\nclustering_inputs = enrich_clustering_inputs(\n    tx_df, \n    outputs_df,\n    min_inputs=2,\n    max_inputs=50\n)\nclustering_inputs.cache()\n\nenriched_count = clustering_inputs.count()\nprint(f\"\\nAngereicherte Inputs: {enriched_count:,}\")\n\n# Beispiel\nprint(\"\\nBeispiel (tx_hash -> address):\")\nclustering_inputs.show(10, truncate=40)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaktionen mit â‰¥2 Adressen: 57,606\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, size as spark_size\n",
    "from itertools import combinations\n",
    "\n",
    "# Adressen pro Transaktion gruppieren\n",
    "tx_addresses = clustering_inputs \\\n",
    "    .groupBy(\"tx_hash\") \\\n",
    "    .agg(collect_set(\"address\").alias(\"addresses\"))\n",
    "\n",
    "# Filtern: Nur TXs mit mindestens 2 verschiedenen Adressen\n",
    "tx_addresses = tx_addresses.filter(spark_size(\"addresses\") >= 2)\n",
    "\n",
    "tx_with_addresses = tx_addresses.count()\n",
    "print(f\"Transaktionen mit â‰¥2 Adressen: {tx_with_addresses:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-Kanten (unique): 400,872\n",
      "+--------------------+--------------------+\n",
      "|                 src|                 dst|\n",
      "+--------------------+--------------------+\n",
      "|192nJoWgPuc3sKFQB...|1Q3fg95C1TcqXS1Xc...|\n",
      "|1HL2U2Cz1tDkh52kJ...|1JwfvhMrphhVSZwNk...|\n",
      "|1KzyYTFAQF6Q5SAMR...|1LaqzJryo46Z4Tmof...|\n",
      "|1CPxeZnow3C3EU6Fr...|1LLaxfmyGB393VkGD...|\n",
      "|1B5fVpnS87hzioWQ3...|1CSzzf96C7fvuLYYu...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode as spark_explode, arrays_zip, transform, struct\n",
    "\n",
    "# Kanten erstellen: Alle Adresspaare pro Transaktion\n",
    "# FÃ¼r jede TX mit Adressen [A, B, C] erstellen wir Kanten: (A,B), (A,C), (B,C)\n",
    "\n",
    "def create_edges_udf(addresses):\n",
    "    \"\"\"Erstellt alle Paare aus einer Liste von Adressen.\"\"\"\n",
    "    if not addresses or len(addresses) < 2:\n",
    "        return []\n",
    "    return [(a, b) for a, b in combinations(sorted(addresses), 2)]\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "edge_schema = ArrayType(StructType([\n",
    "    StructField(\"src\", StringType()),\n",
    "    StructField(\"dst\", StringType())\n",
    "]))\n",
    "\n",
    "create_edges = udf(create_edges_udf, edge_schema)\n",
    "\n",
    "# Kanten erstellen und explodieren\n",
    "edges_df = tx_addresses \\\n",
    "    .withColumn(\"edges\", create_edges(\"addresses\")) \\\n",
    "    .select(spark_explode(\"edges\").alias(\"edge\")) \\\n",
    "    .select(\n",
    "        col(\"edge.src\").alias(\"src\"),\n",
    "        col(\"edge.dst\").alias(\"dst\")\n",
    "    ) \\\n",
    "    .distinct()\n",
    "\n",
    "edges_df.cache()\n",
    "edge_count = edges_df.count()\n",
    "\n",
    "print(f\"Graph-Kanten (unique): {edge_count:,}\")\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-Knoten (Adressen): 147,907\n"
     ]
    }
   ],
   "source": [
    "# Vertices (alle eindeutigen Adressen)\n",
    "vertices_src = edges_df.select(col(\"src\").alias(\"id\"))\n",
    "vertices_dst = edges_df.select(col(\"dst\").alias(\"id\"))\n",
    "vertices_df = vertices_src.union(vertices_dst).distinct()\n",
    "\n",
    "vertex_count = vertices_df.count()\n",
    "print(f\"Graph-Knoten (Adressen): {vertex_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.3 Connected Components mit GraphFrames\n\n### Algorithmus-Ãœberblick\n\nWir modellieren das Problem als **Graph**:\n\n- **Knoten (Vertices)**: Bitcoin-Adressen\n- **Kanten (Edges)**: Zwei Adressen die zusammen als Inputs einer Transaktion erscheinen\n\nDer **Connected Components**-Algorithmus findet alle zusammenhÃ¤ngenden Teilgraphen. Jeder Teilgraph ist eine **Entity**.\n\n```\nBeispiel-Graph:\n\n  A1 â”€â”€â”€ A2 â”€â”€â”€ A3      B1 â”€â”€â”€ B2\n   â”‚           â”‚                â”‚\n   â””â”€â”€â”€ A4 â”€â”€â”€â”€â”˜               B3\n\nErgebnis:\n  Entity 1: {A1, A2, A3, A4}\n  Entity 2: {B1, B2, B3}\n```\n\n### Warum Checkpointing nÃ¶tig ist\n\n**Problem**: Der Connected Components Algorithmus ist iterativ - er durchlÃ¤uft den Graphen mehrfach, bis sich keine Zuordnungen mehr Ã¤ndern. Bei jedem Durchlauf wird der Spark-AusfÃ¼hrungsplan lÃ¤nger:\n\n```\nIteration 1: Plan mit 10 Operationen\nIteration 2: Plan mit 20 Operationen (baut auf Iteration 1 auf)\n...\nIteration N: Plan mit N*10 Operationen â†’ Stack Overflow!\n```\n\n**LÃ¶sung - Checkpointing**: Spark speichert Zwischenergebnisse auf Festplatte und \"vergisst\" den bisherigen Plan. **Ohne Checkpointing funktioniert Connected Components nur fÃ¼r sehr kleine Graphen.**"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle Graph...\n",
      "FÃ¼hre Connected Components aus...\n",
      "  (Dies kann bei groÃŸen Graphen einige Minuten dauern)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o365.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOfRange(Arrays.java:4030)\n\tat java.base/java.lang.StringLatin1.newString(StringLatin1.java:715)\n\tat java.base/java.lang.StringBuilder.toString(StringBuilder.java:452)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:64)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:780)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2994/0x00000008012da040.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2963/0x00000008012c5440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda$6405/0x0000000801ca2840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda$2958/0x00000008012c2840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda$2817/0x000000080123b040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2825/0x000000080123fc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2818/0x000000080123b440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m spark.sparkContext.setCheckpointDir(\u001b[38;5;28mstr\u001b[39m(output_dir / \u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Connected Components berechnen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m entities_df = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m entities_df.cache()\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConnected Components berechnet.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/private/var/folders/nx/qs47_8zn2jsdkcz32b7cr8_r0000gn/T/spark-779e2879-97c0-498b-b0df-01c810819b29/userFiles-19438abe-2204-40f0-bd09-08031ce4346b/graphframes_graphframes-0.8.3-spark3.5-s_2.12.jar/graphframes/graphframe.py:331\u001b[39m, in \u001b[36mGraphFrame.connectedComponents\u001b[39m\u001b[34m(self, algorithm, checkpointInterval, broadcastThreshold)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnectedComponents\u001b[39m(\u001b[38;5;28mself\u001b[39m, algorithm = \u001b[33m\"\u001b[39m\u001b[33mgraphframes\u001b[39m\u001b[33m\"\u001b[39m, checkpointInterval = \u001b[32m2\u001b[39m,\n\u001b[32m    313\u001b[39m                         broadcastThreshold = \u001b[32m1000000\u001b[39m):\n\u001b[32m    314\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    Computes the connected components of the graph.\u001b[39;00m\n\u001b[32m    316\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m \u001b[33;03m    :return: DataFrame with new vertices column \"component\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetAlgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetCheckpointInterval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpointInterval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetBroadcastThreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcastThreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m._spark)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o365.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOfRange(Arrays.java:4030)\n\tat java.base/java.lang.StringLatin1.newString(StringLatin1.java:715)\n\tat java.base/java.lang.StringBuilder.toString(StringBuilder.java:452)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:64)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:780)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2994/0x00000008012da040.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2963/0x00000008012c5440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda$6405/0x0000000801ca2840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda$2958/0x00000008012c2840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda$2817/0x000000080123b040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2825/0x000000080123fc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2818/0x000000080123b440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n"
     ]
    }
   ],
   "source": [
    "# GraphFrames Connected Components\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "print(\"Erstelle Graph...\")\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "print(\"FÃ¼hre Connected Components aus...\")\n",
    "print(\"  (Dies kann bei groÃŸen Graphen einige Minuten dauern)\")\n",
    "\n",
    "# Checkpoint-Verzeichnis setzen (nÃ¶tig fÃ¼r Connected Components)\n",
    "spark.sparkContext.setCheckpointDir(str(output_dir / \"checkpoints\"))\n",
    "\n",
    "# Connected Components berechnen\n",
    "entities_df = graph.connectedComponents()\n",
    "entities_df.cache()\n",
    "\n",
    "print(\"\\nConnected Components berechnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Statistiken\n",
    "entity_count = entities_df.select(\"component\").distinct().count()\n",
    "address_count = entities_df.count()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTITY CLUSTERING ERGEBNIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdressen analysiert: {address_count:,}\")\n",
    "print(f\"Entities gefunden: {entity_count:,}\")\n",
    "print(f\"Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\n",
    "print(f\"\\nâ†’ {address_count:,} Adressen wurden zu {entity_count:,} Entities gruppiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-GrÃ¶ÃŸen analysieren\n",
    "entity_sizes = entities_df \\\n",
    "    .groupBy(\"component\") \\\n",
    "    .agg(count(\"*\").alias(\"address_count\")) \\\n",
    "    .orderBy(col(\"address_count\").desc())\n",
    "\n",
    "print(\"Top 10 grÃ¶ÃŸte Entities:\")\n",
    "entity_sizes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Entity-GrÃ¶ÃŸen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ============================================================================\n",
    "# Chart 1: Normale Skala (zeigt das Problem)\n",
    "# ============================================================================\n",
    "axes[0].hist(size_dist['address_count'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Anzahl Adressen pro Entity')\n",
    "axes[0].set_ylabel('Anzahl Entities')\n",
    "axes[0].set_title('Problem: Normale Skala')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Annotation fÃ¼r das Problem\n",
    "axes[0].text(0.95, 0.95, \n",
    "             'Die meisten Entities (1-2 Adressen)\\nsind als riesiger Balken links,\\nder Rest ist unsichtbar!',\n",
    "             transform=axes[0].transAxes, fontsize=9,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "             color='red')\n",
    "\n",
    "# ============================================================================\n",
    "# Chart 2: Logarithmische Skala (LÃ¶sung)\n",
    "# ============================================================================\n",
    "axes[1].hist(size_dist['address_count'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Anzahl Adressen pro Entity')\n",
    "axes[1].set_ylabel('Anzahl Entities')\n",
    "axes[1].set_title('LÃ¶sung: Logarithmische Skala')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Lesbare Y-Achse\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "def readable_formatter(x, pos):\n",
    "    if x >= 1000:\n",
    "        return f'{int(x/1000)}k'\n",
    "    return f'{int(x)}'\n",
    "axes[1].yaxis.set_major_formatter(FuncFormatter(readable_formatter))\n",
    "\n",
    "# ErklÃ¤rung hinzufÃ¼gen\n",
    "axes[1].text(0.95, 0.95, \n",
    "             'Log-Skala macht alle Werte sichtbar:\\n100 â†’ 1000 â†’ 10000 haben\\ngleiche AbstÃ¤nde auf der Y-Achse',\n",
    "             transform=axes[1].transAxes, fontsize=9,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# ============================================================================\n",
    "# Chart 3: Top 20 grÃ¶ÃŸte Entities\n",
    "# ============================================================================\n",
    "top_20 = size_dist.head(20)\n",
    "bars = axes[2].barh(range(len(top_20)), top_20['address_count'], color='darkorange')\n",
    "axes[2].set_xlabel('Anzahl Adressen')\n",
    "axes[2].set_ylabel('Entity Rang')\n",
    "axes[2].set_title('Top 20: Die grÃ¶ÃŸten Entities')\n",
    "axes[2].set_yticks(range(len(top_20)))\n",
    "axes[2].set_yticklabels([f\"#{i+1}\" for i in range(len(top_20))])\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Werte an die Balken schreiben\n",
    "for bar, count in zip(bars, top_20['address_count']):\n",
    "    axes[2].text(bar.get_width() + max(top_20['address_count'])*0.01, \n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             f'{int(count):,}', \n",
    "             va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nErklÃ¤rung zur logarithmischen Skala:\")\n",
    "print(\"  - Normale Skala: Der dominante Wert (Entities mit 1-2 Adressen) erdrÃ¼ckt alles andere\")\n",
    "print(\"  - Log-Skala: VerhÃ¤ltnisse bleiben erhalten, aber alle Werte werden sichtbar\")\n",
    "print(\"  - Faustregel: Bei Power-Law-Verteilungen immer Log-Skala verwenden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities speichern\n",
    "print(\"Speichere Entity-Mapping...\")\n",
    "\n",
    "# Umbenennen fÃ¼r Klarheit\n",
    "entities_final = entities_df \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"address\"),\n",
    "        col(\"component\").alias(\"entity_id\")\n",
    "    )\n",
    "\n",
    "entities_final.write.mode(\"overwrite\").parquet(str(output_dir / \"entities.parquet\"))\n",
    "\n",
    "print(f\"Gespeichert: {output_dir / 'entities.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.4 Interpretation der Ergebnisse\n\n### Was bedeutet die Reduktion konkret?\n\n```\nVorher: ~200.000 individuelle Adressen\nNachher: ~150.000 Entities (Cluster)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nReduktion: ~25% der Adressen konnten gruppiert werden\n```\n\n**Interpretation**: Nur 25% Reduktion scheint gering, aber:\n- Die restlichen 75% sind **Single-Adress-Entities** (Adressen die nie mit anderen kombiniert wurden)\n- Diese kÃ¶nnten Einmal-Empfangsadressen, Cold Storage oder sehr private Nutzer sein\n- In spÃ¤teren ZeitrÃ¤umen (2015+) steigt die Reduktionsrate auf 40-60% durch hÃ¤ufigere Wiederverwendung\n\n### Was sind die grÃ¶ÃŸten Entities wahrscheinlich?\n\n| Entity-GrÃ¶ÃŸe | Wahrscheinliche IdentitÃ¤t | BegrÃ¼ndung |\n|--------------|---------------------------|------------|\n| >10.000 Adressen | **Exchanges** (BÃ¶rsen) | Sammeln viele Einzahlungsadressen, konsolidieren regelmÃ¤ÃŸig |\n| 1.000-10.000 Adressen | **Mining Pools** | Auszahlungen an viele Miner, gemeinsame Hot-Wallets |\n| 100-1.000 Adressen | **GroÃŸe HÃ¤ndler/Services** | Zahlungsprozessoren, Shops, Gambling-Sites |\n| 10-100 Adressen | **Aktive Nutzer/Trader** | Normale Wallet-Nutzung mit Address-Rotation |\n| 1-10 Adressen | **Gelegenheitsnutzer** | Wenige Transaktionen, einfache Wallets |\n\n**Vorsicht bei 2011-Daten**: Zu dieser Zeit gab es noch kaum groÃŸe Exchanges (Mt. Gox dominierte). GroÃŸe Entities kÃ¶nnten auch Early-Adopter-Wallets oder die Wallets der Bitcoin-Entwickler selbst sein.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Teil V: Whale Detection [GEPLANT]\n\n> **Status**: Dieser Abschnitt wird in einer zukÃ¼nftigen Version implementiert.\n\n## 5.1 Entity-Balances berechnen\n\n**Ziel**: FÃ¼r jede Entity die Summe aller UTXOs berechnen.\n\n```python\n# Geplanter Ansatz:\nentity_balances = utxo_df \\\n    .join(entities_df, utxo_df.address == entities_df.address) \\\n    .groupBy(\"entity_id\") \\\n    .agg(\n        sum(\"value\").alias(\"balance_satoshi\"),\n        count(\"*\").alias(\"utxo_count\")\n    )\n```\n\n## 5.2 Whale-Schwellenwerte definieren\n\n| Kategorie | Balance | Beschreibung |\n|-----------|---------|--------------|\n| Shrimp | < 1 BTC | Kleinanleger |\n| Crab | 1-10 BTC | Aktive Nutzer |\n| Fish | 10-100 BTC | Mittlere Investoren |\n| Dolphin | 100-1.000 BTC | VermÃ¶gende Investoren |\n| Whale | 1.000-10.000 BTC | GroÃŸinvestoren |\n| Humpback | > 10.000 BTC | Institutionen/Exchanges |\n\n## 5.3 Top-Holder identifizieren\n\n**Geplante Analysen**:\n- Top 100 Entities nach Balance\n- Verteilung der Balances (Gini-Koeffizient)\n- Zeitliche Entwicklung der Whale-Population\n\n---"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Teil VI: Verhaltensanalyse [GEPLANT]\n\n> **Status**: Dieser Abschnitt wird in einer zukÃ¼nftigen Version implementiert.\n\n## 6.1 Akkumulation vs. Distribution\n\n**Ziel**: Erkennen ob Whales Bitcoin akkumulieren oder verteilen.\n\n```python\n# Geplanter Ansatz: Zeitreihen pro Entity\nentity_balance_history = transactions_df \\\n    .join(entities_df, ...) \\\n    .groupBy(\"entity_id\", \"date\") \\\n    .agg(\n        sum(\"inflow\").alias(\"daily_inflow\"),\n        sum(\"outflow\").alias(\"daily_outflow\")\n    ) \\\n    .withColumn(\"net_flow\", col(\"daily_inflow\") - col(\"daily_outflow\"))\n```\n\n## 6.2 Zeitliche Muster\n\n**Geplante Metriken**:\n- **Holding Period**: Wie lange hÃ¤lt eine Entity ihre UTXOs?\n- **Transaction Frequency**: Wie oft transagiert eine Entity?\n- **Flow Patterns**: Korrelation zwischen Whale-AktivitÃ¤t und Preisbewegungen\n\n## 6.3 Anomalie-Erkennung\n\n**Geplante Analysen**:\n- UngewÃ¶hnlich groÃŸe Transaktionen identifizieren\n- PlÃ¶tzliche AktivitÃ¤tsspitzen erkennen\n- VerdÃ¤chtige Muster (z.B. Wash-Trading) flaggen\n\n---"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Teil VII: Zusammenfassung und Ausblick\n\n## 7.1 Erreichte Ergebnisse\n\n### Pipeline-Status\n\n| Schritt | Status | Output |\n|---------|--------|--------|\n| Daten laden | Fertig | tx_df, blocks_df |\n| JSON zu Parquet | Fertig | outputs.parquet, inputs.parquet |\n| UTXO Set | Fertig | utxos.parquet |\n| Entity Clustering | Fertig | entities.parquet |\n| Whale Detection | Geplant | - |\n| Verhaltensanalyse | Geplant | - |\n\n### Erzeugte Daten\n\nDie Pipeline hat folgende Parquet-Dateien erzeugt:\n\n| Datei | Inhalt | Verwendung |\n|-------|--------|------------|\n| `outputs.parquet` | Alle Transaction Outputs (flach) | Basis fuer UTXO-Berechnung |\n| `inputs.parquet` | Alle Inputs mit Spent-Referenzen | UTXO-Berechnung, Clustering |\n| `utxos.parquet` | Unspent Outputs | Balance-Berechnung |\n| `entities.parquet` | Address zu Entity Mapping | Whale Detection |\n\n## 7.2 Limitationen\n\n| Limitation | Beschreibung | Moegliche Loesung |\n|------------|--------------|-------------------|\n| **Teil-Export** | Nur H1 2011, nicht vollstaendige Blockchain | Vollstaendigen Export mit bitcoin-etl erstellen |\n| **Heuristik-Grenzen** | CoinJoin/Exchanges koennen falsche Cluster erzeugen | Zusaetzliche Heuristiken (Change Detection, etc.) |\n| **Keine Labels** | Keine bekannten Entitaeten markiert | Integration von Chainalysis/Elliptic Daten |\n\n## 7.3 Naechste Schritte\n\n1. **Teil V implementieren**: Entity-Balances berechnen, Whales identifizieren\n2. **Teil VI implementieren**: Zeitreihen-Analyse, Verhaltensmetriken\n3. **Visualisierung**: Dashboard fuer Whale-Tracking\n4. **Skalierung**: Pipeline auf vollstaendige Blockchain anwenden"
  },
  {
   "cell_type": "code",
   "source": "# Finale Statistiken\nprint(\"=\"*60)\nprint(\"PIPELINE ABGESCHLOSSEN\")\nprint(\"=\"*60)\nprint(f\"\\nDatenquelle: {BLOCKCHAIN_DATA_PATH}\")\nprint(f\"Ausgabe: {OUTPUT_PATH}\")\nprint(f\"\\nVerarbeitete Daten:\")\nprint(f\"  Transaktionen: {tx_count:,}\")\nprint(f\"  Blocks: {block_count:,}\")\nprint(f\"  Outputs: {output_count:,}\")\nprint(f\"  UTXOs: {utxo_count:,}\")\nprint(f\"\\nEntity Clustering:\")\nprint(f\"  Adressen: {address_count:,}\")\nprint(f\"  Entities: {entity_count:,}\")\nprint(f\"  Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\nprint(f\"\\nErzeugte Parquet-Dateien:\")\nfor f in Path(OUTPUT_PATH).glob(\"*.parquet\"):\n    print(f\"  - {f.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Spark-Session beenden (optional - auskommentiert fuer weitere Analysen)\n# spark.stop()\n# print(\"Spark-Session beendet.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}