{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Whale Intelligence: Entity Clustering\n",
    "\n",
    "## Zielsetzung\n",
    "\n",
    "Dieses Notebook führt die vollständige Pipeline von Rohdaten bis zum Entity Clustering durch:\n",
    "\n",
    "1. **Daten laden**: Bitcoin-Blockchain-Daten aus bitcoin-etl JSON Export\n",
    "2. **Daten transformieren**: JSON → optimiertes Parquet-Format\n",
    "3. **UTXO-Set berechnen**: Unspent Transaction Outputs identifizieren\n",
    "4. **Entity Clustering**: Adressen zu Entities gruppieren mittels Common Input Ownership Heuristic\n",
    "\n",
    "### Datenquelle\n",
    "\n",
    "Die Daten wurden mit [bitcoin-etl](https://github.com/blockchain-etl/bitcoin-etl) von einem Bitcoin Full Node exportiert:\n",
    "\n",
    "```bash\n",
    "bitcoinetl export_all \\\n",
    "    --provider-uri http://user:pass@localhost:8332 \\\n",
    "    --start YYYY-MM-DD --end YYYY-MM-DD \\\n",
    "    --output-dir /path/to/blockchain_exports\n",
    "```\n",
    "\n",
    "### Technischer Kontext\n",
    "\n",
    "Bitcoin verwendet das **UTXO-Modell** (Unspent Transaction Output). Im Gegensatz zu Account-basierten Systemen (wie Bankkonten) werden hier \"Münzen\" vollständig ausgegeben und Wechselgeld zurückgegeben. Dieses Modell ermöglicht die **Common Input Ownership Heuristic** - die Grundlage für Entity Clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundkonzepte\n",
    "\n",
    "### Bitcoin-Adresse vs. Entity\n",
    "\n",
    "| Konzept | Beschreibung | Sichtbar in Blockchain? |\n",
    "|---------|--------------|-------------------------|\n",
    "| **Adresse** | Einzelner \"Briefkasten\" für Bitcoin (z.B. `bc1q...`) | Ja |\n",
    "| **Wallet** | Software die viele Adressen verwaltet | Nein |\n",
    "| **Entity** | Person/Firma die ein oder mehrere Wallets besitzt | Nein |\n",
    "\n",
    "### Das Problem\n",
    "\n",
    "```\n",
    "800 Millionen Bitcoin-Adressen existieren\n",
    "         ↓\n",
    "Wer besitzt sie?\n",
    "         ↓\n",
    "Eine Person kann 1,000+ Adressen haben\n",
    "Eine Börse kann 5,000,000+ Adressen haben\n",
    "         ↓\n",
    "Die Blockchain zeigt NICHT welche Adressen zusammengehören!\n",
    "```\n",
    "\n",
    "**Ziel**: Adressen zu Entities gruppieren durch Analyse der Transaktionsmuster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Konfiguration und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-Bibliotheken\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Projektverzeichnis ermitteln und zum Path hinzufügen\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Projektverzeichnis: {project_root}\")\n",
    "\n",
    "# ============================================================================\n",
    "# KONFIGURATION - HIER ANPASSEN\n",
    "# ============================================================================\n",
    "\n",
    "# Pfad zu den bitcoin-etl exportierten Daten\n",
    "BLOCKCHAIN_DATA_PATH = \"/Users/roman/spark_project/blockchain_exports\"\n",
    "\n",
    "# Ausgabeverzeichnis für Parquet-Dateien\n",
    "OUTPUT_PATH = str(project_root / \"data\")\n",
    "\n",
    "# Spark-Konfiguration\n",
    "DRIVER_MEMORY = \"8g\"  # Erhöhen für größere Datasets\n",
    "\n",
    "print(f\"Datenquelle: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Ausgabe: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL-Modul importieren\n",
    "from src.etl import (\n",
    "    create_spark_session,\n",
    "    load_transactions,\n",
    "    load_blocks,\n",
    "    explode_outputs,\n",
    "    explode_inputs,\n",
    "    compute_utxo_set,\n",
    "    enrich_clustering_inputs,\n",
    ")\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Module erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-Session erstellen\n",
    "spark = create_spark_session(\n",
    "    app_name=\"Bitcoin Whale Intelligence\",\n",
    "    driver_memory=DRIVER_MEMORY,\n",
    "    enable_graphframes=True\n",
    ")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten laden\n",
    "\n",
    "Die bitcoin-etl Daten liegen als Hive-partitionierte JSON-Dateien vor:\n",
    "\n",
    "```\n",
    "blockchain_exports/\n",
    "└── 2011-01-01_2011-06-01/\n",
    "    ├── blocks/\n",
    "    │   └── date=YYYY-MM-DD/\n",
    "    │       └── blocks_*.json\n",
    "    └── transactions/\n",
    "        └── date=YYYY-MM-DD/\n",
    "            └── transactions_*.json\n",
    "```\n",
    "\n",
    "**Wichtig**: Die Transaktionen enthalten **nested Arrays** für inputs und outputs. Dies ist anders als bei normalisierten Datenbanken (z.B. BigQuery) wo diese in separaten Tabellen liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaktionen und Blocks laden\n",
    "print(\"Lade Transaktionen...\")\n",
    "tx_df = load_transactions(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "print(\"Lade Blocks...\")\n",
    "blocks_df = load_blocks(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "# Cache für wiederholten Zugriff\n",
    "tx_df.cache()\n",
    "blocks_df.cache()\n",
    "\n",
    "print(\"\\nDaten geladen und gecacht.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grundstatistiken\n",
    "tx_count = tx_df.count()\n",
    "block_count = blocks_df.count()\n",
    "\n",
    "print(f\"Geladene Daten:\")\n",
    "print(f\"  Transaktionen: {tx_count:,}\")\n",
    "print(f\"  Blocks: {block_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema anzeigen\n",
    "print(\"Transaction Schema (Auszug):\")\n",
    "tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Transaktion anzeigen\n",
    "print(\"Beispiel-Transaktion:\")\n",
    "tx_df.select(\n",
    "    \"hash\", \"block_number\", \"input_count\", \"output_count\", \n",
    "    \"is_coinbase\", \"output_value\", \"fee\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Das UTXO-Modell\n",
    "\n",
    "### Grundkonzept\n",
    "\n",
    "Bitcoin verwendet im Gegensatz zu Account-basierten Systemen ein **UTXO-basiertes Modell**:\n",
    "\n",
    "| Account-basiert (Bank) | UTXO-basiert (Bitcoin) |\n",
    "|------------------------|------------------------|\n",
    "| Konto hat Kontostand: 100 EUR | Besitz von \"Münzen\" verschiedener Größe |\n",
    "| Überweisung reduziert Kontostand | Münzen werden **vollständig** ausgegeben |\n",
    "| Einfache Subtraktion | Wechselgeld als neue UTXO zurück |\n",
    "\n",
    "### Praktisches Beispiel\n",
    "\n",
    "Alice besitzt zwei UTXOs:\n",
    "- **UTXO A**: 0.5 BTC (auf Adresse A1)\n",
    "- **UTXO B**: 0.3 BTC (auf Adresse A2)\n",
    "- **Gesamt**: 0.8 BTC\n",
    "\n",
    "Alice möchte **0.7 BTC** an Bob senden:\n",
    "\n",
    "```\n",
    "INPUTS (was Alice ausgibt):        OUTPUTS (was erstellt wird):\n",
    "┌─────────────────────┐            ┌─────────────────────┐\n",
    "│ UTXO A: 0.5 BTC     │            │ An Bob: 0.7 BTC     │\n",
    "│ (Adresse A1)        │   ───►     │ (neue UTXO für Bob) │\n",
    "├─────────────────────┤            ├─────────────────────┤\n",
    "│ UTXO B: 0.3 BTC     │            │ Wechselgeld: 0.09   │\n",
    "│ (Adresse A2)        │            │ (neue UTXO für      │\n",
    "└─────────────────────┘            │  Alice auf A3)      │\n",
    "  Summe: 0.8 BTC                   ├─────────────────────┤\n",
    "                                   │ Fee: 0.01 BTC       │\n",
    "                                   │ (an Miner)          │\n",
    "                                   └─────────────────────┘\n",
    "                                     Summe: 0.8 BTC\n",
    "```\n",
    "\n",
    "**Wichtige Erkenntnis**: Alice musste **beide Adressen A1 und A2** als Inputs verwenden. Dafür braucht sie die Private Keys beider Adressen. **→ A1 und A2 gehören zur selben Person!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Common Input Ownership Heuristic\n",
    "\n",
    "### Die Heuristik\n",
    "\n",
    "**Beobachtung**: Wenn eine Transaktion mehrere Adressen als Inputs verwendet, müssen alle diese Adressen von derselben Person kontrolliert werden.\n",
    "\n",
    "**Warum?** Um eine Bitcoin-Transaktion zu signieren, braucht man die Private Keys **aller** Input-Adressen. Nur wer alle Keys besitzt, kann die Transaktion erstellen.\n",
    "\n",
    "### Transitive Verknüpfung\n",
    "\n",
    "```\n",
    "Transaction 1: Inputs von A1 + A2  →  A1 und A2 gehören zusammen\n",
    "Transaction 2: Inputs von A2 + A3  →  A2 und A3 gehören zusammen\n",
    "─────────────────────────────────────────────────────────────────\n",
    "Schlussfolgerung: A1, A2, A3 gehören alle zur selben Person!\n",
    "\n",
    "Graph-Darstellung:\n",
    "┌────┐         ┌────┐         ┌────┐\n",
    "│ A1 │ ──tx1── │ A2 │ ──tx2── │ A3 │\n",
    "└────┘         └────┘         └────┘\n",
    "          ↓\n",
    "  Connected Component\n",
    "          ↓\n",
    "┌─────────────────────────────────┐\n",
    "│  Entity 1: {A1, A2, A3}         │\n",
    "└─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Einschränkungen\n",
    "\n",
    "Die Heuristik funktioniert **nicht** bei:\n",
    "\n",
    "1. **Exchange-Transaktionen**: Börsen bündeln Auszahlungen vieler Nutzer\n",
    "   - **Lösung**: Filter `input_count > 50` → wahrscheinlich Exchange\n",
    "\n",
    "2. **CoinJoin**: Privacy-Protokoll das Transaktionen mehrerer Nutzer mischt\n",
    "   - **Lösung**: Pattern-Erkennung (alle Outputs gleich groß)\n",
    "\n",
    "3. **Mining-Pools**: Batch-Auszahlungen an Miner\n",
    "   - **Lösung**: Bekannte Pool-Adressen filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Input Transaktionen analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, sum as spark_sum, avg\n",
    "\n",
    "# Input-Count Verteilung\n",
    "input_dist = tx_df \\\n",
    "    .filter(col(\"is_coinbase\") == False) \\\n",
    "    .groupBy(\"input_count\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\")) \\\n",
    "    .orderBy(\"input_count\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"Input-Count Verteilung (Top 15):\")\n",
    "print(input_dist.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiken berechnen\n",
    "total_non_coinbase = tx_df.filter(col(\"is_coinbase\") == False).count()\n",
    "single_input = input_dist[input_dist['input_count'] == 1]['transaction_count'].sum()\n",
    "multi_input = input_dist[input_dist['input_count'] > 1]['transaction_count'].sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-INPUT TRANSACTION STATISTIK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGesamt (ohne Coinbase): {total_non_coinbase:,}\")\n",
    "print(f\"Single-Input (1 Adresse):   {single_input:,} ({single_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"Multi-Input (≥2 Adressen):  {multi_input:,} ({multi_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"\\n→ {multi_input/total_non_coinbase*100:.1f}% der Transaktionen sind für Clustering nutzbar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram (Log-Skala)\n",
    "plot_data = input_dist[input_dist['input_count'] <= 20]\n",
    "ax1.bar(plot_data['input_count'], plot_data['transaction_count'], \n",
    "        color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Anzahl Inputs pro Transaktion')\n",
    "ax1.set_ylabel('Anzahl Transaktionen (log)')\n",
    "ax1.set_title('Verteilung der Input-Counts')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pie Chart\n",
    "labels = ['Single-Input\\n(nicht nutzbar)', 'Multi-Input\\n(nutzbar für Clustering)']\n",
    "sizes = [single_input, multi_input]\n",
    "colors = ['lightgray', 'steelblue']\n",
    "ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Anteil der Transaktionen für Clustering')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: Multi-Input Transaktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Multi-Input Transaktion detailliert betrachten\n",
    "example_tx = tx_df \\\n",
    "    .filter(\n",
    "        (col(\"input_count\") >= 3) & \n",
    "        (col(\"input_count\") <= 10) &\n",
    "        (col(\"is_coinbase\") == False)\n",
    "    ) \\\n",
    "    .first()\n",
    "\n",
    "if example_tx:\n",
    "    print(f\"Beispiel Multi-Input Transaktion:\")\n",
    "    print(f\"  Hash: {example_tx['hash'][:20]}...\")\n",
    "    print(f\"  Block: {example_tx['block_number']}\")\n",
    "    print(f\"  Input Count: {example_tx['input_count']}\")\n",
    "    print(f\"  Output Count: {example_tx['output_count']}\")\n",
    "    print(f\"  Wert: {example_tx['output_value'] / 100000000:.8f} BTC\")\n",
    "    \n",
    "    print(f\"\\n  Inputs (Adressen die zusammengehören):\")\n",
    "    for i, inp in enumerate(example_tx['inputs'][:5]):\n",
    "        addr = inp['addresses'][0] if inp['addresses'] else \"(enrichment nötig)\"\n",
    "        print(f\"    [{i}] {addr}\")\n",
    "    if len(example_tx['inputs']) > 5:\n",
    "        print(f\"    ... und {len(example_tx['inputs']) - 5} weitere\")\n",
    "    \n",
    "    print(f\"\\n  → Diese {example_tx['input_count']} Adressen gehören zur selben Entity!\")\n",
    "else:\n",
    "    print(\"Keine passende Multi-Input Transaktion gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ETL: Transformation zu Parquet\n",
    "\n",
    "Bevor wir das Clustering durchführen, transformieren wir die Daten in ein effizienteres Format:\n",
    "\n",
    "| JSON (Rohdaten) | Parquet (optimiert) |\n",
    "|-----------------|---------------------|\n",
    "| Text-basiert, langsam zu parsen | Binär, schnell zu lesen |\n",
    "| Keine Kompression | Snappy-Kompression (70-90% kleiner) |\n",
    "| Liest immer alles | Liest nur benötigte Spalten |\n",
    "\n",
    "### Datenfluss\n",
    "\n",
    "```\n",
    "transactions.json\n",
    "      │\n",
    "      ├──► explode_outputs() ──► outputs.parquet\n",
    "      │\n",
    "      └──► explode_inputs()  ──► inputs.parquet (für spent-Referenzen)\n",
    "                                       │\n",
    "                                       ▼\n",
    "                               UTXO-Berechnung\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs explodieren (nested → flat)\n",
    "print(\"Explodiere Outputs...\")\n",
    "outputs_df = explode_outputs(tx_df)\n",
    "outputs_df.cache()\n",
    "\n",
    "output_count = outputs_df.count()\n",
    "print(f\"Outputs: {output_count:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "outputs_df.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs explodieren\n",
    "print(\"Explodiere Inputs...\")\n",
    "inputs_df = explode_inputs(tx_df)\n",
    "inputs_df.cache()\n",
    "\n",
    "input_count_flat = inputs_df.count()\n",
    "print(f\"Inputs: {input_count_flat:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "inputs_df.select(\n",
    "    \"tx_hash\", \"input_index\", \"spent_tx_hash\", \"spent_output_index\", \"value\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Als Parquet speichern\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(OUTPUT_PATH)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Speichere Outputs als Parquet...\")\n",
    "outputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"outputs.parquet\"))\n",
    "\n",
    "print(\"Speichere Inputs als Parquet...\")\n",
    "inputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"inputs.parquet\"))\n",
    "\n",
    "print(f\"\\nParquet-Dateien gespeichert in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. UTXO Set berechnen\n",
    "\n",
    "Das **UTXO Set** (Unspent Transaction Output Set) enthält alle Outputs die noch nicht ausgegeben wurden.\n",
    "\n",
    "### Berechnung\n",
    "\n",
    "```\n",
    "UTXO Set = Alle Outputs MINUS Outputs die als Input referenziert wurden\n",
    "\n",
    "SQL-Äquivalent:\n",
    "SELECT * FROM outputs\n",
    "WHERE (tx_hash, output_index) NOT IN (\n",
    "    SELECT spent_tx_hash, spent_output_index FROM inputs\n",
    ")\n",
    "```\n",
    "\n",
    "**Hinweis**: Bei Teil-Exporten (nicht die gesamte Blockchain) kann das UTXO Set unvollständig sein, da spending-Referenzen aus späteren Blöcken fehlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTXO Set berechnen\n",
    "print(\"Berechne UTXO Set...\")\n",
    "utxo_df = compute_utxo_set(outputs_df, inputs_df)\n",
    "utxo_df.cache()\n",
    "\n",
    "utxo_count = utxo_df.count()\n",
    "spent_count = output_count - utxo_count\n",
    "\n",
    "print(f\"\\nUTXO Statistik:\")\n",
    "print(f\"  Gesamt Outputs: {output_count:,}\")\n",
    "print(f\"  Spent (ausgegeben): {spent_count:,} ({spent_count/output_count*100:.1f}%)\")\n",
    "print(f\"  Unspent (UTXOs): {utxo_count:,} ({utxo_count/output_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTXO Set speichern\n",
    "print(\"Speichere UTXO Set...\")\n",
    "utxo_df.write.mode(\"overwrite\").parquet(str(output_dir / \"utxos.parquet\"))\n",
    "print(f\"Gespeichert: {output_dir / 'utxos.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entity Clustering mit GraphFrames\n",
    "\n",
    "### Algorithmus: Connected Components\n",
    "\n",
    "Wir modellieren das Problem als **Graph**:\n",
    "\n",
    "- **Knoten (Vertices)**: Bitcoin-Adressen\n",
    "- **Kanten (Edges)**: Zwei Adressen die zusammen als Inputs einer Transaktion erscheinen\n",
    "\n",
    "Der **Connected Components**-Algorithmus findet alle zusammenhängenden Teilgraphen. Jeder Teilgraph ist eine **Entity**.\n",
    "\n",
    "```\n",
    "Beispiel-Graph:\n",
    "\n",
    "  A1 ─── A2 ─── A3      B1 ─── B2\n",
    "   │           │                │\n",
    "   └─── A4 ────┘               B3\n",
    "\n",
    "Ergebnis:\n",
    "  Entity 1: {A1, A2, A3, A4}\n",
    "  Entity 2: {B1, B2, B3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs für Clustering anreichern\n",
    "print(\"Reichere Multi-Input-Transaktionen mit Adressen an...\")\n",
    "print(\"  (Filter: 2-50 Inputs, keine Coinbase)\")\n",
    "\n",
    "clustering_inputs = enrich_clustering_inputs(\n",
    "    tx_df, \n",
    "    outputs_df,\n",
    "    min_inputs=2,\n",
    "    max_inputs=50\n",
    ")\n",
    "clustering_inputs.cache()\n",
    "\n",
    "enriched_count = clustering_inputs.count()\n",
    "print(f\"\\nAngereicherte Inputs: {enriched_count:,}\")\n",
    "\n",
    "# Beispiel\n",
    "print(\"\\nBeispiel (tx_hash → address):\")\n",
    "clustering_inputs.show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, size as spark_size\n",
    "from itertools import combinations\n",
    "\n",
    "# Adressen pro Transaktion gruppieren\n",
    "tx_addresses = clustering_inputs \\\n",
    "    .groupBy(\"tx_hash\") \\\n",
    "    .agg(collect_set(\"address\").alias(\"addresses\"))\n",
    "\n",
    "# Filtern: Nur TXs mit mindestens 2 verschiedenen Adressen\n",
    "tx_addresses = tx_addresses.filter(spark_size(\"addresses\") >= 2)\n",
    "\n",
    "tx_with_addresses = tx_addresses.count()\n",
    "print(f\"Transaktionen mit ≥2 Adressen: {tx_with_addresses:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode as spark_explode, arrays_zip, transform, struct\n",
    "\n",
    "# Kanten erstellen: Alle Adresspaare pro Transaktion\n",
    "# Für jede TX mit Adressen [A, B, C] erstellen wir Kanten: (A,B), (A,C), (B,C)\n",
    "\n",
    "def create_edges_udf(addresses):\n",
    "    \"\"\"Erstellt alle Paare aus einer Liste von Adressen.\"\"\"\n",
    "    if not addresses or len(addresses) < 2:\n",
    "        return []\n",
    "    return [(a, b) for a, b in combinations(sorted(addresses), 2)]\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "edge_schema = ArrayType(StructType([\n",
    "    StructField(\"src\", StringType()),\n",
    "    StructField(\"dst\", StringType())\n",
    "]))\n",
    "\n",
    "create_edges = udf(create_edges_udf, edge_schema)\n",
    "\n",
    "# Kanten erstellen und explodieren\n",
    "edges_df = tx_addresses \\\n",
    "    .withColumn(\"edges\", create_edges(\"addresses\")) \\\n",
    "    .select(spark_explode(\"edges\").alias(\"edge\")) \\\n",
    "    .select(\n",
    "        col(\"edge.src\").alias(\"src\"),\n",
    "        col(\"edge.dst\").alias(\"dst\")\n",
    "    ) \\\n",
    "    .distinct()\n",
    "\n",
    "edges_df.cache()\n",
    "edge_count = edges_df.count()\n",
    "\n",
    "print(f\"Graph-Kanten (unique): {edge_count:,}\")\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertices (alle eindeutigen Adressen)\n",
    "vertices_src = edges_df.select(col(\"src\").alias(\"id\"))\n",
    "vertices_dst = edges_df.select(col(\"dst\").alias(\"id\"))\n",
    "vertices_df = vertices_src.union(vertices_dst).distinct()\n",
    "\n",
    "vertex_count = vertices_df.count()\n",
    "print(f\"Graph-Knoten (Adressen): {vertex_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphFrames Connected Components\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "print(\"Erstelle Graph...\")\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "print(\"Führe Connected Components aus...\")\n",
    "print(\"  (Dies kann bei großen Graphen einige Minuten dauern)\")\n",
    "\n",
    "# Checkpoint-Verzeichnis setzen (nötig für Connected Components)\n",
    "spark.sparkContext.setCheckpointDir(str(output_dir / \"checkpoints\"))\n",
    "\n",
    "# Connected Components berechnen\n",
    "entities_df = graph.connectedComponents()\n",
    "entities_df.cache()\n",
    "\n",
    "print(\"\\nConnected Components berechnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Statistiken\n",
    "entity_count = entities_df.select(\"component\").distinct().count()\n",
    "address_count = entities_df.count()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTITY CLUSTERING ERGEBNIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdressen analysiert: {address_count:,}\")\n",
    "print(f\"Entities gefunden: {entity_count:,}\")\n",
    "print(f\"Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\n",
    "print(f\"\\n→ {address_count:,} Adressen wurden zu {entity_count:,} Entities gruppiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-Größen analysieren\n",
    "entity_sizes = entities_df \\\n",
    "    .groupBy(\"component\") \\\n",
    "    .agg(count(\"*\").alias(\"address_count\")) \\\n",
    "    .orderBy(col(\"address_count\").desc())\n",
    "\n",
    "print(\"Top 10 größte Entities:\")\n",
    "entity_sizes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-Größen-Verteilung\n",
    "size_dist = entity_sizes.toPandas()\n",
    "\n",
    "print(\"Entity-Größen-Statistik:\")\n",
    "print(f\"  Min: {size_dist['address_count'].min()} Adressen\")\n",
    "print(f\"  Max: {size_dist['address_count'].max()} Adressen\")\n",
    "print(f\"  Median: {size_dist['address_count'].median():.0f} Adressen\")\n",
    "print(f\"  Durchschnitt: {size_dist['address_count'].mean():.1f} Adressen\")\n",
    "\n",
    "# Wie viele sind Single-Address Entities?\n",
    "single_entities = (size_dist['address_count'] == 1).sum()\n",
    "multi_entities = (size_dist['address_count'] > 1).sum()\n",
    "print(f\"\\n  Single-Address Entities: {single_entities:,}\")\n",
    "print(f\"  Multi-Address Entities: {multi_entities:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Entity-Größen\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram (Log-Skala)\n",
    "ax1.hist(size_dist['address_count'], bins=50, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Anzahl Adressen pro Entity')\n",
    "ax1.set_ylabel('Anzahl Entities (log)')\n",
    "ax1.set_title('Verteilung der Entity-Größen')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top 20 Entities\n",
    "top_20 = size_dist.head(20)\n",
    "ax2.barh(range(len(top_20)), top_20['address_count'], color='darkorange')\n",
    "ax2.set_xlabel('Anzahl Adressen')\n",
    "ax2.set_ylabel('Entity Rang')\n",
    "ax2.set_title('Top 20 größte Entities')\n",
    "ax2.set_yticks(range(len(top_20)))\n",
    "ax2.set_yticklabels([f\"#{i+1}\" for i in range(len(top_20))])\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities speichern\n",
    "print(\"Speichere Entity-Mapping...\")\n",
    "\n",
    "# Umbenennen für Klarheit\n",
    "entities_final = entities_df \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"address\"),\n",
    "        col(\"component\").alias(\"entity_id\")\n",
    "    )\n",
    "\n",
    "entities_final.write.mode(\"overwrite\").parquet(str(output_dir / \"entities.parquet\"))\n",
    "\n",
    "print(f\"Gespeichert: {output_dir / 'entities.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Zusammenfassung\n",
    "\n",
    "### Was wurde erreicht?\n",
    "\n",
    "1. **Daten geladen**: Bitcoin-ETL JSON Daten erfolgreich in Spark geladen\n",
    "\n",
    "2. **ETL durchgeführt**: Nested JSON → flache Parquet-Tabellen\n",
    "   - `outputs.parquet`: Alle Transaction Outputs\n",
    "   - `inputs.parquet`: Alle Transaction Inputs mit Spent-Referenzen\n",
    "\n",
    "3. **UTXO Set berechnet**: Unspent Outputs identifiziert\n",
    "   - `utxos.parquet`: Alle nicht ausgegebenen Outputs\n",
    "\n",
    "4. **Entity Clustering durchgeführt**: Adressen zu Entities gruppiert\n",
    "   - `entities.parquet`: Mapping address → entity_id\n",
    "\n",
    "### Nächste Schritte\n",
    "\n",
    "Mit dem Entity-Mapping können nun weitere Analysen durchgeführt werden:\n",
    "\n",
    "- **Whale Detection**: Entity-Balances berechnen, große Holder identifizieren\n",
    "- **Verhaltensanalyse**: Akkumulation vs. Distribution über Zeit\n",
    "- **Exchange-Identifikation**: Entities mit ungewöhnlichen Mustern markieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Statistiken\n",
    "print(\"=\"*60)\n",
    "print(\"PIPELINE ABGESCHLOSSEN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDatenquelle: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Ausgabe: {OUTPUT_PATH}\")\n",
    "print(f\"\\nVerarbeitete Daten:\")\n",
    "print(f\"  Transaktionen: {tx_count:,}\")\n",
    "print(f\"  Blocks: {block_count:,}\")\n",
    "print(f\"  Outputs: {output_count:,}\")\n",
    "print(f\"  UTXOs: {utxo_count:,}\")\n",
    "print(f\"\\nEntity Clustering:\")\n",
    "print(f\"  Adressen: {address_count:,}\")\n",
    "print(f\"  Entities: {entity_count:,}\")\n",
    "print(f\"  Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\n",
    "print(f\"\\nErzeugte Parquet-Dateien:\")\n",
    "for f in Path(OUTPUT_PATH).glob(\"*.parquet\"):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-Session beenden\n",
    "# spark.stop()\n",
    "# print(\"Spark-Session beendet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
