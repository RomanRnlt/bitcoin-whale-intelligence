{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Whale Intelligence: Entity Clustering\n",
    "\n",
    "## Zielsetzung\n",
    "\n",
    "Dieses Notebook führt die vollständige Pipeline von Rohdaten bis zum Entity Clustering durch:\n",
    "\n",
    "1. **Daten laden**: Bitcoin-Blockchain-Daten aus bitcoin-etl JSON Export\n",
    "2. **Daten transformieren**: JSON → optimiertes Parquet-Format\n",
    "3. **UTXO-Set berechnen**: Unspent Transaction Outputs identifizieren\n",
    "4. **Entity Clustering**: Adressen zu Entities gruppieren mittels Common Input Ownership Heuristic\n",
    "\n",
    "### Datenquelle\n",
    "\n",
    "Die Daten wurden mit [bitcoin-etl](https://github.com/blockchain-etl/bitcoin-etl) von einem Bitcoin Full Node exportiert:\n",
    "\n",
    "```bash\n",
    "bitcoinetl export_all \\\n",
    "    --provider-uri http://user:pass@localhost:8332 \\\n",
    "    --start YYYY-MM-DD --end YYYY-MM-DD \\\n",
    "    --output-dir /path/to/blockchain_exports\n",
    "```\n",
    "\n",
    "### Technischer Kontext\n",
    "\n",
    "Bitcoin verwendet das **UTXO-Modell** (Unspent Transaction Output). Im Gegensatz zu Account-basierten Systemen (wie Bankkonten) werden hier \"Münzen\" vollständig ausgegeben und Wechselgeld zurückgegeben. Dieses Modell ermöglicht die **Common Input Ownership Heuristic** - die Grundlage für Entity Clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundkonzepte\n",
    "\n",
    "### Bitcoin-Adresse vs. Entity\n",
    "\n",
    "| Konzept | Beschreibung | Sichtbar in Blockchain? |\n",
    "|---------|--------------|-------------------------|\n",
    "| **Adresse** | Einzelner \"Briefkasten\" für Bitcoin (z.B. `bc1q...`) | Ja |\n",
    "| **Wallet** | Software die viele Adressen verwaltet | Nein |\n",
    "| **Entity** | Person/Firma die ein oder mehrere Wallets besitzt | Nein |\n",
    "\n",
    "### Das Problem\n",
    "\n",
    "```\n",
    "800 Millionen Bitcoin-Adressen existieren\n",
    "         ↓\n",
    "Wer besitzt sie?\n",
    "         ↓\n",
    "Eine Person kann 1,000+ Adressen haben\n",
    "Eine Börse kann 5,000,000+ Adressen haben\n",
    "         ↓\n",
    "Die Blockchain zeigt NICHT welche Adressen zusammengehören!\n",
    "```\n",
    "\n",
    "**Ziel**: Adressen zu Entities gruppieren durch Analyse der Transaktionsmuster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Konfiguration und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projektverzeichnis: /Users/roman/spark_project/bitcoin-whale-intelligence\n",
      "Datenquelle: /Users/roman/spark_project/blockchain_exports\n",
      "Ausgabe: /Users/roman/spark_project/bitcoin-whale-intelligence/data\n"
     ]
    }
   ],
   "source": [
    "# Standard-Bibliotheken\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Projektverzeichnis ermitteln und zum Path hinzufügen\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Projektverzeichnis: {project_root}\")\n",
    "\n",
    "# ============================================================================\n",
    "# KONFIGURATION - HIER ANPASSEN\n",
    "# ============================================================================\n",
    "\n",
    "# Pfad zu den bitcoin-etl exportierten Daten\n",
    "BLOCKCHAIN_DATA_PATH = \"/Users/roman/spark_project/blockchain_exports\"\n",
    "\n",
    "# Ausgabeverzeichnis für Parquet-Dateien\n",
    "OUTPUT_PATH = str(project_root / \"data\")\n",
    "\n",
    "# Spark-Konfiguration\n",
    "DRIVER_MEMORY = \"8g\"  # Erhöhen für größere Datasets\n",
    "\n",
    "print(f\"Datenquelle: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Ausgabe: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "# ETL-Modul importieren\n",
    "from src.etl import (\n",
    "    create_spark_session,\n",
    "    load_transactions,\n",
    "    load_blocks,\n",
    "    explode_outputs,\n",
    "    explode_inputs,\n",
    "    compute_utxo_set,\n",
    "    enrich_clustering_inputs,\n",
    ")\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Module erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.5.7-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/roman/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/roman/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f2aee8ca-4158-48b4-92bc-1ace7d43942e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.3-spark3.5-s_2.12!graphframes.jar (210ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (138ms)\n",
      ":: resolution report :: resolve 1946ms :: artifacts dl 355ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f2aee8ca-4158-48b4-92bc-1ace7d43942e\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (280kB/13ms)\n",
      "25/12/08 18:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.7\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Spark-Session erstellen\n",
    "spark = create_spark_session(\n",
    "    app_name=\"Bitcoin Whale Intelligence\",\n",
    "    driver_memory=DRIVER_MEMORY,\n",
    "    enable_graphframes=True\n",
    ")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten laden\n",
    "\n",
    "Die bitcoin-etl Daten liegen als Hive-partitionierte JSON-Dateien vor:\n",
    "\n",
    "```\n",
    "blockchain_exports/\n",
    "└── 2011-01-01_2011-06-01/\n",
    "    ├── blocks/\n",
    "    │   └── date=YYYY-MM-DD/\n",
    "    │       └── blocks_*.json\n",
    "    └── transactions/\n",
    "        └── date=YYYY-MM-DD/\n",
    "            └── transactions_*.json\n",
    "```\n",
    "\n",
    "**Wichtig**: Die Transaktionen enthalten **nested Arrays** für inputs und outputs. Dies ist anders als bei normalisierten Datenbanken (z.B. BigQuery) wo diese in separaten Tabellen liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Transaktionen...\n",
      "Lade Blocks...\n",
      "\n",
      "Daten geladen und gecacht.\n"
     ]
    }
   ],
   "source": [
    "# Transaktionen und Blocks laden\n",
    "print(\"Lade Transaktionen...\")\n",
    "tx_df = load_transactions(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "print(\"Lade Blocks...\")\n",
    "blocks_df = load_blocks(spark, BLOCKCHAIN_DATA_PATH)\n",
    "\n",
    "# Cache für wiederholten Zugriff\n",
    "tx_df.cache()\n",
    "blocks_df.cache()\n",
    "\n",
    "print(\"\\nDaten geladen und gecacht.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladene Daten:\n",
      "  Transaktionen: 382,402\n",
      "  Blocks: 27,644\n"
     ]
    }
   ],
   "source": [
    "# Grundstatistiken\n",
    "tx_count = tx_df.count()\n",
    "block_count = blocks_df.count()\n",
    "\n",
    "print(f\"Geladene Daten:\")\n",
    "print(f\"  Transaktionen: {tx_count:,}\")\n",
    "print(f\"  Blocks: {block_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Schema (Auszug):\n",
      "root\n",
      " |-- hash: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      " |-- virtual_size: integer (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- lock_time: long (nullable = true)\n",
      " |-- block_number: long (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- block_timestamp: long (nullable = true)\n",
      " |-- is_coinbase: boolean (nullable = true)\n",
      " |-- index: integer (nullable = true)\n",
      " |-- inputs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- index: integer (nullable = true)\n",
      " |    |    |-- spent_transaction_hash: string (nullable = true)\n",
      " |    |    |-- spent_output_index: integer (nullable = true)\n",
      " |    |    |-- script_asm: string (nullable = true)\n",
      " |    |    |-- script_hex: string (nullable = true)\n",
      " |    |    |-- sequence: long (nullable = true)\n",
      " |    |    |-- required_signatures: integer (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- addresses: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- outputs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- index: integer (nullable = true)\n",
      " |    |    |-- script_asm: string (nullable = true)\n",
      " |    |    |-- script_hex: string (nullable = true)\n",
      " |    |    |-- required_signatures: integer (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- addresses: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- input_count: integer (nullable = true)\n",
      " |-- output_count: integer (nullable = true)\n",
      " |-- input_value: long (nullable = true)\n",
      " |-- output_value: long (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- block_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema anzeigen\n",
    "print(\"Transaction Schema (Auszug):\")\n",
    "tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel-Transaktion:\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "|                hash|block_number|input_count|output_count|is_coinbase|output_value|        fee|\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "|630bb912bea097180...|      126766|          0|           1|       true|  5000000000|          0|\n",
      "|0556dd5dba67f4476...|      126766|          2|           2|      false|  2128000000|-2128000000|\n",
      "|7a41ec18684517921...|      126766|          1|           1|      false|    50000000|  -50000000|\n",
      "|c0b929b9c6abdecb3...|      126766|          1|           1|      false|   100000000| -100000000|\n",
      "|4cedd988b1b9018c1...|      126765|          0|           1|       true|  5002000000|          0|\n",
      "+--------------------+------------+-----------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-Transaktion anzeigen\n",
    "print(\"Beispiel-Transaktion:\")\n",
    "tx_df.select(\n",
    "    \"hash\", \"block_number\", \"input_count\", \"output_count\", \n",
    "    \"is_coinbase\", \"output_value\", \"fee\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Das UTXO-Modell\n",
    "\n",
    "### Grundkonzept\n",
    "\n",
    "Bitcoin verwendet im Gegensatz zu Account-basierten Systemen ein **UTXO-basiertes Modell**:\n",
    "\n",
    "| Account-basiert (Bank) | UTXO-basiert (Bitcoin) |\n",
    "|------------------------|------------------------|\n",
    "| Konto hat Kontostand: 100 EUR | Besitz von \"Münzen\" verschiedener Größe |\n",
    "| Überweisung reduziert Kontostand | Münzen werden **vollständig** ausgegeben |\n",
    "| Einfache Subtraktion | Wechselgeld als neue UTXO zurück |\n",
    "\n",
    "### Praktisches Beispiel\n",
    "\n",
    "Alice besitzt zwei UTXOs:\n",
    "- **UTXO A**: 0.5 BTC (auf Adresse A1)\n",
    "- **UTXO B**: 0.3 BTC (auf Adresse A2)\n",
    "- **Gesamt**: 0.8 BTC\n",
    "\n",
    "Alice möchte **0.7 BTC** an Bob senden:\n",
    "\n",
    "```\n",
    "INPUTS (was Alice ausgibt):        OUTPUTS (was erstellt wird):\n",
    "┌─────────────────────┐            ┌─────────────────────┐\n",
    "│ UTXO A: 0.5 BTC     │            │ An Bob: 0.7 BTC     │\n",
    "│ (Adresse A1)        │   ───►     │ (neue UTXO für Bob) │\n",
    "├─────────────────────┤            ├─────────────────────┤\n",
    "│ UTXO B: 0.3 BTC     │            │ Wechselgeld: 0.09   │\n",
    "│ (Adresse A2)        │            │ (neue UTXO für      │\n",
    "└─────────────────────┘            │  Alice auf A3)      │\n",
    "  Summe: 0.8 BTC                   ├─────────────────────┤\n",
    "                                   │ Fee: 0.01 BTC       │\n",
    "                                   │ (an Miner)          │\n",
    "                                   └─────────────────────┘\n",
    "                                     Summe: 0.8 BTC\n",
    "```\n",
    "\n",
    "**Wichtige Erkenntnis**: Alice musste **beide Adressen A1 und A2** als Inputs verwenden. Dafür braucht sie die Private Keys beider Adressen. **→ A1 und A2 gehören zur selben Person!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Common Input Ownership Heuristic\n",
    "\n",
    "### Die Heuristik\n",
    "\n",
    "**Beobachtung**: Wenn eine Transaktion mehrere Adressen als Inputs verwendet, müssen alle diese Adressen von derselben Person kontrolliert werden.\n",
    "\n",
    "**Warum?** Um eine Bitcoin-Transaktion zu signieren, braucht man die Private Keys **aller** Input-Adressen. Nur wer alle Keys besitzt, kann die Transaktion erstellen.\n",
    "\n",
    "### Transitive Verknüpfung\n",
    "\n",
    "```\n",
    "Transaction 1: Inputs von A1 + A2  →  A1 und A2 gehören zusammen\n",
    "Transaction 2: Inputs von A2 + A3  →  A2 und A3 gehören zusammen\n",
    "─────────────────────────────────────────────────────────────────\n",
    "Schlussfolgerung: A1, A2, A3 gehören alle zur selben Person!\n",
    "\n",
    "Graph-Darstellung:\n",
    "┌────┐         ┌────┐         ┌────┐\n",
    "│ A1 │ ──tx1── │ A2 │ ──tx2── │ A3 │\n",
    "└────┘         └────┘         └────┘\n",
    "          ↓\n",
    "  Connected Component\n",
    "          ↓\n",
    "┌─────────────────────────────────┐\n",
    "│  Entity 1: {A1, A2, A3}         │\n",
    "└─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Einschränkungen\n",
    "\n",
    "Die Heuristik funktioniert **nicht** bei:\n",
    "\n",
    "1. **Exchange-Transaktionen**: Börsen bündeln Auszahlungen vieler Nutzer\n",
    "   - **Lösung**: Filter `input_count > 50` → wahrscheinlich Exchange\n",
    "\n",
    "2. **CoinJoin**: Privacy-Protokoll das Transaktionen mehrerer Nutzer mischt\n",
    "   - **Lösung**: Pattern-Erkennung (alle Outputs gleich groß)\n",
    "\n",
    "3. **Mining-Pools**: Batch-Auszahlungen an Miner\n",
    "   - **Lösung**: Bekannte Pool-Adressen filtern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Input Transaktionen analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Count Verteilung (Top 15):\n",
      " input_count  transaction_count\n",
      "           1             287512\n",
      "           2              33911\n",
      "           3              11259\n",
      "           4               6672\n",
      "           5               4027\n",
      "           6               2449\n",
      "           7               1696\n",
      "           8               1186\n",
      "           9                935\n",
      "          10                782\n",
      "          11                487\n",
      "          12                433\n",
      "          13                311\n",
      "          14                267\n",
      "          15                247\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, sum as spark_sum, avg\n",
    "\n",
    "# Input-Count Verteilung\n",
    "input_dist = tx_df \\\n",
    "    .filter(col(\"is_coinbase\") == False) \\\n",
    "    .groupBy(\"input_count\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\")) \\\n",
    "    .orderBy(\"input_count\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"Input-Count Verteilung (Top 15):\")\n",
    "print(input_dist.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-INPUT TRANSACTION STATISTIK\n",
      "============================================================\n",
      "\n",
      "Gesamt (ohne Coinbase): 354,758\n",
      "Single-Input (1 Adresse):   287,512 (81.0%)\n",
      "Multi-Input (≥2 Adressen):  67,246 (19.0%)\n",
      "\n",
      "→ 19.0% der Transaktionen sind für Clustering nutzbar\n"
     ]
    }
   ],
   "source": [
    "# Statistiken berechnen\n",
    "total_non_coinbase = tx_df.filter(col(\"is_coinbase\") == False).count()\n",
    "single_input = input_dist[input_dist['input_count'] == 1]['transaction_count'].sum()\n",
    "multi_input = input_dist[input_dist['input_count'] > 1]['transaction_count'].sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-INPUT TRANSACTION STATISTIK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGesamt (ohne Coinbase): {total_non_coinbase:,}\")\n",
    "print(f\"Single-Input (1 Adresse):   {single_input:,} ({single_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"Multi-Input (≥2 Adressen):  {multi_input:,} ({multi_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"\\n→ {multi_input/total_non_coinbase*100:.1f}% der Transaktionen sind für Clustering nutzbar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualisierung\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# ============================================================================\n# Chart 1: Normale Skala (zeigt das Problem)\n# ============================================================================\nplot_data = input_dist[input_dist['input_count'] <= 20]\naxes[0].bar(plot_data['input_count'], plot_data['transaction_count'], \n            color='steelblue', edgecolor='black', linewidth=0.5)\naxes[0].set_xlabel('Anzahl Inputs pro Transaktion')\naxes[0].set_ylabel('Anzahl Transaktionen')\naxes[0].set_title('Problem: 1-Input dominiert alles')\naxes[0].grid(True, alpha=0.3)\n# Annotation\naxes[0].annotate(f'{int(single_input):,}', \n                 xy=(1, single_input), \n                 xytext=(3, single_input * 0.8),\n                 fontsize=9,\n                 arrowprops=dict(arrowstyle='->', color='red'))\naxes[0].text(10, single_input * 0.5, \n             '← Die anderen Balken\\n    sind kaum sichtbar!', \n             fontsize=9, color='red')\n\n# ============================================================================\n# Chart 2: Logarithmische Skala (macht alle Balken sichtbar)\n# ============================================================================\naxes[1].bar(plot_data['input_count'], plot_data['transaction_count'], \n            color='steelblue', edgecolor='black', linewidth=0.5)\naxes[1].set_xlabel('Anzahl Inputs pro Transaktion')\naxes[1].set_ylabel('Anzahl Transaktionen')\naxes[1].set_title('Lösung: Logarithmische Skala')\naxes[1].set_yscale('log')\naxes[1].grid(True, alpha=0.3)\n\n# Y-Achse mit lesbaren Zahlen statt 10^x\nfrom matplotlib.ticker import FuncFormatter\ndef readable_formatter(x, pos):\n    if x >= 1000:\n        return f'{int(x/1000)}k'\n    return f'{int(x)}'\naxes[1].yaxis.set_major_formatter(FuncFormatter(readable_formatter))\naxes[1].text(10, 100, 'Jetzt sieht man\\nauch 2, 3, 4+ Inputs', fontsize=9, color='green')\n\n# ============================================================================\n# Chart 3: Pie Chart\n# ============================================================================\nlabels = ['Single-Input\\n(nicht nutzbar)', 'Multi-Input\\n(nutzbar für Clustering)']\nsizes = [single_input, multi_input]\ncolors = ['lightgray', 'steelblue']\naxes[2].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\naxes[2].set_title('Anteil für Clustering')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\nErklärung zur logarithmischen Skala:\")\nprint(\"  - Normale Skala: 1-Input-TXs (~290k) dominieren, Rest unsichtbar\")\nprint(\"  - Log-Skala: Abstände werden gestaucht, alle Werte sichtbar\")\nprint(\"  - Beispiel: 100 → 1000 → 10000 haben gleiche Abstände auf der Y-Achse\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: Multi-Input Transaktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel Multi-Input Transaktion:\n",
      "  Hash: a55cfdd8677056d6b0fb...\n",
      "  Block: 126765\n",
      "  Input Count: 4\n",
      "  Output Count: 1\n",
      "  Wert: 20.27000000 BTC\n",
      "\n",
      "  Inputs (Adressen die zusammengehören):\n",
      "    [0] (enrichment nötig)\n",
      "    [1] (enrichment nötig)\n",
      "    [2] (enrichment nötig)\n",
      "    [3] (enrichment nötig)\n",
      "\n",
      "  → Diese 4 Adressen gehören zur selben Entity!\n"
     ]
    }
   ],
   "source": [
    "# Eine Multi-Input Transaktion detailliert betrachten\n",
    "example_tx = tx_df \\\n",
    "    .filter(\n",
    "        (col(\"input_count\") >= 3) & \n",
    "        (col(\"input_count\") <= 10) &\n",
    "        (col(\"is_coinbase\") == False)\n",
    "    ) \\\n",
    "    .first()\n",
    "\n",
    "if example_tx:\n",
    "    print(f\"Beispiel Multi-Input Transaktion:\")\n",
    "    print(f\"  Hash: {example_tx['hash'][:20]}...\")\n",
    "    print(f\"  Block: {example_tx['block_number']}\")\n",
    "    print(f\"  Input Count: {example_tx['input_count']}\")\n",
    "    print(f\"  Output Count: {example_tx['output_count']}\")\n",
    "    print(f\"  Wert: {example_tx['output_value'] / 100000000:.8f} BTC\")\n",
    "    \n",
    "    print(f\"\\n  Inputs (Adressen die zusammengehören):\")\n",
    "    for i, inp in enumerate(example_tx['inputs'][:5]):\n",
    "        addr = inp['addresses'][0] if inp['addresses'] else \"(enrichment nötig)\"\n",
    "        print(f\"    [{i}] {addr}\")\n",
    "    if len(example_tx['inputs']) > 5:\n",
    "        print(f\"    ... und {len(example_tx['inputs']) - 5} weitere\")\n",
    "    \n",
    "    print(f\"\\n  → Diese {example_tx['input_count']} Adressen gehören zur selben Entity!\")\n",
    "else:\n",
    "    print(\"Keine passende Multi-Input Transaktion gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ETL: Transformation zu Parquet\n",
    "\n",
    "Bevor wir das Clustering durchführen, transformieren wir die Daten in ein effizienteres Format:\n",
    "\n",
    "| JSON (Rohdaten) | Parquet (optimiert) |\n",
    "|-----------------|---------------------|\n",
    "| Text-basiert, langsam zu parsen | Binär, schnell zu lesen |\n",
    "| Keine Kompression | Snappy-Kompression (70-90% kleiner) |\n",
    "| Liest immer alles | Liest nur benötigte Spalten |\n",
    "\n",
    "### Datenfluss\n",
    "\n",
    "```\n",
    "transactions.json\n",
    "      │\n",
    "      ├──► explode_outputs() ──► outputs.parquet\n",
    "      │\n",
    "      └──► explode_inputs()  ──► inputs.parquet (für spent-Referenzen)\n",
    "                                       │\n",
    "                                       ▼\n",
    "                               UTXO-Berechnung\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explodiere Outputs...\n",
      "Outputs: 769,081\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "|                       tx_hash|block_number|block_timestamp|output_index|     value|                     addresses|output_type|\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "|630bb912bea0971803979417615...|      126766|     1306369241|           0|5000000000|[nonstandarde36d71a77f0b72f...|nonstandard|\n",
      "|0556dd5dba67f4476841e530125...|      126766|     1306369241|           0| 128000000|[1N1HR4BPwhP5WvFXF6JCTkRnKj...| pubkeyhash|\n",
      "|0556dd5dba67f4476841e530125...|      126766|     1306369241|           1|2000000000|[16UksCM6jXXR8XGq9cXWiP48P1...| pubkeyhash|\n",
      "|7a41ec18684517921585f710b37...|      126766|     1306369241|           0|  50000000|[1HHETdBA3zrUf9RHoBdgteucMX...| pubkeyhash|\n",
      "|c0b929b9c6abdecb3ebd3857f93...|      126766|     1306369241|           0| 100000000|[12bAvkf2ku6za5XKHwvjgaVvZS...| pubkeyhash|\n",
      "+------------------------------+------------+---------------+------------+----------+------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outputs explodieren (nested → flat)\n",
    "print(\"Explodiere Outputs...\")\n",
    "outputs_df = explode_outputs(tx_df)\n",
    "outputs_df.cache()\n",
    "\n",
    "output_count = outputs_df.count()\n",
    "print(f\"Outputs: {output_count:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "outputs_df.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explodiere Inputs...\n",
      "Inputs: 632,295\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "|             tx_hash|input_index|       spent_tx_hash|spent_output_index|value|\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "|630bb912bea097180...|       NULL|                NULL|              NULL| NULL|\n",
      "|0556dd5dba67f4476...|          0|3ccb1d88e9f0f8067...|                 1| NULL|\n",
      "|0556dd5dba67f4476...|          1|6f5594a671cd2b686...|                 1| NULL|\n",
      "|7a41ec18684517921...|          0|5259ab3a1d0045aa3...|                 1| NULL|\n",
      "|c0b929b9c6abdecb3...|          0|d683c9078dad5625e...|                22| NULL|\n",
      "+--------------------+-----------+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inputs explodieren\n",
    "print(\"Explodiere Inputs...\")\n",
    "inputs_df = explode_inputs(tx_df)\n",
    "inputs_df.cache()\n",
    "\n",
    "input_count_flat = inputs_df.count()\n",
    "print(f\"Inputs: {input_count_flat:,}\")\n",
    "\n",
    "# Beispiel anzeigen\n",
    "inputs_df.select(\n",
    "    \"tx_hash\", \"input_index\", \"spent_tx_hash\", \"spent_output_index\", \"value\"\n",
    ").show(5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speichere Outputs als Parquet...\n",
      "Speichere Inputs als Parquet...\n",
      "\n",
      "Parquet-Dateien gespeichert in: /Users/roman/spark_project/bitcoin-whale-intelligence/data\n"
     ]
    }
   ],
   "source": [
    "# Als Parquet speichern\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(OUTPUT_PATH)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Speichere Outputs als Parquet...\")\n",
    "outputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"outputs.parquet\"))\n",
    "\n",
    "print(\"Speichere Inputs als Parquet...\")\n",
    "inputs_df.write.mode(\"overwrite\").parquet(str(output_dir / \"inputs.parquet\"))\n",
    "\n",
    "print(f\"\\nParquet-Dateien gespeichert in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. UTXO Set berechnen\n",
    "\n",
    "Das **UTXO Set** (Unspent Transaction Output Set) enthält alle Outputs die noch nicht ausgegeben wurden.\n",
    "\n",
    "### Berechnung\n",
    "\n",
    "```\n",
    "UTXO Set = Alle Outputs MINUS Outputs die als Input referenziert wurden\n",
    "\n",
    "SQL-Äquivalent:\n",
    "SELECT * FROM outputs\n",
    "WHERE (tx_hash, output_index) NOT IN (\n",
    "    SELECT spent_tx_hash, spent_output_index FROM inputs\n",
    ")\n",
    "```\n",
    "\n",
    "**Hinweis**: Bei Teil-Exporten (nicht die gesamte Blockchain) kann das UTXO Set unvollständig sein, da spending-Referenzen aus späteren Blöcken fehlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne UTXO Set...\n",
      "\n",
      "UTXO Statistik:\n",
      "  Gesamt Outputs: 769,081\n",
      "  Spent (ausgegeben): 592,040 (77.0%)\n",
      "  Unspent (UTXOs): 177,041 (23.0%)\n"
     ]
    }
   ],
   "source": [
    "# UTXO Set berechnen\n",
    "print(\"Berechne UTXO Set...\")\n",
    "utxo_df = compute_utxo_set(outputs_df, inputs_df)\n",
    "utxo_df.cache()\n",
    "\n",
    "utxo_count = utxo_df.count()\n",
    "spent_count = output_count - utxo_count\n",
    "\n",
    "print(f\"\\nUTXO Statistik:\")\n",
    "print(f\"  Gesamt Outputs: {output_count:,}\")\n",
    "print(f\"  Spent (ausgegeben): {spent_count:,} ({spent_count/output_count*100:.1f}%)\")\n",
    "print(f\"  Unspent (UTXOs): {utxo_count:,} ({utxo_count/output_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speichere UTXO Set...\n",
      "Gespeichert: /Users/roman/spark_project/bitcoin-whale-intelligence/data/utxos.parquet\n"
     ]
    }
   ],
   "source": [
    "# UTXO Set speichern\n",
    "print(\"Speichere UTXO Set...\")\n",
    "utxo_df.write.mode(\"overwrite\").parquet(str(output_dir / \"utxos.parquet\"))\n",
    "print(f\"Gespeichert: {output_dir / 'utxos.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entity Clustering mit GraphFrames\n",
    "\n",
    "### Algorithmus: Connected Components\n",
    "\n",
    "Wir modellieren das Problem als **Graph**:\n",
    "\n",
    "- **Knoten (Vertices)**: Bitcoin-Adressen\n",
    "- **Kanten (Edges)**: Zwei Adressen die zusammen als Inputs einer Transaktion erscheinen\n",
    "\n",
    "Der **Connected Components**-Algorithmus findet alle zusammenhängenden Teilgraphen. Jeder Teilgraph ist eine **Entity**.\n",
    "\n",
    "```\n",
    "Beispiel-Graph:\n",
    "\n",
    "  A1 ─── A2 ─── A3      B1 ─── B2\n",
    "   │           │                │\n",
    "   └─── A4 ────┘               B3\n",
    "\n",
    "Ergebnis:\n",
    "  Entity 1: {A1, A2, A3, A4}\n",
    "  Entity 2: {B1, B2, B3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reichere Multi-Input-Transaktionen mit Adressen an...\n",
      "  (Filter: 2-50 Inputs, keine Coinbase)\n",
      "\n",
      "Angereicherte Inputs: 274,791\n",
      "\n",
      "Beispiel (tx_hash → address):\n",
      "+----------------------------------------+----------------------------------+\n",
      "|                                 tx_hash|                           address|\n",
      "+----------------------------------------+----------------------------------+\n",
      "|b18bcc349f24e3e004ea8df08d96bef60c47a...|1A2NwhNs7i3sWqw3Nv8sKDF3QJ7xmuWSG2|\n",
      "|340208d7e8424f95423d08b0d589c14048439...|1Dp7jL1yjRgBGiET8tbm4cFJX8CqaYAB4X|\n",
      "|bdac5ed2d94524940521eff3eb939b2657e71...|13kv7pPW6yqMRpuxdNHzQKwmJss9z8Bcti|\n",
      "|0bdb8db6c908596b9c766e14a4cfd785ee185...|13dsHpD2VirpqkUFQpXScTyywKSh2UYCCQ|\n",
      "|d6a81e7a7e4473d4a1e5d0c70793adaa080ca...|1JkEiX7H21di63CHY32ByMxF8zXiMuYrSH|\n",
      "|c911bd92b16b94ce4701c0654cfec37310258...|1GRLYdjP6kwgGXTS2Z1tfPDStwEqrh8nWq|\n",
      "|c38435d59c46199e007f43e56925ecd41444a...|13eDj81bs7XVdnbHvoeKgdkEkpmgC9jPpd|\n",
      "|5aab43cb4d83fa8b231d7728654332d5e6a37...|13FW27uYdh1nLLUXpYzu8cQT8MdybxqM61|\n",
      "|cfc8a6622702bf7b3a1e29f619ff7c0b31a6d...|15nRrsfB5s6BCEr2CKBrmoxn52kSeTuHf5|\n",
      "|27bd1e6b23ef590fc105ecab93f9a91ce5f3d...|123Zdp9Td4auQb4EDMMgKHfdyEtcnNMpaM|\n",
      "+----------------------------------------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inputs für Clustering anreichern\n",
    "print(\"Reichere Multi-Input-Transaktionen mit Adressen an...\")\n",
    "print(\"  (Filter: 2-50 Inputs, keine Coinbase)\")\n",
    "\n",
    "clustering_inputs = enrich_clustering_inputs(\n",
    "    tx_df, \n",
    "    outputs_df,\n",
    "    min_inputs=2,\n",
    "    max_inputs=50\n",
    ")\n",
    "clustering_inputs.cache()\n",
    "\n",
    "enriched_count = clustering_inputs.count()\n",
    "print(f\"\\nAngereicherte Inputs: {enriched_count:,}\")\n",
    "\n",
    "# Beispiel\n",
    "print(\"\\nBeispiel (tx_hash → address):\")\n",
    "clustering_inputs.show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaktionen mit ≥2 Adressen: 57,606\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, size as spark_size\n",
    "from itertools import combinations\n",
    "\n",
    "# Adressen pro Transaktion gruppieren\n",
    "tx_addresses = clustering_inputs \\\n",
    "    .groupBy(\"tx_hash\") \\\n",
    "    .agg(collect_set(\"address\").alias(\"addresses\"))\n",
    "\n",
    "# Filtern: Nur TXs mit mindestens 2 verschiedenen Adressen\n",
    "tx_addresses = tx_addresses.filter(spark_size(\"addresses\") >= 2)\n",
    "\n",
    "tx_with_addresses = tx_addresses.count()\n",
    "print(f\"Transaktionen mit ≥2 Adressen: {tx_with_addresses:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-Kanten (unique): 400,872\n",
      "+--------------------+--------------------+\n",
      "|                 src|                 dst|\n",
      "+--------------------+--------------------+\n",
      "|192nJoWgPuc3sKFQB...|1Q3fg95C1TcqXS1Xc...|\n",
      "|1HL2U2Cz1tDkh52kJ...|1JwfvhMrphhVSZwNk...|\n",
      "|1KzyYTFAQF6Q5SAMR...|1LaqzJryo46Z4Tmof...|\n",
      "|1CPxeZnow3C3EU6Fr...|1LLaxfmyGB393VkGD...|\n",
      "|1B5fVpnS87hzioWQ3...|1CSzzf96C7fvuLYYu...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode as spark_explode, arrays_zip, transform, struct\n",
    "\n",
    "# Kanten erstellen: Alle Adresspaare pro Transaktion\n",
    "# Für jede TX mit Adressen [A, B, C] erstellen wir Kanten: (A,B), (A,C), (B,C)\n",
    "\n",
    "def create_edges_udf(addresses):\n",
    "    \"\"\"Erstellt alle Paare aus einer Liste von Adressen.\"\"\"\n",
    "    if not addresses or len(addresses) < 2:\n",
    "        return []\n",
    "    return [(a, b) for a, b in combinations(sorted(addresses), 2)]\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "edge_schema = ArrayType(StructType([\n",
    "    StructField(\"src\", StringType()),\n",
    "    StructField(\"dst\", StringType())\n",
    "]))\n",
    "\n",
    "create_edges = udf(create_edges_udf, edge_schema)\n",
    "\n",
    "# Kanten erstellen und explodieren\n",
    "edges_df = tx_addresses \\\n",
    "    .withColumn(\"edges\", create_edges(\"addresses\")) \\\n",
    "    .select(spark_explode(\"edges\").alias(\"edge\")) \\\n",
    "    .select(\n",
    "        col(\"edge.src\").alias(\"src\"),\n",
    "        col(\"edge.dst\").alias(\"dst\")\n",
    "    ) \\\n",
    "    .distinct()\n",
    "\n",
    "edges_df.cache()\n",
    "edge_count = edges_df.count()\n",
    "\n",
    "print(f\"Graph-Kanten (unique): {edge_count:,}\")\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-Knoten (Adressen): 147,907\n"
     ]
    }
   ],
   "source": [
    "# Vertices (alle eindeutigen Adressen)\n",
    "vertices_src = edges_df.select(col(\"src\").alias(\"id\"))\n",
    "vertices_dst = edges_df.select(col(\"dst\").alias(\"id\"))\n",
    "vertices_df = vertices_src.union(vertices_dst).distinct()\n",
    "\n",
    "vertex_count = vertices_df.count()\n",
    "print(f\"Graph-Knoten (Adressen): {vertex_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle Graph...\n",
      "Führe Connected Components aus...\n",
      "  (Dies kann bei großen Graphen einige Minuten dauern)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o365.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOfRange(Arrays.java:4030)\n\tat java.base/java.lang.StringLatin1.newString(StringLatin1.java:715)\n\tat java.base/java.lang.StringBuilder.toString(StringBuilder.java:452)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:64)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:780)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2994/0x00000008012da040.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2963/0x00000008012c5440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda$6405/0x0000000801ca2840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda$2958/0x00000008012c2840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda$2817/0x000000080123b040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2825/0x000000080123fc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2818/0x000000080123b440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m spark.sparkContext.setCheckpointDir(\u001b[38;5;28mstr\u001b[39m(output_dir / \u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Connected Components berechnen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m entities_df = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m entities_df.cache()\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConnected Components berechnet.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/private/var/folders/nx/qs47_8zn2jsdkcz32b7cr8_r0000gn/T/spark-779e2879-97c0-498b-b0df-01c810819b29/userFiles-19438abe-2204-40f0-bd09-08031ce4346b/graphframes_graphframes-0.8.3-spark3.5-s_2.12.jar/graphframes/graphframe.py:331\u001b[39m, in \u001b[36mGraphFrame.connectedComponents\u001b[39m\u001b[34m(self, algorithm, checkpointInterval, broadcastThreshold)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnectedComponents\u001b[39m(\u001b[38;5;28mself\u001b[39m, algorithm = \u001b[33m\"\u001b[39m\u001b[33mgraphframes\u001b[39m\u001b[33m\"\u001b[39m, checkpointInterval = \u001b[32m2\u001b[39m,\n\u001b[32m    313\u001b[39m                         broadcastThreshold = \u001b[32m1000000\u001b[39m):\n\u001b[32m    314\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    Computes the connected components of the graph.\u001b[39;00m\n\u001b[32m    316\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m \u001b[33;03m    :return: DataFrame with new vertices column \"component\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetAlgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetCheckpointInterval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpointInterval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msetBroadcastThreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcastThreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m._spark)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark_project/py11_venv_project/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o365.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOfRange(Arrays.java:4030)\n\tat java.base/java.lang.StringLatin1.newString(StringLatin1.java:715)\n\tat java.base/java.lang.StringBuilder.toString(StringBuilder.java:452)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:64)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:780)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2994/0x00000008012da040.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2963/0x00000008012c5440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda$6405/0x0000000801ca2840.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda$2958/0x00000008012c2840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda$2817/0x000000080123b040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2825/0x000000080123fc40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2818/0x000000080123b440.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n"
     ]
    }
   ],
   "source": [
    "# GraphFrames Connected Components\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "print(\"Erstelle Graph...\")\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "print(\"Führe Connected Components aus...\")\n",
    "print(\"  (Dies kann bei großen Graphen einige Minuten dauern)\")\n",
    "\n",
    "# Checkpoint-Verzeichnis setzen (nötig für Connected Components)\n",
    "spark.sparkContext.setCheckpointDir(str(output_dir / \"checkpoints\"))\n",
    "\n",
    "# Connected Components berechnen\n",
    "entities_df = graph.connectedComponents()\n",
    "entities_df.cache()\n",
    "\n",
    "print(\"\\nConnected Components berechnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Statistiken\n",
    "entity_count = entities_df.select(\"component\").distinct().count()\n",
    "address_count = entities_df.count()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTITY CLUSTERING ERGEBNIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdressen analysiert: {address_count:,}\")\n",
    "print(f\"Entities gefunden: {entity_count:,}\")\n",
    "print(f\"Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\n",
    "print(f\"\\n→ {address_count:,} Adressen wurden zu {entity_count:,} Entities gruppiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-Größen analysieren\n",
    "entity_sizes = entities_df \\\n",
    "    .groupBy(\"component\") \\\n",
    "    .agg(count(\"*\").alias(\"address_count\")) \\\n",
    "    .orderBy(col(\"address_count\").desc())\n",
    "\n",
    "print(\"Top 10 größte Entities:\")\n",
    "entity_sizes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-Größen-Verteilung\n",
    "size_dist = entity_sizes.toPandas()\n",
    "\n",
    "print(\"Entity-Größen-Statistik:\")\n",
    "print(f\"  Min: {size_dist['address_count'].min()} Adressen\")\n",
    "print(f\"  Max: {size_dist['address_count'].max()} Adressen\")\n",
    "print(f\"  Median: {size_dist['address_count'].median():.0f} Adressen\")\n",
    "print(f\"  Durchschnitt: {size_dist['address_count'].mean():.1f} Adressen\")\n",
    "\n",
    "# Wie viele sind Single-Address Entities?\n",
    "single_entities = (size_dist['address_count'] == 1).sum()\n",
    "multi_entities = (size_dist['address_count'] > 1).sum()\n",
    "print(f\"\\n  Single-Address Entities: {single_entities:,}\")\n",
    "print(f\"  Multi-Address Entities: {multi_entities:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualisierung der Entity-Größen\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# ============================================================================\n# Chart 1: Verteilung der Entity-Größen\n# ============================================================================\nax1.hist(size_dist['address_count'], bins=50, color='steelblue', edgecolor='black')\nax1.set_xlabel('Anzahl Adressen pro Entity')\nax1.set_ylabel('Anzahl Entities')\nax1.set_title('Verteilung: Wie groß sind die Entities?')\nax1.set_yscale('log')\nax1.grid(True, alpha=0.3)\n\n# Lesbare Y-Achse\nfrom matplotlib.ticker import FuncFormatter\ndef readable_formatter(x, pos):\n    if x >= 1000:\n        return f'{int(x/1000)}k'\n    return f'{int(x)}'\nax1.yaxis.set_major_formatter(FuncFormatter(readable_formatter))\n\n# Erklärung hinzufügen\nax1.text(0.95, 0.95, \n         'Log-Skala: Die meisten Entities\\\\nhaben nur 1-2 Adressen',\n         transform=ax1.transAxes, fontsize=9,\n         verticalalignment='top', horizontalalignment='right',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# ============================================================================\n# Chart 2: Top 20 größte Entities\n# ============================================================================\ntop_20 = size_dist.head(20)\nbars = ax2.barh(range(len(top_20)), top_20['address_count'], color='darkorange')\nax2.set_xlabel('Anzahl Adressen')\nax2.set_ylabel('Entity Rang')\nax2.set_title('Top 20: Die größten Entities (vermutlich Exchanges/Pools)')\nax2.set_yticks(range(len(top_20)))\nax2.set_yticklabels([f\"#{i+1}\" for i in range(len(top_20))])\nax2.invert_yaxis()\nax2.grid(True, alpha=0.3, axis='x')\n\n# Werte an die Balken schreiben\nfor bar, count in zip(bars, top_20['address_count']):\n    ax2.text(bar.get_width() + max(top_20['address_count'])*0.01, \n             bar.get_y() + bar.get_height()/2,\n             f'{int(count):,}', \n             va='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\\\nInterpretation:\")\nprint(f\"  - Die meisten Entities haben nur wenige Adressen (normale Nutzer)\")\nprint(f\"  - Wenige sehr große Entities = wahrscheinlich Börsen oder Mining-Pools\")\nprint(f\"  - Größte Entity hat {int(size_dist['address_count'].max()):,} Adressen\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities speichern\n",
    "print(\"Speichere Entity-Mapping...\")\n",
    "\n",
    "# Umbenennen für Klarheit\n",
    "entities_final = entities_df \\\n",
    "    .select(\n",
    "        col(\"id\").alias(\"address\"),\n",
    "        col(\"component\").alias(\"entity_id\")\n",
    "    )\n",
    "\n",
    "entities_final.write.mode(\"overwrite\").parquet(str(output_dir / \"entities.parquet\"))\n",
    "\n",
    "print(f\"Gespeichert: {output_dir / 'entities.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Zusammenfassung\n",
    "\n",
    "### Was wurde erreicht?\n",
    "\n",
    "1. **Daten geladen**: Bitcoin-ETL JSON Daten erfolgreich in Spark geladen\n",
    "\n",
    "2. **ETL durchgeführt**: Nested JSON → flache Parquet-Tabellen\n",
    "   - `outputs.parquet`: Alle Transaction Outputs\n",
    "   - `inputs.parquet`: Alle Transaction Inputs mit Spent-Referenzen\n",
    "\n",
    "3. **UTXO Set berechnet**: Unspent Outputs identifiziert\n",
    "   - `utxos.parquet`: Alle nicht ausgegebenen Outputs\n",
    "\n",
    "4. **Entity Clustering durchgeführt**: Adressen zu Entities gruppiert\n",
    "   - `entities.parquet`: Mapping address → entity_id\n",
    "\n",
    "### Nächste Schritte\n",
    "\n",
    "Mit dem Entity-Mapping können nun weitere Analysen durchgeführt werden:\n",
    "\n",
    "- **Whale Detection**: Entity-Balances berechnen, große Holder identifizieren\n",
    "- **Verhaltensanalyse**: Akkumulation vs. Distribution über Zeit\n",
    "- **Exchange-Identifikation**: Entities mit ungewöhnlichen Mustern markieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Statistiken\n",
    "print(\"=\"*60)\n",
    "print(\"PIPELINE ABGESCHLOSSEN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDatenquelle: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Ausgabe: {OUTPUT_PATH}\")\n",
    "print(f\"\\nVerarbeitete Daten:\")\n",
    "print(f\"  Transaktionen: {tx_count:,}\")\n",
    "print(f\"  Blocks: {block_count:,}\")\n",
    "print(f\"  Outputs: {output_count:,}\")\n",
    "print(f\"  UTXOs: {utxo_count:,}\")\n",
    "print(f\"\\nEntity Clustering:\")\n",
    "print(f\"  Adressen: {address_count:,}\")\n",
    "print(f\"  Entities: {entity_count:,}\")\n",
    "print(f\"  Reduktion: {(1 - entity_count/address_count)*100:.1f}%\")\n",
    "print(f\"\\nErzeugte Parquet-Dateien:\")\n",
    "for f in Path(OUTPUT_PATH).glob(\"*.parquet\"):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-Session beenden\n",
    "# spark.stop()\n",
    "# print(\"Spark-Session beendet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}