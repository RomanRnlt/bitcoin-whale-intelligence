{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Blockchair TSV Loading mit src/schemas.py\n",
    "\n",
    "Dieses Notebook demonstriert, wie die Blockchair TSV-Dateien mit den definierten Schemas geladen werden.\n",
    "\n",
    "## Voraussetzungen\n",
    "\n",
    "1. Blockchair-Daten heruntergeladen (mit `blockchair-downloader`)\n",
    "2. TSV-Dateien extrahiert in einem lokalen Ordner\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.schemas import BLOCKS_SCHEMA, TRANSACTIONS_SCHEMA, INPUTS_SCHEMA, OUTPUTS_SCHEMA\n",
    "from src.schemas import load_blockchair_data\n",
    "\n",
    "print(\"Schemas erfolgreich importiert!\")\n",
    "print(f\"\\nBlocks Schema: {len(BLOCKS_SCHEMA.fields)} Felder\")\n",
    "print(f\"Transactions Schema: {len(TRANSACTIONS_SCHEMA.fields)} Felder\")\n",
    "print(f\"Inputs Schema: {len(INPUTS_SCHEMA.fields)} Felder\")\n",
    "print(f\"Outputs Schema: {len(OUTPUTS_SCHEMA.fields)} Felder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguration\n",
    "\n",
    "**Passe diesen Pfad an:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WICHTIG: Pfad zu deinen extrahierten Blockchair-Daten\n",
    "LOCAL_DATA_PATH = '/Users/roman/Documents/Master/Module/ADE/test'\n",
    "\n",
    "print(f\"Lade Daten von: {LOCAL_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Blockchair TSV Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode 1: Einzelne Dateien laden (manuell)\n",
    "\n",
    "Diese Methode zeigt, wie jede Tabelle einzeln geladen wird mit explizitem Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocks laden\n",
    "blocks_df = spark.read.csv(\n",
    "    f\"{LOCAL_DATA_PATH}/*blocks*.tsv\",\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    schema=BLOCKS_SCHEMA\n",
    ")\n",
    "\n",
    "print(\"Blocks DataFrame geladen!\")\n",
    "print(f\"Anzahl Zeilen: {blocks_df.count()}\")\n",
    "blocks_df.printSchema()\n",
    "blocks_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions laden\n",
    "transactions_df = spark.read.csv(\n",
    "    f\"{LOCAL_DATA_PATH}/*transactions*.tsv\",\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    schema=TRANSACTIONS_SCHEMA\n",
    ")\n",
    "\n",
    "print(\"Transactions DataFrame geladen!\")\n",
    "print(f\"Anzahl Zeilen: {transactions_df.count()}\")\n",
    "transactions_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs laden\n",
    "inputs_df = spark.read.csv(\n",
    "    f\"{LOCAL_DATA_PATH}/*inputs*.tsv\",\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    schema=INPUTS_SCHEMA\n",
    ")\n",
    "\n",
    "print(\"Inputs DataFrame geladen!\")\n",
    "print(f\"Anzahl Zeilen: {inputs_df.count()}\")\n",
    "inputs_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs laden\n",
    "outputs_df = spark.read.csv(\n",
    "    f\"{LOCAL_DATA_PATH}/*outputs*.tsv\",\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    schema=OUTPUTS_SCHEMA\n",
    ")\n",
    "\n",
    "print(\"Outputs DataFrame geladen!\")\n",
    "print(f\"Anzahl Zeilen: {outputs_df.count()}\")\n",
    "outputs_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode 2: Helper-Funktion verwenden (empfohlen)\n",
    "\n",
    "Die `load_blockchair_data()` Funktion lädt alle 4 Tabellen auf einmal.\n",
    "\n",
    "**Hinweis:** Diese Methode erwartet eine bestimmte Ordnerstruktur:\n",
    "```\n",
    "LOCAL_DATA_PATH/\n",
    "├── blocks/*.tsv\n",
    "├── transactions/*.tsv\n",
    "├── inputs/*.tsv\n",
    "└── outputs/*.tsv\n",
    "```\n",
    "\n",
    "Wenn deine Dateien direkt im Root-Ordner liegen (wie `/Users/roman/Documents/Master/Module/ADE/test`), nutze Methode 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenqualität prüfen\n",
    "\n",
    "Prüfe, ob die Datentypen korrekt sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATENTYP-VALIDIERUNG\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Blocks: Prüfe Timestamp\n",
    "print(\"\\n1. Blocks - Timestamp-Spalte:\")\n",
    "blocks_df.select(\"time\").show(3)\n",
    "print(f\"   Typ: {blocks_df.schema['time'].dataType}\")\n",
    "\n",
    "# Transactions: Prüfe Boolean\n",
    "print(\"\\n2. Transactions - Boolean-Spalte:\")\n",
    "transactions_df.select(\"is_coinbase\", \"has_witness\").show(3)\n",
    "print(f\"   Typ is_coinbase: {transactions_df.schema['is_coinbase'].dataType}\")\n",
    "\n",
    "# Outputs: Prüfe Value (muss LongType sein für Satoshis)\n",
    "print(\"\\n3. Outputs - Value-Spalte (Satoshis):\")\n",
    "outputs_df.select(\"value\").show(3)\n",
    "print(f\"   Typ: {outputs_df.schema['value'].dataType}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Alle Datentypen korrekt!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Dieses Notebook hat gezeigt:\n",
    "\n",
    "1. ✅ Import der Schemas aus `src/schemas.py` funktioniert\n",
    "2. ✅ TSV-Dateien können mit expliziten Schemas geladen werden\n",
    "3. ✅ Datentypen (Timestamps, Booleans, LongType für Satoshis) sind korrekt\n",
    "4. ✅ Nur 3-4 Zeilen Code pro Tabelle nötig\n",
    "\n",
    "### Verwendung im Haupt-Notebook\n",
    "\n",
    "Im finalen Notebook (für Professor) würde der Code so aussehen:\n",
    "\n",
    "```python\n",
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from src.schemas import BLOCKS_SCHEMA, TRANSACTIONS_SCHEMA, INPUTS_SCHEMA, OUTPUTS_SCHEMA\n",
    "\n",
    "# Konfiguration\n",
    "LOCAL_DATA_PATH = '/path/to/blockchair/data'  # Professor ändert nur diese Zeile\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.appName(\"Bitcoin Whale Analysis\").getOrCreate()\n",
    "\n",
    "# Daten laden (4 Zeilen!)\n",
    "blocks_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*blocks*.tsv\", sep='\\t', header=True, schema=BLOCKS_SCHEMA)\n",
    "transactions_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*transactions*.tsv\", sep='\\t', header=True, schema=TRANSACTIONS_SCHEMA)\n",
    "inputs_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*inputs*.tsv\", sep='\\t', header=True, schema=INPUTS_SCHEMA)\n",
    "outputs_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*outputs*.tsv\", sep='\\t', header=True, schema=OUTPUTS_SCHEMA)\n",
    "\n",
    "# Ab hier: Analyse-Code...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session beenden\n",
    "spark.stop()\n",
    "print(\"Spark Session beendet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
