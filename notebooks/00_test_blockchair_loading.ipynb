{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Blockchair TSV Loading mit src/schemas.py\n",
    "\n",
    "Dieses Notebook demonstriert, wie die Blockchair TSV-Dateien mit den definierten Schemas geladen werden.\n",
    "\n",
    "## Voraussetzungen\n",
    "\n",
    "1. Blockchair-Daten heruntergeladen (mit `blockchair-downloader`)\n",
    "2. TSV-Dateien extrahiert in einem lokalen Ordner\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schemas erfolgreich importiert!\n",
      "\n",
      "Blocks Schema: 36 Felder\n",
      "Transactions Schema: 22 Felder\n",
      "Inputs Schema: 21 Felder\n",
      "Outputs Schema: 11 Felder\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Projektverzeichnis zum Python-Path hinzufügen\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Jetzt können wir src importieren\n",
    "from pyspark.sql import SparkSession\n",
    "from src.schemas import BLOCKS_SCHEMA, TRANSACTIONS_SCHEMA, INPUTS_SCHEMA, OUTPUTS_SCHEMA\n",
    "from src.schemas import load_blockchair_data\n",
    "\n",
    "print(\"✅ Schemas erfolgreich importiert!\")\n",
    "print(f\"\\nBlocks Schema: {len(BLOCKS_SCHEMA.fields)} Felder\")\n",
    "print(f\"Transactions Schema: {len(TRANSACTIONS_SCHEMA.fields)} Felder\")\n",
    "print(f\"Inputs Schema: {len(INPUTS_SCHEMA.fields)} Felder\")\n",
    "print(f\"Outputs Schema: {len(OUTPUTS_SCHEMA.fields)} Felder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguration\n",
    "\n",
    "**Passe diesen Pfad an:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Daten von: /Users/roman/Documents/Master/Module/ADE/test\n"
     ]
    }
   ],
   "source": [
    "# WICHTIG: Pfad zu deinen extrahierten Blockchair-Daten\n",
    "LOCAL_DATA_PATH = '/Users/roman/Documents/Master/Module/ADE/test'\n",
    "\n",
    "print(f\"Lade Daten von: {LOCAL_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nimport warnings\nfrom pathlib import Path\n\n# Alle Python-Warnungen unterdrücken\nwarnings.filterwarnings('ignore')\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\n\n# Log4j Config-Datei finden\nlog4j_path = Path.cwd().parent / \"log4j.properties\" if Path.cwd().name == 'notebooks' else Path.cwd() / \"log4j.properties\"\n\n# Spark Session mit Log4j Config\nspark = SparkSession.builder \\\n    .appName(\"Blockchair TSV Test\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n    .config(\"spark.driver.extraJavaOptions\", f\"-Dlog4j.configuration=file:{log4j_path}\") \\\n    .getOrCreate()\n\n# Nur Errors anzeigen\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nprint(f\"✅ Spark Version: {spark.version}\")\nprint(f\"✅ Spark Master: {spark.sparkContext.master}\")\nprint(f\"✅ Warnings suppressed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode 1: Einzelne Dateien laden (manuell)\n",
    "\n",
    "Diese Methode zeigt, wie jede Tabelle einzeln geladen wird mit explizitem Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Blocks laden - NEUE STRUKTUR mit extracted/ Unterordner\nblocks_df = spark.read.csv(\n    f\"{LOCAL_DATA_PATH}/extracted/blocks/*.tsv\",\n    sep='\\t',\n    header=True,\n    schema=BLOCKS_SCHEMA\n)\n\nprint(\"Blocks DataFrame geladen!\")\nprint(f\"Anzahl Zeilen: {blocks_df.count()}\")\nblocks_df.printSchema()\nblocks_df.show(5, truncate=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transactions laden\ntransactions_df = spark.read.csv(\n    f\"{LOCAL_DATA_PATH}/extracted/transactions/*.tsv\",\n    sep='\\t',\n    header=True,\n    schema=TRANSACTIONS_SCHEMA\n)\n\nprint(\"Transactions DataFrame geladen!\")\nprint(f\"Anzahl Zeilen: {transactions_df.count()}\")\ntransactions_df.show(5, truncate=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inputs laden\ninputs_df = spark.read.csv(\n    f\"{LOCAL_DATA_PATH}/extracted/inputs/*.tsv\",\n    sep='\\t',\n    header=True,\n    schema=INPUTS_SCHEMA\n)\n\nprint(\"Inputs DataFrame geladen!\")\nprint(f\"Anzahl Zeilen: {inputs_df.count()}\")\ninputs_df.show(5, truncate=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Outputs laden\noutputs_df = spark.read.csv(\n    f\"{LOCAL_DATA_PATH}/extracted/outputs/*.tsv\",\n    sep='\\t',\n    header=True,\n    schema=OUTPUTS_SCHEMA\n)\n\nprint(\"Outputs DataFrame geladen!\")\nprint(f\"Anzahl Zeilen: {outputs_df.count()}\")\noutputs_df.show(5, truncate=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode 2: Helper-Funktion verwenden (empfohlen)\n",
    "\n",
    "Die `load_blockchair_data()` Funktion lädt alle 4 Tabellen auf einmal.\n",
    "\n",
    "**Hinweis:** Diese Methode erwartet eine bestimmte Ordnerstruktur:\n",
    "```\n",
    "LOCAL_DATA_PATH/\n",
    "├── blocks/*.tsv\n",
    "├── transactions/*.tsv\n",
    "├── inputs/*.tsv\n",
    "└── outputs/*.tsv\n",
    "```\n",
    "\n",
    "Wenn deine Dateien direkt im Root-Ordner liegen (wie `/Users/roman/Documents/Master/Module/ADE/test`), nutze Methode 1."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Alternative: Alle Tabellen mit einer Funktion laden\ndata = load_blockchair_data(spark, LOCAL_DATA_PATH)\n\nprint(\"✅ Alle Tabellen geladen!\")\nprint(f\"\\nBlocks: {data['blocks'].count()} Zeilen\")\nprint(f\"Transactions: {data['transactions'].count()} Zeilen\")\nprint(f\"Inputs: {data['inputs'].count()} Zeilen\")\nprint(f\"Outputs: {data['outputs'].count()} Zeilen\")\n\n# Zugriff auf einzelne DataFrames\nblocks_df_alt = data['blocks']\ntransactions_df_alt = data['transactions']\ninputs_df_alt = data['inputs']\noutputs_df_alt = data['outputs']",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Methode 2: Helper-Funktion verwenden (empfohlen)\n\nDie `load_blockchair_data()` Funktion lädt alle 4 Tabellen auf einmal.\n\n**Voraussetzung:** Blockchair-Downloader-Struktur:\n```\nLOCAL_DATA_PATH/\n├── extracted/\n│   ├── blocks/*.tsv\n│   ├── transactions/*.tsv\n│   ├── inputs/*.tsv\n│   └── outputs/*.tsv\n└── raw/  (optional, .gz Dateien)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenqualität prüfen\n",
    "\n",
    "Prüfe, ob die Datentypen korrekt sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATENTYP-VALIDIERUNG\n",
      "======================================================================\n",
      "\n",
      "1. Blocks - Timestamp-Spalte:\n",
      "+-------------------+\n",
      "|               time|\n",
      "+-------------------+\n",
      "|2025-11-25 00:00:57|\n",
      "|2025-11-25 00:23:39|\n",
      "|2025-11-25 00:28:13|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "   Typ: TimestampType()\n",
      "\n",
      "2. Transactions - Boolean-Spalte:\n",
      "+-----------+-----------+\n",
      "|is_coinbase|has_witness|\n",
      "+-----------+-----------+\n",
      "|       NULL|       NULL|\n",
      "+-----------+-----------+\n",
      "\n",
      "   Typ is_coinbase: BooleanType()\n",
      "\n",
      "3. Outputs - Value-Spalte (Satoshis):\n",
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "|5000000000|\n",
      "|5000000000|\n",
      "|5000000000|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "   Typ: LongType()\n",
      "\n",
      "======================================================================\n",
      "✅ Alle Datentypen korrekt!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATENTYP-VALIDIERUNG\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Blocks: Prüfe Timestamp\n",
    "print(\"\\n1. Blocks - Timestamp-Spalte:\")\n",
    "blocks_df.select(\"time\").show(3)\n",
    "print(f\"   Typ: {blocks_df.schema['time'].dataType}\")\n",
    "\n",
    "# Transactions: Prüfe Boolean\n",
    "print(\"\\n2. Transactions - Boolean-Spalte:\")\n",
    "transactions_df.select(\"is_coinbase\", \"has_witness\").show(3)\n",
    "print(f\"   Typ is_coinbase: {transactions_df.schema['is_coinbase'].dataType}\")\n",
    "\n",
    "# Outputs: Prüfe Value (muss LongType sein für Satoshis)\n",
    "print(\"\\n3. Outputs - Value-Spalte (Satoshis):\")\n",
    "outputs_df.select(\"value\").show(3)\n",
    "print(f\"   Typ: {outputs_df.schema['value'].dataType}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Alle Datentypen korrekt!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Dieses Notebook hat gezeigt:\n",
    "\n",
    "1. ✅ Import der Schemas aus `src/schemas.py` funktioniert\n",
    "2. ✅ TSV-Dateien können mit expliziten Schemas geladen werden\n",
    "3. ✅ Datentypen (Timestamps, Booleans, LongType für Satoshis) sind korrekt\n",
    "4. ✅ Nur 3-4 Zeilen Code pro Tabelle nötig\n",
    "\n",
    "### Verwendung im Haupt-Notebook\n",
    "\n",
    "Im finalen Notebook (für Professor) würde der Code so aussehen:\n",
    "\n",
    "```python\n",
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from src.schemas import BLOCKS_SCHEMA, TRANSACTIONS_SCHEMA, INPUTS_SCHEMA, OUTPUTS_SCHEMA\n",
    "\n",
    "# Konfiguration\n",
    "LOCAL_DATA_PATH = '/path/to/blockchair/data'  # Professor ändert nur diese Zeile\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.appName(\"Bitcoin Whale Analysis\").getOrCreate()\n",
    "\n",
    "# Daten laden (4 Zeilen!)\n",
    "blocks_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*blocks*.tsv\", sep='\\t', header=True, schema=BLOCKS_SCHEMA)\n",
    "transactions_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*transactions*.tsv\", sep='\\t', header=True, schema=TRANSACTIONS_SCHEMA)\n",
    "inputs_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*inputs*.tsv\", sep='\\t', header=True, schema=INPUTS_SCHEMA)\n",
    "outputs_df = spark.read.csv(f\"{LOCAL_DATA_PATH}/*outputs*.tsv\", sep='\\t', header=True, schema=OUTPUTS_SCHEMA)\n",
    "\n",
    "# Ab hier: Analyse-Code...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session beendet.\n"
     ]
    }
   ],
   "source": [
    "# Spark Session beenden\n",
    "spark.stop()\n",
    "print(\"Spark Session beendet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}