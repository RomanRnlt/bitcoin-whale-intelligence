{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Whale Intelligence Analysis\n",
    "\n",
    "**Standalone Analysis Notebook** - No external dependencies except Python, PySpark, and GraphFrames.\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| 1. Setup & Configuration | Imports, paths, and Spark initialization |\n",
    "| 2. Helper Functions | All ETL functions (inline, no external imports) |\n",
    "| 3. Data Loading | Load blockchain data from bitcoin-etl exports |\n",
    "| 4. Entity Clustering | Group addresses using graph analysis |\n",
    "| 5. Results & Visualization | Whale identification and charts |\n",
    "| 6. Executive Summary | Key findings and metrics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, desc, when, round as spark_round,\n",
    "    explode, explode_outer, collect_set, size as spark_size,\n",
    "    from_unixtime, to_timestamp, element_at\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType, IntegerType,\n",
    "    BooleanType, ArrayType\n",
    ")\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6),\n",
    "    'figure.dpi': 100,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 11,\n",
    "    'font.size': 10\n",
    "})\n",
    "COLORS = {'primary': '#2E86AB', 'secondary': '#A23B72', 'accent': '#F18F01', 'dark': '#1B1B1E'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION - ADJUST PATHS FOR YOUR SYSTEM\n",
    "# ==============================================================================\n",
    "\n",
    "SYSTEM = platform.system()\n",
    "\n",
    "# Path to bitcoin-etl exported data\n",
    "# Examples:\n",
    "#   Windows: r\"C:\\Users\\YourName\\blockchain_exports\"\n",
    "#   macOS:   \"/Users/YourName/blockchain_exports\"\n",
    "#   Linux:   \"/home/YourName/blockchain_exports\"\n",
    "\n",
    "if SYSTEM == \"Windows\":\n",
    "    BLOCKCHAIN_DATA_PATH = r\"C:\\Users\\YourName\\blockchain_exports\"  # <-- CHANGE THIS\n",
    "    OUTPUT_PATH = r\"C:\\Users\\YourName\\bitcoin_analysis_output\"       # <-- CHANGE THIS\n",
    "elif SYSTEM == \"Darwin\":  # macOS\n",
    "    BLOCKCHAIN_DATA_PATH = \"/Users/roman/spark_project/blockchain_exports\"  # <-- CHANGE THIS\n",
    "    OUTPUT_PATH = \"/Users/roman/spark_project/bitcoin-whale-intelligence/data\"\n",
    "else:  # Linux\n",
    "    BLOCKCHAIN_DATA_PATH = \"/home/YourName/blockchain_exports\"  # <-- CHANGE THIS\n",
    "    OUTPUT_PATH = \"/home/YourName/bitcoin_analysis_output\"\n",
    "\n",
    "DRIVER_MEMORY = \"16g\"\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"System: {SYSTEM}\")\n",
    "print(f\"Data source: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Helper Functions\n",
    "\n",
    "All ETL functions defined inline for standalone execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SCHEMAS FOR BITCOIN-ETL JSON DATA\n",
    "# ==============================================================================\n",
    "\n",
    "INPUT_SCHEMA = StructType([\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"spent_transaction_hash\", StringType(), True),\n",
    "    StructField(\"spent_output_index\", IntegerType(), True),\n",
    "    StructField(\"script_asm\", StringType(), True),\n",
    "    StructField(\"script_hex\", StringType(), True),\n",
    "    StructField(\"sequence\", LongType(), True),\n",
    "    StructField(\"required_signatures\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"addresses\", ArrayType(StringType()), True),\n",
    "    StructField(\"value\", LongType(), True),\n",
    "])\n",
    "\n",
    "OUTPUT_SCHEMA = StructType([\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"script_asm\", StringType(), True),\n",
    "    StructField(\"script_hex\", StringType(), True),\n",
    "    StructField(\"required_signatures\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"addresses\", ArrayType(StringType()), True),\n",
    "    StructField(\"value\", LongType(), True),\n",
    "])\n",
    "\n",
    "TRANSACTION_SCHEMA = StructType([\n",
    "    StructField(\"hash\", StringType(), False),\n",
    "    StructField(\"size\", IntegerType(), True),\n",
    "    StructField(\"virtual_size\", IntegerType(), True),\n",
    "    StructField(\"version\", IntegerType(), True),\n",
    "    StructField(\"lock_time\", LongType(), True),\n",
    "    StructField(\"block_number\", LongType(), True),\n",
    "    StructField(\"block_hash\", StringType(), True),\n",
    "    StructField(\"block_timestamp\", LongType(), True),\n",
    "    StructField(\"is_coinbase\", BooleanType(), True),\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"inputs\", ArrayType(INPUT_SCHEMA), True),\n",
    "    StructField(\"outputs\", ArrayType(OUTPUT_SCHEMA), True),\n",
    "    StructField(\"input_count\", IntegerType(), True),\n",
    "    StructField(\"output_count\", IntegerType(), True),\n",
    "    StructField(\"input_value\", LongType(), True),\n",
    "    StructField(\"output_value\", LongType(), True),\n",
    "    StructField(\"fee\", LongType(), True),\n",
    "])\n",
    "\n",
    "BLOCK_SCHEMA = StructType([\n",
    "    StructField(\"hash\", StringType(), False),\n",
    "    StructField(\"size\", IntegerType(), True),\n",
    "    StructField(\"stripped_size\", IntegerType(), True),\n",
    "    StructField(\"weight\", IntegerType(), True),\n",
    "    StructField(\"number\", LongType(), True),\n",
    "    StructField(\"version\", IntegerType(), True),\n",
    "    StructField(\"merkle_root\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"nonce\", StringType(), True),\n",
    "    StructField(\"bits\", StringType(), True),\n",
    "    StructField(\"coinbase_param\", StringType(), True),\n",
    "    StructField(\"transaction_count\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "print(\"Schemas defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ETL FUNCTIONS (STANDALONE - NO EXTERNAL IMPORTS)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_spark_session(app_name=\"Bitcoin Whale Intelligence\", driver_memory=\"8g\",\n",
    "                         enable_graphframes=True, suppress_logs=True):\n",
    "    \"\"\"\n",
    "    Create an optimized Spark session for Bitcoin data processing.\n",
    "    \"\"\"\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", driver_memory) \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "\n",
    "    if enable_graphframes:\n",
    "        builder = builder.config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\"\n",
    "        )\n",
    "\n",
    "    if suppress_logs:\n",
    "        devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "        old_stdout_fd = os.dup(1)\n",
    "        old_stderr_fd = os.dup(2)\n",
    "        os.dup2(devnull, 1)\n",
    "        os.dup2(devnull, 2)\n",
    "        try:\n",
    "            spark = builder.getOrCreate()\n",
    "            spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        finally:\n",
    "            os.dup2(old_stdout_fd, 1)\n",
    "            os.dup2(old_stderr_fd, 2)\n",
    "            os.close(devnull)\n",
    "            os.close(old_stdout_fd)\n",
    "            os.close(old_stderr_fd)\n",
    "    else:\n",
    "        spark = builder.getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "def load_transactions(spark, base_path, use_schema=True):\n",
    "    \"\"\"\n",
    "    Load bitcoin-etl transaction data from Hive-partitioned JSON files.\n",
    "    \"\"\"\n",
    "    base = Path(base_path)\n",
    "    batch_folders = [d for d in base.iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "\n",
    "    if not batch_folders:\n",
    "        raise ValueError(f\"No batch folders found in {base_path}\")\n",
    "\n",
    "    tx_paths = []\n",
    "    for batch in batch_folders:\n",
    "        tx_path = batch / \"transactions\"\n",
    "        if tx_path.exists():\n",
    "            tx_paths.append(str(tx_path))\n",
    "\n",
    "    if not tx_paths:\n",
    "        raise ValueError(\"No transactions/ folders found\")\n",
    "\n",
    "    if use_schema:\n",
    "        df = spark.read.schema(TRANSACTION_SCHEMA).json(tx_paths)\n",
    "    else:\n",
    "        df = spark.read.json(tx_paths)\n",
    "\n",
    "    df = df.withColumn(\"block_datetime\", to_timestamp(from_unixtime(col(\"block_timestamp\"))))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_blocks(spark, base_path, use_schema=True):\n",
    "    \"\"\"\n",
    "    Load bitcoin-etl block data from Hive-partitioned JSON files.\n",
    "    \"\"\"\n",
    "    base = Path(base_path)\n",
    "    batch_folders = [d for d in base.iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "\n",
    "    block_paths = []\n",
    "    for batch in batch_folders:\n",
    "        block_path = batch / \"blocks\"\n",
    "        if block_path.exists():\n",
    "            block_paths.append(str(block_path))\n",
    "\n",
    "    if not block_paths:\n",
    "        raise ValueError(\"No blocks/ folders found\")\n",
    "\n",
    "    if use_schema:\n",
    "        df = spark.read.schema(BLOCK_SCHEMA).json(block_paths)\n",
    "    else:\n",
    "        df = spark.read.json(block_paths)\n",
    "\n",
    "    df = df.withColumn(\"timestamp_dt\", to_timestamp(from_unixtime(col(\"timestamp\"))))\n",
    "    return df\n",
    "\n",
    "\n",
    "def explode_outputs(tx_df):\n",
    "    \"\"\"\n",
    "    Transform nested outputs to a flat table.\n",
    "    \"\"\"\n",
    "    return tx_df \\\n",
    "        .select(\n",
    "            col(\"hash\").alias(\"tx_hash\"),\n",
    "            col(\"block_number\"),\n",
    "            col(\"block_timestamp\"),\n",
    "            explode_outer(\"outputs\").alias(\"output\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            \"tx_hash\",\n",
    "            \"block_number\",\n",
    "            \"block_timestamp\",\n",
    "            col(\"output.index\").alias(\"output_index\"),\n",
    "            col(\"output.value\").alias(\"value\"),\n",
    "            col(\"output.addresses\").alias(\"addresses\"),\n",
    "            col(\"output.type\").alias(\"output_type\"),\n",
    "        )\n",
    "\n",
    "\n",
    "def explode_inputs(tx_df):\n",
    "    \"\"\"\n",
    "    Transform nested inputs to a flat table.\n",
    "    \"\"\"\n",
    "    return tx_df \\\n",
    "        .select(\n",
    "            col(\"hash\").alias(\"tx_hash\"),\n",
    "            col(\"block_number\"),\n",
    "            col(\"block_timestamp\"),\n",
    "            col(\"is_coinbase\"),\n",
    "            explode_outer(\"inputs\").alias(\"input\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            \"tx_hash\",\n",
    "            \"block_number\",\n",
    "            \"block_timestamp\",\n",
    "            \"is_coinbase\",\n",
    "            col(\"input.index\").alias(\"input_index\"),\n",
    "            col(\"input.spent_transaction_hash\").alias(\"spent_tx_hash\"),\n",
    "            col(\"input.spent_output_index\").alias(\"spent_output_index\"),\n",
    "            col(\"input.addresses\").alias(\"addresses\"),\n",
    "            col(\"input.value\").alias(\"value\"),\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_utxo_set(outputs_df, inputs_df):\n",
    "    \"\"\"\n",
    "    Compute UTXO Set (Unspent Transaction Outputs).\n",
    "    UTXO = Outputs LEFT ANTI JOIN Inputs\n",
    "    \"\"\"\n",
    "    spent_refs = inputs_df \\\n",
    "        .filter(col(\"is_coinbase\") == False) \\\n",
    "        .select(\n",
    "            col(\"spent_tx_hash\").alias(\"ref_tx_hash\"),\n",
    "            col(\"spent_output_index\").alias(\"ref_output_index\")\n",
    "        ) \\\n",
    "        .distinct()\n",
    "\n",
    "    utxos = outputs_df.join(\n",
    "        spent_refs,\n",
    "        on=[\n",
    "            outputs_df.tx_hash == spent_refs.ref_tx_hash,\n",
    "            outputs_df.output_index == spent_refs.ref_output_index\n",
    "        ],\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    return utxos\n",
    "\n",
    "\n",
    "def enrich_clustering_inputs(tx_df, outputs_df, min_inputs=2, max_inputs=50):\n",
    "    \"\"\"\n",
    "    Enrich inputs from multi-input transactions with addresses for clustering.\n",
    "    \"\"\"\n",
    "    multi_input_txs = tx_df \\\n",
    "        .filter(\n",
    "            (col(\"input_count\") >= min_inputs) &\n",
    "            (col(\"input_count\") <= max_inputs) &\n",
    "            (col(\"is_coinbase\") == False)\n",
    "        )\n",
    "\n",
    "    inputs_exploded = multi_input_txs \\\n",
    "        .select(\n",
    "            col(\"hash\").alias(\"tx_hash\"),\n",
    "            explode(\"inputs\").alias(\"input\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            \"tx_hash\",\n",
    "            col(\"input.spent_transaction_hash\").alias(\"spent_tx_hash\"),\n",
    "            col(\"input.spent_output_index\").alias(\"spent_output_index\"),\n",
    "            col(\"input.addresses\").alias(\"raw_addresses\"),\n",
    "        )\n",
    "\n",
    "    output_lookup = outputs_df \\\n",
    "        .select(\n",
    "            col(\"tx_hash\").alias(\"source_tx_hash\"),\n",
    "            col(\"output_index\").alias(\"source_output_index\"),\n",
    "            col(\"addresses\").alias(\"source_addresses\"),\n",
    "        )\n",
    "\n",
    "    enriched = inputs_exploded.join(\n",
    "        output_lookup,\n",
    "        on=[\n",
    "            inputs_exploded.spent_tx_hash == output_lookup.source_tx_hash,\n",
    "            inputs_exploded.spent_output_index == output_lookup.source_output_index\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    enriched = enriched.withColumn(\n",
    "        \"address\",\n",
    "        when(\n",
    "            (col(\"source_addresses\").isNotNull()) & (spark_size(col(\"source_addresses\")) > 0),\n",
    "            element_at(col(\"source_addresses\"), 1)\n",
    "        ).otherwise(\n",
    "            when(\n",
    "                (col(\"raw_addresses\").isNotNull()) & (spark_size(col(\"raw_addresses\")) > 0),\n",
    "                element_at(col(\"raw_addresses\"), 1)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    result = enriched \\\n",
    "        .filter(col(\"address\").isNotNull()) \\\n",
    "        .select(\"tx_hash\", \"address\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"All helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize Spark\n",
    "spark = create_spark_session(app_name=\"Bitcoin Whale Analysis\", driver_memory=DRIVER_MEMORY, enable_graphframes=True)\n",
    "spark.sparkContext.setCheckpointDir(str(Path(OUTPUT_PATH) / \"checkpoints\"))\n",
    "print(f\"Spark {spark.version} initialized | UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading\n",
    "\n",
    "Loading Bitcoin blockchain data exported via [bitcoin-etl](https://github.com/blockchain-etl/bitcoin-etl).\n",
    "\n",
    "**Key Concept - UTXOs:** Unspent Transaction Outputs are Bitcoin's way of tracking balances. Unlike bank accounts, Bitcoin has no \"balance\" field - your wealth is the sum of all unspent outputs you control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load transactions and blocks\n",
    "tx_df = load_transactions(spark, BLOCKCHAIN_DATA_PATH).cache()\n",
    "blocks_df = load_blocks(spark, BLOCKCHAIN_DATA_PATH).cache()\n",
    "\n",
    "TX_COUNT = tx_df.count()\n",
    "BLOCK_COUNT = blocks_df.count()\n",
    "\n",
    "print(f\"Loaded {TX_COUNT:,} transactions from {BLOCK_COUNT:,} blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Explode outputs/inputs and compute UTXO set\n",
    "outputs_df = explode_outputs(tx_df).cache()\n",
    "inputs_df = explode_inputs(tx_df).cache()\n",
    "utxo_df = compute_utxo_set(outputs_df, inputs_df).cache()\n",
    "\n",
    "OUTPUT_COUNT = outputs_df.count()\n",
    "UTXO_COUNT = utxo_df.count()\n",
    "SPENT_COUNT = OUTPUT_COUNT - UTXO_COUNT\n",
    "\n",
    "print(f\"Outputs: {OUTPUT_COUNT:,} | Spent: {SPENT_COUNT:,} ({SPENT_COUNT/OUTPUT_COUNT*100:.1f}%) | UTXOs: {UTXO_COUNT:,} ({UTXO_COUNT/OUTPUT_COUNT*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction input analysis\n",
    "input_dist = tx_df.filter(col(\"is_coinbase\") == False) \\\n",
    "    .groupBy(\"input_count\").agg(count(\"*\").alias(\"tx_count\")) \\\n",
    "    .orderBy(\"input_count\").toPandas()\n",
    "\n",
    "total_non_coinbase = input_dist['tx_count'].sum()\n",
    "single_input = input_dist[input_dist['input_count'] == 1]['tx_count'].sum()\n",
    "multi_input = input_dist[input_dist['input_count'] > 1]['tx_count'].sum()\n",
    "\n",
    "print(f\"Transaction Input Analysis\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total (non-coinbase): {total_non_coinbase:,}\")\n",
    "print(f\"Single-input:         {single_input:,} ({single_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"Multi-input:          {multi_input:,} ({multi_input/total_non_coinbase*100:.1f}%) <- usable for clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Entity Clustering\n",
    "\n",
    "Using **Connected Components** algorithm on a graph where:\n",
    "- **Nodes** = Bitcoin addresses\n",
    "- **Edges** = Addresses used together in multi-input transactions\n",
    "\n",
    "All connected addresses belong to the same entity (wallet/person/organization).\n",
    "\n",
    "**Common Input Ownership Heuristic**: If multiple addresses are used as inputs in the same transaction, they are controlled by the same entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepare inputs for clustering\n",
    "clustering_inputs = enrich_clustering_inputs(tx_df, outputs_df, min_inputs=2, max_inputs=50).cache()\n",
    "print(f\"Enriched inputs for clustering: {clustering_inputs.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build graph edges from co-input relationships\n",
    "tx_addresses = clustering_inputs.groupBy(\"tx_hash\").agg(collect_set(\"address\").alias(\"addresses\"))\n",
    "tx_addresses = tx_addresses.filter(spark_size(\"addresses\") >= 2)\n",
    "\n",
    "def create_edges_udf(addresses):\n",
    "    if not addresses or len(addresses) < 2:\n",
    "        return []\n",
    "    return [(a, b) for a, b in combinations(sorted(addresses), 2)]\n",
    "\n",
    "edge_schema = ArrayType(StructType([StructField(\"src\", StringType()), StructField(\"dst\", StringType())]))\n",
    "create_edges = udf(create_edges_udf, edge_schema)\n",
    "\n",
    "edges_df = tx_addresses.withColumn(\"edges\", create_edges(\"addresses\")) \\\n",
    "    .select(explode(\"edges\").alias(\"edge\")) \\\n",
    "    .select(col(\"edge.src\").alias(\"src\"), col(\"edge.dst\").alias(\"dst\")).distinct()\n",
    "\n",
    "# Persist to disk for checkpointing\n",
    "edges_path = str(Path(OUTPUT_PATH) / \"edges_temp.parquet\")\n",
    "edges_df.write.mode(\"overwrite\").parquet(edges_path)\n",
    "edges_df = spark.read.parquet(edges_path).cache()\n",
    "\n",
    "vertices_df = edges_df.select(col(\"src\").alias(\"id\")).union(edges_df.select(col(\"dst\").alias(\"id\"))).distinct()\n",
    "vertices_path = str(Path(OUTPUT_PATH) / \"vertices_temp.parquet\")\n",
    "vertices_df.write.mode(\"overwrite\").parquet(vertices_path)\n",
    "vertices_df = spark.read.parquet(vertices_path).cache()\n",
    "\n",
    "EDGE_COUNT = edges_df.count()\n",
    "VERTEX_COUNT = vertices_df.count()\n",
    "print(f\"Graph: {VERTEX_COUNT:,} vertices (addresses) | {EDGE_COUNT:,} edges (co-input pairs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run Connected Components algorithm\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "entities_df = graph.connectedComponents(algorithm=\"graphframes\", checkpointInterval=1, broadcastThreshold=100000)\n",
    "\n",
    "# Persist results\n",
    "entities_path = str(Path(OUTPUT_PATH) / \"entities_temp.parquet\")\n",
    "entities_df.write.mode(\"overwrite\").parquet(entities_path)\n",
    "entities_df = spark.read.parquet(entities_path).cache()\n",
    "\n",
    "ADDRESS_COUNT = entities_df.count()\n",
    "ENTITY_COUNT = entities_df.select(\"component\").distinct().count()\n",
    "REDUCTION = (1 - ENTITY_COUNT/ADDRESS_COUNT) * 100\n",
    "\n",
    "print(f\"\\nClustering Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Addresses analyzed: {ADDRESS_COUNT:,}\")\n",
    "print(f\"Entities identified: {ENTITY_COUNT:,}\")\n",
    "print(f\"Reduction: {REDUCTION:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results & Visualization\n",
    "\n",
    "Calculating entity balances by joining the UTXO set with entity mappings. A **whale** is defined as an entity holding significant BTC (>1,000 BTC in this analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate entity balances\n",
    "entities_final = entities_df.select(col(\"id\").alias(\"address\"), col(\"component\").alias(\"entity_id\"))\n",
    "\n",
    "utxo_exploded = utxo_df.select(col(\"tx_hash\"), col(\"output_index\"), col(\"value\"), explode(col(\"addresses\")).alias(\"address\"))\n",
    "\n",
    "utxo_with_entities = utxo_exploded.join(entities_final, \"address\", \"inner\").select(\"entity_id\", \"value\")\n",
    "\n",
    "entity_balances = utxo_with_entities.groupBy(\"entity_id\").agg(\n",
    "    spark_sum(\"value\").alias(\"balance_satoshi\"),\n",
    "    count(\"*\").alias(\"utxo_count\")\n",
    ").withColumn(\"balance_btc\", spark_round(col(\"balance_satoshi\") / 100000000, 8)).orderBy(desc(\"balance_btc\")).cache()\n",
    "\n",
    "ENTITIES_WITH_BALANCE = entity_balances.count()\n",
    "TOTAL_BTC = entity_balances.agg(spark_sum(\"balance_btc\")).collect()[0][0]\n",
    "\n",
    "print(f\"Entity Balance Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Entities with balance: {ENTITIES_WITH_BALANCE:,}\")\n",
    "print(f\"Total BTC in UTXOs: {TOTAL_BTC:,.2f} BTC\")\n",
    "print(f\"Average per entity: {TOTAL_BTC/ENTITIES_WITH_BALANCE:.4f} BTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Whales\n",
    "top_whales = entity_balances.limit(20).toPandas()\n",
    "TOP_20_BTC = top_whales['balance_btc'].sum()\n",
    "TOP_20_SHARE = TOP_20_BTC / TOTAL_BTC * 100\n",
    "\n",
    "print(f\"Top 20 Whales\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Rank':<6} {'Entity ID':<20} {'Balance (BTC)':<18} {'UTXOs':<10} {'Share'}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for i, row in top_whales.iterrows():\n",
    "    share = row['balance_btc'] / TOTAL_BTC * 100\n",
    "    print(f\"#{i+1:<5} {int(row['entity_id']):<20} {row['balance_btc']:>15,.2f}   {int(row['utxo_count']):>8}   {share:>5.2f}%\")\n",
    "print(f\"{'-'*70}\")\n",
    "print(f\"{'Top 20 Total:':<27} {TOP_20_BTC:>15,.2f} BTC         {TOP_20_SHARE:>5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity categorization\n",
    "entity_categories = entity_balances.withColumn(\"category\",\n",
    "    when(col(\"balance_btc\") >= 1000, \"Mega Whale (1000+ BTC)\")\n",
    "    .when(col(\"balance_btc\") >= 100, \"Whale (100-1000 BTC)\")\n",
    "    .when(col(\"balance_btc\") >= 10, \"Large (10-100 BTC)\")\n",
    "    .when(col(\"balance_btc\") >= 1, \"Medium (1-10 BTC)\")\n",
    "    .otherwise(\"Small (<1 BTC)\")\n",
    ")\n",
    "\n",
    "category_stats = entity_categories.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"entity_count\"),\n",
    "    spark_sum(\"balance_btc\").alias(\"total_btc\")\n",
    ").orderBy(desc(\"total_btc\")).toPandas()\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "print(category_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Whale Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Top 10 Whales\n",
    "top_10 = entity_balances.limit(10).toPandas()\n",
    "bars = axes[0].barh(range(len(top_10)), top_10['balance_btc'], color=COLORS['primary'])\n",
    "axes[0].set_xlabel('Balance (BTC)')\n",
    "axes[0].set_ylabel('Entity Rank')\n",
    "axes[0].set_title('Top 10 Whales by Balance')\n",
    "axes[0].set_yticks(range(len(top_10)))\n",
    "axes[0].set_yticklabels([f\"#{i+1}\" for i in range(len(top_10))])\n",
    "axes[0].invert_yaxis()\n",
    "for bar, val in zip(bars, top_10['balance_btc']):\n",
    "    axes[0].text(val + val*0.02, bar.get_y() + bar.get_height()/2, f'{val:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "# Entity Distribution by Category\n",
    "colors_pie = [COLORS['secondary'], COLORS['accent'], COLORS['primary'], '#6B7280', '#D1D5DB']\n",
    "axes[1].pie(category_stats['entity_count'], labels=category_stats['category'], autopct='%1.1f%%',\n",
    "            colors=colors_pie[:len(category_stats)], startangle=90, textprops={'fontsize': 9})\n",
    "axes[1].set_title('Entity Distribution by Category')\n",
    "\n",
    "# BTC Distribution by Category\n",
    "axes[2].pie(category_stats['total_btc'], labels=category_stats['category'], autopct='%1.1f%%',\n",
    "            colors=colors_pie[:len(category_stats)], startangle=90, textprops={'fontsize': 9})\n",
    "axes[2].set_title('BTC Distribution by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(OUTPUT_PATH) / 'whale_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation: Extreme wealth concentration - Mega Whales are <1% of entities but hold >60% of BTC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "entities_final.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"entities.parquet\"))\n",
    "utxo_df.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"utxos.parquet\"))\n",
    "outputs_df.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"outputs.parquet\"))\n",
    "inputs_df.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"inputs.parquet\"))\n",
    "\n",
    "print(f\"Data exported to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary metrics\n",
    "mega_whales = category_stats[category_stats['category'] == 'Mega Whale (1000+ BTC)']\n",
    "MEGA_WHALE_COUNT = int(mega_whales['entity_count'].values[0]) if len(mega_whales) > 0 else 0\n",
    "MEGA_WHALE_BTC = float(mega_whales['total_btc'].values[0]) if len(mega_whales) > 0 else 0\n",
    "MEGA_WHALE_SHARE = MEGA_WHALE_BTC / TOTAL_BTC * 100 if TOTAL_BTC > 0 else 0\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*70}\n",
    "                    BITCOIN WHALE INTELLIGENCE REPORT\n",
    "{'='*70}\n",
    "\n",
    "DATA SCOPE\n",
    "{'-'*70}\n",
    "  Blocks analyzed:        {BLOCK_COUNT:>15,}\n",
    "  Transactions:           {TX_COUNT:>15,}\n",
    "  Outputs created:        {OUTPUT_COUNT:>15,}\n",
    "  UTXOs (unspent):        {UTXO_COUNT:>15,}\n",
    "\n",
    "CLUSTERING RESULTS\n",
    "{'-'*70}\n",
    "  Addresses clustered:    {ADDRESS_COUNT:>15,}\n",
    "  Entities identified:    {ENTITY_COUNT:>15,}\n",
    "  Clustering reduction:   {REDUCTION:>14.1f}%\n",
    "\n",
    "WHALE ANALYSIS\n",
    "{'-'*70}\n",
    "  Total BTC tracked:      {TOTAL_BTC:>15,.2f} BTC\n",
    "  Mega Whales (1000+ BTC):{MEGA_WHALE_COUNT:>15,}\n",
    "  Mega Whale holdings:    {MEGA_WHALE_BTC:>15,.2f} BTC ({MEGA_WHALE_SHARE:.1f}%)\n",
    "  Top 20 concentration:   {TOP_20_BTC:>15,.2f} BTC ({TOP_20_SHARE:.1f}%)\n",
    "\n",
    "KEY INSIGHTS\n",
    "{'-'*70}\n",
    "  1. High concentration: Top 20 entities control {TOP_20_SHARE:.1f}% of tracked BTC\n",
    "  2. Power-law distribution: Few large whales, many small entities\n",
    "  3. Clustering effective: {REDUCTION:.1f}% address reduction achieved\n",
    "\n",
    "LIKELY WHALE IDENTITIES\n",
    "{'-'*70}\n",
    "  - Mega Whales (1000+ BTC): Early adopters, exchanges, institutional\n",
    "  - Whales (100-1000 BTC): Mining pools, large traders, services\n",
    "  - Large (10-100 BTC): Active traders, businesses, wealthy individuals\n",
    "\n",
    "{'='*70}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Key Metrics Overview\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "metrics = [\n",
    "    f'Transactions\\n{TX_COUNT:,}',\n",
    "    f'Addresses\\n{ADDRESS_COUNT:,}',\n",
    "    f'Entities\\n{ENTITY_COUNT:,}',\n",
    "    f'Total BTC\\n{TOTAL_BTC:,.0f}',\n",
    "    f'Mega Whales\\n{MEGA_WHALE_COUNT}'\n",
    "]\n",
    "values = [TX_COUNT, ADDRESS_COUNT, ENTITY_COUNT, TOTAL_BTC, MEGA_WHALE_COUNT]\n",
    "normalized = [v / max(values) for v in values]\n",
    "\n",
    "bars = ax.bar(metrics, normalized, color=[COLORS['primary'], COLORS['secondary'], COLORS['accent'], COLORS['dark'], COLORS['secondary']])\n",
    "ax.set_ylabel('Relative Scale', fontsize=12)\n",
    "ax.set_title('Bitcoin Whale Intelligence: Key Metrics Overview', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 1.15)\n",
    "\n",
    "for bar, val, norm in zip(bars, values, normalized):\n",
    "    if val >= 1000:\n",
    "        label = f'{val:,.0f}'\n",
    "    else:\n",
    "        label = str(int(val))\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, norm + 0.03, label, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.text(0.5, -0.15, f'Top 20 Whale Concentration: {TOP_20_SHARE:.1f}% of all BTC | Clustering Reduction: {REDUCTION:.1f}%',\n",
    "        transform=ax.transAxes, ha='center', fontsize=12, style='italic', color=COLORS['dark'])\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(OUTPUT_PATH) / 'executive_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark session (optional)\n",
    "# spark.stop()\n",
    "print(\"Analysis complete. Spark session still active for further exploration.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
