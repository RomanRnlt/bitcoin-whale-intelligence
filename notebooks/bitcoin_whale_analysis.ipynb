{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Whale Intelligence Analysis\n",
    "\n",
    "**Objective**: Identify and analyze Bitcoin whales by clustering addresses into entities using the Common Input Ownership Heuristic.\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| 1. Setup | Configuration and imports |\n",
    "| 2. Data Loading | Load blockchain data from bitcoin-etl exports |\n",
    "| 3. Data Exploration | Quick overview of transaction patterns |\n",
    "| 4. Entity Clustering | Group addresses using graph analysis |\n",
    "| 5. Results & Insights | Whale identification and visualization |\n",
    "| 6. Executive Summary | Key findings and business implications |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, desc, when, round as spark_round,\n",
    "    explode, collect_set, size as spark_size\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.etl import (\n",
    "    create_spark_session, load_transactions, load_blocks,\n",
    "    explode_outputs, explode_inputs, compute_utxo_set, enrich_clustering_inputs\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6),\n",
    "    'figure.dpi': 100,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 11,\n",
    "    'font.size': 10\n",
    "})\n",
    "COLORS = {'primary': '#2E86AB', 'secondary': '#A23B72', 'accent': '#F18F01', 'dark': '#1B1B1E'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKCHAIN_DATA_PATH = \"/Users/roman/spark_project/blockchain_exports\"\n",
    "OUTPUT_PATH = str(project_root / \"data\")\n",
    "DRIVER_MEMORY = \"16g\"\n",
    "\n",
    "Path(OUTPUT_PATH).mkdir(exist_ok=True)\n",
    "print(f\"Data source: {BLOCKCHAIN_DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spark = create_spark_session(app_name=\"Bitcoin Whale Analysis\", driver_memory=DRIVER_MEMORY, enable_graphframes=True)\n",
    "spark.sparkContext.setCheckpointDir(str(Path(OUTPUT_PATH) / \"checkpoints\"))\n",
    "print(f\"Spark {spark.version} initialized | UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading\n",
    "\n",
    "Loading Bitcoin blockchain data exported via [bitcoin-etl](https://github.com/blockchain-etl/bitcoin-etl). The data contains nested JSON structures for transaction inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tx_df = load_transactions(spark, BLOCKCHAIN_DATA_PATH).cache()\n",
    "blocks_df = load_blocks(spark, BLOCKCHAIN_DATA_PATH).cache()\n",
    "\n",
    "TX_COUNT = tx_df.count()\n",
    "BLOCK_COUNT = blocks_df.count()\n",
    "\n",
    "print(f\"Loaded {TX_COUNT:,} transactions from {BLOCK_COUNT:,} blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs_df = explode_outputs(tx_df).cache()\n",
    "inputs_df = explode_inputs(tx_df).cache()\n",
    "utxo_df = compute_utxo_set(outputs_df, inputs_df).cache()\n",
    "\n",
    "OUTPUT_COUNT = outputs_df.count()\n",
    "UTXO_COUNT = utxo_df.count()\n",
    "SPENT_COUNT = OUTPUT_COUNT - UTXO_COUNT\n",
    "\n",
    "print(f\"Outputs: {OUTPUT_COUNT:,} | Spent: {SPENT_COUNT:,} ({SPENT_COUNT/OUTPUT_COUNT*100:.1f}%) | UTXOs: {UTXO_COUNT:,} ({UTXO_COUNT/OUTPUT_COUNT*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Exploration\n",
    "\n",
    "Analyzing transaction patterns to understand the clustering potential. Multi-input transactions (>=2 inputs) reveal address ownership through the **Common Input Ownership Heuristic**: addresses used together as inputs are controlled by the same entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dist = tx_df.filter(col(\"is_coinbase\") == False) \\\n",
    "    .groupBy(\"input_count\").agg(count(\"*\").alias(\"tx_count\")) \\\n",
    "    .orderBy(\"input_count\").toPandas()\n",
    "\n",
    "total_non_coinbase = input_dist['tx_count'].sum()\n",
    "single_input = input_dist[input_dist['input_count'] == 1]['tx_count'].sum()\n",
    "multi_input = input_dist[input_dist['input_count'] > 1]['tx_count'].sum()\n",
    "\n",
    "print(f\"Transaction Input Analysis\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total (non-coinbase): {total_non_coinbase:,}\")\n",
    "print(f\"Single-input:         {single_input:,} ({single_input/total_non_coinbase*100:.1f}%)\")\n",
    "print(f\"Multi-input:          {multi_input:,} ({multi_input/total_non_coinbase*100:.1f}%) <- usable for clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "plot_data = input_dist[input_dist['input_count'] <= 20]\n",
    "\n",
    "axes[0].bar(plot_data['input_count'], plot_data['tx_count'], color=COLORS['primary'], edgecolor='white')\n",
    "axes[0].set_xlabel('Inputs per Transaction')\n",
    "axes[0].set_ylabel('Number of Transactions')\n",
    "axes[0].set_title('Input Count Distribution (Linear Scale)')\n",
    "axes[0].annotate(f'{int(single_input):,}', xy=(1, single_input), xytext=(4, single_input*0.7),\n",
    "                 fontsize=9, arrowprops=dict(arrowstyle='->', color=COLORS['secondary']))\n",
    "\n",
    "axes[1].bar(plot_data['input_count'], plot_data['tx_count'], color=COLORS['primary'], edgecolor='white')\n",
    "axes[1].set_xlabel('Inputs per Transaction')\n",
    "axes[1].set_ylabel('Number of Transactions (log)')\n",
    "axes[1].set_title('Input Count Distribution (Log Scale)')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "sizes = [single_input, multi_input]\n",
    "labels = ['Single-Input\\n(not usable)', 'Multi-Input\\n(clustering)']\n",
    "colors = ['#cccccc', COLORS['primary']]\n",
    "axes[2].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90,\n",
    "            explode=(0, 0.05), textprops={'fontsize': 10})\n",
    "axes[2].set_title('Clustering Potential')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(OUTPUT_PATH) / 'input_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Entity Clustering\n",
    "\n",
    "Using **Connected Components** algorithm on a graph where:\n",
    "- **Nodes** = Bitcoin addresses\n",
    "- **Edges** = Addresses used together in multi-input transactions\n",
    "\n",
    "All connected addresses belong to the same entity (wallet/person/organization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clustering_inputs = enrich_clustering_inputs(tx_df, outputs_df, min_inputs=2, max_inputs=50).cache()\n",
    "print(f\"Enriched inputs for clustering: {clustering_inputs.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tx_addresses = clustering_inputs.groupBy(\"tx_hash\").agg(collect_set(\"address\").alias(\"addresses\"))\n",
    "tx_addresses = tx_addresses.filter(spark_size(\"addresses\") >= 2)\n",
    "\n",
    "def create_edges_udf(addresses):\n",
    "    if not addresses or len(addresses) < 2:\n",
    "        return []\n",
    "    return [(a, b) for a, b in combinations(sorted(addresses), 2)]\n",
    "\n",
    "edge_schema = ArrayType(StructType([StructField(\"src\", StringType()), StructField(\"dst\", StringType())]))\n",
    "create_edges = udf(create_edges_udf, edge_schema)\n",
    "\n",
    "edges_df = tx_addresses.withColumn(\"edges\", create_edges(\"addresses\")) \\\n",
    "    .select(explode(\"edges\").alias(\"edge\")) \\\n",
    "    .select(col(\"edge.src\").alias(\"src\"), col(\"edge.dst\").alias(\"dst\")).distinct()\n",
    "\n",
    "edges_path = str(Path(OUTPUT_PATH) / \"edges_temp.parquet\")\n",
    "edges_df.write.mode(\"overwrite\").parquet(edges_path)\n",
    "edges_df = spark.read.parquet(edges_path).cache()\n",
    "\n",
    "vertices_df = edges_df.select(col(\"src\").alias(\"id\")).union(edges_df.select(col(\"dst\").alias(\"id\"))).distinct()\n",
    "vertices_path = str(Path(OUTPUT_PATH) / \"vertices_temp.parquet\")\n",
    "vertices_df.write.mode(\"overwrite\").parquet(vertices_path)\n",
    "vertices_df = spark.read.parquet(vertices_path).cache()\n",
    "\n",
    "EDGE_COUNT = edges_df.count()\n",
    "VERTEX_COUNT = vertices_df.count()\n",
    "print(f\"Graph: {VERTEX_COUNT:,} vertices (addresses) | {EDGE_COUNT:,} edges (co-input pairs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "entities_df = graph.connectedComponents(algorithm=\"graphframes\", checkpointInterval=1, broadcastThreshold=100000)\n",
    "\n",
    "entities_path = str(Path(OUTPUT_PATH) / \"entities_temp.parquet\")\n",
    "entities_df.write.mode(\"overwrite\").parquet(entities_path)\n",
    "entities_df = spark.read.parquet(entities_path).cache()\n",
    "\n",
    "ADDRESS_COUNT = entities_df.count()\n",
    "ENTITY_COUNT = entities_df.select(\"component\").distinct().count()\n",
    "REDUCTION = (1 - ENTITY_COUNT/ADDRESS_COUNT) * 100\n",
    "\n",
    "print(f\"\\nClustering Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Addresses analyzed: {ADDRESS_COUNT:,}\")\n",
    "print(f\"Entities identified: {ENTITY_COUNT:,}\")\n",
    "print(f\"Reduction: {REDUCTION:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sizes = entities_df.groupBy(\"component\").agg(count(\"*\").alias(\"address_count\")).orderBy(desc(\"address_count\"))\n",
    "size_dist = entity_sizes.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "axes[0].hist(size_dist['address_count'], bins=50, color=COLORS['primary'], edgecolor='white', alpha=0.8)\n",
    "axes[0].set_xlabel('Addresses per Entity')\n",
    "axes[0].set_ylabel('Number of Entities')\n",
    "axes[0].set_title('Entity Size Distribution (Linear)')\n",
    "\n",
    "axes[1].hist(size_dist['address_count'], bins=50, color=COLORS['primary'], edgecolor='white', alpha=0.8)\n",
    "axes[1].set_xlabel('Addresses per Entity')\n",
    "axes[1].set_ylabel('Number of Entities (log)')\n",
    "axes[1].set_title('Entity Size Distribution (Log Scale)')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "top_20 = size_dist.head(20)\n",
    "bars = axes[2].barh(range(len(top_20)), top_20['address_count'], color=COLORS['accent'])\n",
    "axes[2].set_xlabel('Number of Addresses')\n",
    "axes[2].set_ylabel('Entity Rank')\n",
    "axes[2].set_title('Top 20 Largest Entities')\n",
    "axes[2].set_yticks(range(len(top_20)))\n",
    "axes[2].set_yticklabels([f\"#{i+1}\" for i in range(len(top_20))])\n",
    "axes[2].invert_yaxis()\n",
    "for bar, cnt in zip(bars, top_20['address_count']):\n",
    "    axes[2].text(bar.get_width() + max(top_20['address_count'])*0.02, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{int(cnt):,}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(OUTPUT_PATH) / 'entity_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results & Insights\n",
    "\n",
    "Calculating entity balances by joining the UTXO set with entity mappings. A **whale** is defined as an entity holding significant BTC (>1,000 BTC in this analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "entities_final = entities_df.select(col(\"id\").alias(\"address\"), col(\"component\").alias(\"entity_id\"))\n",
    "\n",
    "utxo_exploded = utxo_df.select(col(\"tx_hash\"), col(\"output_index\"), col(\"value\"), explode(col(\"addresses\")).alias(\"address\"))\n",
    "\n",
    "utxo_with_entities = utxo_exploded.join(entities_final, \"address\", \"inner\").select(\"entity_id\", \"value\")\n",
    "\n",
    "entity_balances = utxo_with_entities.groupBy(\"entity_id\").agg(\n",
    "    spark_sum(\"value\").alias(\"balance_satoshi\"),\n",
    "    count(\"*\").alias(\"utxo_count\")\n",
    ").withColumn(\"balance_btc\", spark_round(col(\"balance_satoshi\") / 100000000, 8)).orderBy(desc(\"balance_btc\")).cache()\n",
    "\n",
    "ENTITIES_WITH_BALANCE = entity_balances.count()\n",
    "TOTAL_BTC = entity_balances.agg(spark_sum(\"balance_btc\")).collect()[0][0]\n",
    "\n",
    "print(f\"Entity Balance Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Entities with balance: {ENTITIES_WITH_BALANCE:,}\")\n",
    "print(f\"Total BTC in UTXOs: {TOTAL_BTC:,.2f} BTC\")\n",
    "print(f\"Average per entity: {TOTAL_BTC/ENTITIES_WITH_BALANCE:.4f} BTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_whales = entity_balances.limit(20).toPandas()\n",
    "TOP_20_BTC = top_whales['balance_btc'].sum()\n",
    "TOP_20_SHARE = TOP_20_BTC / TOTAL_BTC * 100\n",
    "\n",
    "print(f\"Top 20 Whales\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Rank':<6} {'Entity ID':<20} {'Balance (BTC)':<18} {'UTXOs':<10} {'Share'}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for i, row in top_whales.iterrows():\n",
    "    share = row['balance_btc'] / TOTAL_BTC * 100\n",
    "    print(f\"#{i+1:<5} {int(row['entity_id']):<20} {row['balance_btc']:>15,.2f}   {int(row['utxo_count']):>8}   {share:>5.2f}%\")\n",
    "print(f\"{'-'*70}\")\n",
    "print(f\"{'Top 20 Total:':<27} {TOP_20_BTC:>15,.2f} BTC         {TOP_20_SHARE:>5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_categories = entity_balances.withColumn(\"category\",\n",
    "    when(col(\"balance_btc\") >= 1000, \"Mega Whale (1000+ BTC)\")\n",
    "    .when(col(\"balance_btc\") >= 100, \"Whale (100-1000 BTC)\")\n",
    "    .when(col(\"balance_btc\") >= 10, \"Large (10-100 BTC)\")\n",
    "    .when(col(\"balance_btc\") >= 1, \"Medium (1-10 BTC)\")\n",
    "    .otherwise(\"Small (<1 BTC)\")\n",
    ")\n",
    "\n",
    "category_stats = entity_categories.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"entity_count\"),\n",
    "    spark_sum(\"balance_btc\").alias(\"total_btc\")\n",
    ").orderBy(desc(\"total_btc\")).toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "top_10 = entity_balances.limit(10).toPandas()\n",
    "bars = axes[0].barh(range(len(top_10)), top_10['balance_btc'], color=COLORS['primary'])\n",
    "axes[0].set_xlabel('Balance (BTC)')\n",
    "axes[0].set_ylabel('Entity Rank')\n",
    "axes[0].set_title('Top 10 Whales by Balance')\n",
    "axes[0].set_yticks(range(len(top_10)))\n",
    "axes[0].set_yticklabels([f\"#{i+1}\" for i in range(len(top_10))])\n",
    "axes[0].invert_yaxis()\n",
    "for bar, val in zip(bars, top_10['balance_btc']):\n",
    "    axes[0].text(val + val*0.02, bar.get_y() + bar.get_height()/2, f'{val:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "colors_pie = [COLORS['secondary'], COLORS['accent'], COLORS['primary'], '#6B7280', '#D1D5DB']\n",
    "axes[1].pie(category_stats['entity_count'], labels=category_stats['category'], autopct='%1.1f%%',\n",
    "            colors=colors_pie[:len(category_stats)], startangle=90, textprops={'fontsize': 9})\n",
    "axes[1].set_title('Entity Distribution by Category')\n",
    "\n",
    "axes[2].pie(category_stats['total_btc'], labels=category_stats['category'], autopct='%1.1f%%',\n",
    "            colors=colors_pie[:len(category_stats)], startangle=90, textprops={'fontsize': 9})\n",
    "axes[2].set_title('BTC Distribution by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(OUTPUT_PATH) / 'whale_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_final.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"entities.parquet\"))\n",
    "utxo_df.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"utxos.parquet\"))\n",
    "outputs_df.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"outputs.parquet\"))\n",
    "inputs_df.write.mode(\"overwrite\").parquet(str(Path(OUTPUT_PATH) / \"inputs.parquet\"))\n",
    "\n",
    "print(f\"Data exported to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Executive Summary\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_whales = category_stats[category_stats['category'] == 'Mega Whale (1000+ BTC)']\n",
    "MEGA_WHALE_COUNT = int(mega_whales['entity_count'].values[0]) if len(mega_whales) > 0 else 0\n",
    "MEGA_WHALE_BTC = float(mega_whales['total_btc'].values[0]) if len(mega_whales) > 0 else 0\n",
    "MEGA_WHALE_SHARE = MEGA_WHALE_BTC / TOTAL_BTC * 100 if TOTAL_BTC > 0 else 0\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*70}\n",
    "                    BITCOIN WHALE INTELLIGENCE REPORT\n",
    "{'='*70}\n",
    "\n",
    "DATA SCOPE\n",
    "{'-'*70}\n",
    "  Blocks analyzed:        {BLOCK_COUNT:>15,}\n",
    "  Transactions:           {TX_COUNT:>15,}\n",
    "  Outputs created:        {OUTPUT_COUNT:>15,}\n",
    "  UTXOs (unspent):        {UTXO_COUNT:>15,}\n",
    "\n",
    "CLUSTERING RESULTS\n",
    "{'-'*70}\n",
    "  Addresses clustered:    {ADDRESS_COUNT:>15,}\n",
    "  Entities identified:    {ENTITY_COUNT:>15,}\n",
    "  Clustering reduction:   {REDUCTION:>14.1f}%\n",
    "\n",
    "WHALE ANALYSIS\n",
    "{'-'*70}\n",
    "  Total BTC tracked:      {TOTAL_BTC:>15,.2f} BTC\n",
    "  Mega Whales (1000+ BTC):{MEGA_WHALE_COUNT:>15,}\n",
    "  Mega Whale holdings:    {MEGA_WHALE_BTC:>15,.2f} BTC ({MEGA_WHALE_SHARE:.1f}%)\n",
    "  Top 20 concentration:   {TOP_20_BTC:>15,.2f} BTC ({TOP_20_SHARE:.1f}%)\n",
    "\n",
    "KEY INSIGHTS\n",
    "{'-'*70}\n",
    "  1. High concentration: Top 20 entities control {TOP_20_SHARE:.1f}% of tracked BTC\n",
    "  2. Power-law distribution: Few large whales, many small entities\n",
    "  3. Clustering effective: {REDUCTION:.1f}% address reduction achieved\n",
    "\n",
    "LIKELY WHALE IDENTITIES\n",
    "{'-'*70}\n",
    "  - Mega Whales (1000+ BTC): Early adopters, exchanges, institutional\n",
    "  - Whales (100-1000 BTC): Mining pools, large traders, services\n",
    "  - Large (10-100 BTC): Active traders, businesses, wealthy individuals\n",
    "\n",
    "{'='*70}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "metrics = [\n",
    "    f'Transactions\\n{TX_COUNT:,}',\n",
    "    f'Addresses\\n{ADDRESS_COUNT:,}',\n",
    "    f'Entities\\n{ENTITY_COUNT:,}',\n",
    "    f'Total BTC\\n{TOTAL_BTC:,.0f}',\n",
    "    f'Mega Whales\\n{MEGA_WHALE_COUNT}'\n",
    "]\n",
    "values = [TX_COUNT, ADDRESS_COUNT, ENTITY_COUNT, TOTAL_BTC, MEGA_WHALE_COUNT]\n",
    "normalized = [v / max(values) for v in values]\n",
    "\n",
    "bars = ax.bar(metrics, normalized, color=[COLORS['primary'], COLORS['secondary'], COLORS['accent'], COLORS['dark'], COLORS['secondary']])\n",
    "ax.set_ylabel('Relative Scale', fontsize=12)\n",
    "ax.set_title('Bitcoin Whale Intelligence: Key Metrics Overview', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 1.15)\n",
    "\n",
    "for bar, val, norm in zip(bars, values, normalized):\n",
    "    if val >= 1000:\n",
    "        label = f'{val:,.0f}'\n",
    "    else:\n",
    "        label = str(int(val))\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, norm + 0.03, label, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.text(0.5, -0.15, f'Top 20 Whale Concentration: {TOP_20_SHARE:.1f}% of all BTC | Clustering Reduction: {REDUCTION:.1f}%',\n",
    "        transform=ax.transAxes, ha='center', fontsize=12, style='italic', color=COLORS['dark'])\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(Path(OUTPUT_PATH) / 'executive_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
